{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Compiler Design Course","text":"<p>By: Morteza Zakeri\u2020</p> <p>\u2020 Ph.D., Computer Science, ACM Member (<code>mzakeri-nasrabadi[at]acm.org</code>).</p> <p></p> <p>Abstract\u2014 My compiler course is now more practical than ever. This repository contains several code snippets that I developed to teach the ANTLR compiler generator at Iran University of Science and Technology (UST).  Grammars have been written in ANTLR4 format. For each grammar, the source code of Lexer and Parser is available in Python 3.x.  The repository is assumed to be updated regularly. It would be appreciated if you use this repository by forking it. For any question please contact me <code>mzakeri-nasrabadi[at]acm.org</code>.</p>"},{"location":"#quick-access","title":"Quick Access","text":"<p>Sample exams</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The course is intended to teach the students the basic techniques that underlie the practice of Compiler Construction. The course will introduce the theory and tools that can be employed to perform syntax-directed translation of a high-level programming language into executable code.</p> <p>These techniques can also be employed in broader areas of application, whenever we need a syntax-directed analysis of symbolic expressions and languages and their translation into a lower-level description. They have multiple uses for man-machine interaction, including verification and program analysis.</p>"},{"location":"#objectives","title":"Objectives","text":"<ul> <li>To learn the overall compiler architecture,</li> <li>To learn various parsing methods and techniques,</li> <li>To learn low-level code generation and optimization,</li> <li>To learn an intellectual paradigm in system programming and testing.</li> </ul>"},{"location":"#motivations","title":"Motivations","text":"<ul> <li>Compiler construction is a microcosm of computer science!</li> <li>You dive into the heart of the system when you are building a compiler.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>The following outputs could be generated by code snippets in this repository.</p>"},{"location":"#three-addresses-codes","title":"Three addresses codes","text":"<p>Figure 1 shows how a single pass compiler can generate three address code for assignment statements with minimum numbers of temporary variable, started with <code>T</code>:</p> <pre><code>Input stream:\na1 := (2+3*3) * 6      // Numerical input\na2: := a1 + b * c / (p-q-r)     // Symbolic input\n\nComplier results:\n9 = 3 * 3\n11 = 2 + 9\n66 = 11 * 6\nAssignment value: a1 = 66\nAssignment type: int\n-----------\nT1 =  b * c     // Machine generated three-adresses codes\nT2 = p - q\nT2 = T2 - r\nT1 = T1 / T2 \nT1 = a1 + T1\nAssignment value: a1 = 66\nAssignment type: str\n-----------\nParsing and code generation was done!\n</code></pre> <p>Fig 1: Examples of three address codes generated by ANTLR for AssignmentStatement grammar.</p>"},{"location":"#abstract-syntax-tree","title":"Abstract Syntax Tree","text":"<p>Abstract syntax trees (ASTs) are a useful abstraction when dealing with programming languages as an object for analysis or manipulation (e.g. compilation). Figure 2 demonstrates how a single pass compiler can generate abstract syntax three (AST) for assignment statements.</p> <p></p> <p>Figure 2: Examples of abstract syntax tree (AST) generated by ANTLR for AssignmentStatement grammar.</p> <p>The above tree corresponds to the following expression: </p> <p><code>a1 := (2 + 12 * 3) / (6 - 19)</code></p>"},{"location":"#structure","title":"Structure","text":"<p>The following describes the structure of the repository:</p>"},{"location":"#grammars","title":"grammars","text":"<p><code>gram1</code>: ANTLR hello world grammar.</p> <p><code>Expr1</code>: Simple grammar for handling mathematical expressions without any attribute and action.</p> <p><code>Expr2</code>: Simple attributed grammar for handling mathematical expressions with code() attribute.</p> <p><code>Expr3</code>: Currently, it is the same <code>Expr2</code> grammar.</p> <p><code>AssignmentStatement1.g4</code>: The grammar to handle multiple assignment statements and mathematical expressions in programming languages like Pascal and C/C++. </p> <p><code>AssignmentStatement2.g4</code>: It is the same <code>AssignmentStatement1.g4</code> grammar plus attributes for code and type of rules.</p> <p><code>AssignmentStatement3.g4</code>: The grammar to handle multiple assignment statements and mathematical expressions in programming languages like Pascal and C/C++. It provides semantic rules to perform type checking and semantic routines to generate intermediate representation.</p> <p><code>AssignmentStatement4.g4</code>: The grammar to handle multiple assignment statements and mathematical expressions in programming languages like Pascal and C/C++. It provides semantic rules to perform type checking and semantic routines to generate intermediate representation. It has been implemented to generate intermediate representation (three addresses codes) with minimum number of \"temp\" variables. </p> <p><code>CPP14_v2</code>: ANTLR grammar for C++14 forked from the official ANTRL website. Some bugs have been fixed and also the rule identifiers have been added to the grammar rules.</p> <p><code>EMail.g4</code>: Lexical grammar to validate email addresses.</p> <p><code>EMail2.g4</code>: Lexical grammar to validate email addresses, fixing bugs in <code>EMail.g4</code></p>"},{"location":"#language_apps","title":"language_apps","text":"<p>The <code>language_apps</code> package currently contains Lexer and Parser codes for each grammar in directory <code>grammars</code>, with a main driver script to demonstrate the type checking and intermediate code generation based on semantic rules and semantic routines. </p>"},{"location":"#terminal_batch_scripts","title":"terminal_batch_scripts","text":"<p>The <code>termina_batch_script</code> directory contains several batch script to run ANTLR in terminal (Windows) to generate target code in JAVA language. The code snippets in this directory belong to my first experiences with ANTLR. </p>"},{"location":"#read-more","title":"Read more","text":"<p>IUST compiler course official webpage</p> <ul> <li>ANTLR slides: PART 1: Introduction</li> <li>ANTLR slides: PART 2: Getting started in Java</li> <li>ANTLR slides: PART 3: Getting started in C#</li> </ul>"},{"location":"announcements/","title":"Announcements","text":"<p>September 09, 2024: Course schedule for Fall 2024 is online:</p> <ul> <li> <p>Amirkabir University of Technology</p> </li> <li> <p>Sharif University of Technology</p> </li> </ul> <p>September 10, 2022: I am designing some new applications to be used as compiler teaching projects. Please let me know if you have any good ideas/proposals. </p>"},{"location":"antlr_tutorials/antlr_advanced/","title":"ANTLR advanced tutorials","text":"<p>By: Morteza Zakeri</p> <p>Last update: April 30, 2022</p>"},{"location":"antlr_tutorials/antlr_advanced/#compiler-background","title":"Compiler background","text":"<p>We first define some terms used in compiler literature when analyzing the program based on the ANTLR vocabulary.</p> <ul> <li> <p>Compiler pass. Each time that the <code>walk</code> method of <code>ParseTreeWalker</code> class is called, it visits all nodes of the parse tree. In compiler literature, we called this process a pass. The ANTLR pass can be annotated with listener classes to perform specific analysis or transformation. An analysis pass refers to the pass in which some information is obtained from the source code, but the source code is not changed, or no new code is generated. A transformation pass refers to the pass in which the program source code is modified or new codes are generated. As we discussed in the next sections, refactoring automation consists of both the analysis and transformation passes.</p> </li> <li> <p>Single v.s. multiple pass.  Often to perform specific analyses or transformations, the program should be visited multiple times. Indeed, such tasks required multiple passes. For instance, if a class attribute is defined after it is used in a method, which is possible in Java programming language, to find the definition of the field and then modify its usage, we should visit the program twice. The reason is that the program tokens are read from left to right, and then when traversing the parse tree, the node only is visited once in the order they appear in the program text. For a given task, if we visit a node and require the information obtained from the next nodes, we cannot complete the task in one pass. In such a case, a pass is required to obtain the necessary information from the next nodes, and another pass is required to use this information for the current node. Most refactoring operations we described in this chapter require multiple passes to complete the refactoring process. For each pass, we develop a separate listener class and pass it to <code>ParseTreeWalker</code> class.</p> </li> </ul>"},{"location":"antlr_tutorials/antlr_advanced/#summary","title":"Summary","text":"<p>An analysis that is performed by traversing the program once is called a single pass-analysis. </p> <p>An analysis that is performed by traversing the program source code twice or more is called multiple analysis passes. </p> <p>Todo: To be completed ...</p>"},{"location":"antlr_tutorials/antlr_basics/","title":"ANTLR basic tutorials","text":"<p>By: Morteza Zakeri</p> <p>Last update: April 30, 2022</p>"},{"location":"antlr_tutorials/antlr_basics/#introduction","title":"Introduction","text":"<p>The ANTLR tool generates a top-down parser from the grammar rules defined with the ANTLR meta-grammar (Parr and Fisher 2011). The initial version of ANTLR generated the target parser source code in Java. In the current version (version 4), the parser source code can be generated in a wide range of programming languages listed on the ANTLR official website (Parr 2022a).  For simplicity, we generate the parser in Python 3, which provides us to run the tool on every platform having Python 3 installed on it.  Another reason to use Python is that we can integrate the developed program easily with other libraries available in Python, such as machine learning and optimization libraries.  Finally, I found that there is no comprehensive tutorial on using ANTLR with the Python backend. </p> <p>To use ANTLR in other programming languages, specifically Java and C#, refer to the ANTLR slides I created before this tutorial. </p> <p>The ANTLR tool is a small \u201c.jar\u201d file that must be run from the command line to generate the parser codes. The ANTLR tool jar file can be downloaded from here. </p>"},{"location":"antlr_tutorials/antlr_basics/#generating-parser","title":"Generating parser","text":"<p>As mentioned, to generate a parser for a programming language, the grammar specification described with ANTLR meta-grammar is required. ANTLR grammar files are named with the \u201c.g4\u201d suffix. </p> <p>We obtain the grammar of Java 8 to build our parser for the Java programming language. The grammar can be downloaded from ANTLR 4 grammar repository on GitHub:  https://github.com/antlr/grammars-v4.  Once the ANTLR tool and required grammar files are prepared, we can generate the parser for that with the following command:</p> <pre><code>$ java -Xmx500M -cp antlr-4.9.3-complete.jar org.antlr.v4.Tool -Dlanguage=Python3 -o . JavaLexer.g4\n\n$ java -Xmx500M -cp antlr-4.9.3-complete.jar org.antlr.v4.Tool -Dlanguage=Python3 -visitor -listener -o . JavaLabeledParser.g4\n</code></pre> <p>The first command generates the lexer from the <code>JavaLexer.g4</code> description file and the second command generates the parser from the <code>JavaLabeledParser.g4</code> description file. It is worth noting that the lexer and parser can be written in one file. In such a case, a single command generates all required codes in one step.</p> <p>The grammar files used in the above command are also available in grammars directory of the CodART repository. You may see that I have made some modifications to the Parser rules. </p> <p>In the above commands, the <code>antlr-4.9.3-complete.jar</code> is the ANTLR tool that requires Java to be executed. <code>-Dlanguage</code> denotes the destination language that the ANTLR parser (and lexer) source code is generated in which. In our case, we set it to Python3. </p> <p>After executing the ANTLR parser generation commands, eight files, including parser source code and other required information, are generated. Figure 1 shows the generated files. The \u201c.py\u201d contains lexer and parser source code that can parse any Java input file. The <code>-visitor -listener</code> switches in the second command result in generating two separate source files, <code>JavaLabledParserListener.py</code> and <code>JavaLabledParserVistor.py</code>, which provide interfaces to implement the required codes for a specific language application. Our application is source code refactoring which uses the listener mechanism to implement necessary actions transforming the program to the refactored version.  The parse tree structure in and listener mechanism are discussed in the next sections.</p> <p></p> <p>Figure 1. Generated files by ANTLR.</p> <p>It should be noted that to use the generated classes in Figure 1, for developing a specific program, we need to install the appropriate ANTLR runtime library. For creating ANTLR-based programs in Python, the command <code>pip install antlr-python3-runtime</code> can be used. It installed all runtime dependencies required to program using the ANTLR library.</p>"},{"location":"antlr_tutorials/antlr_basics/#antlr-parse-tree","title":"ANTLR parse tree","text":"<p>The generated parser by ANTLR is responsible for parsing every Java source code file and generating the parse tree or designating the syntax errors in the input file. The parse tree for real-world programs with thousands of lines of code has a non-trivial structure. ANTLR developers have provided some IDE plugins that can visualize the parse tree to better understand the structure of the parse tree generated by ANTLR. We use Pycharm IDE developed by Jetbrains  to work with Python code. </p> <p>Figure 2 shows how we can install the ANTLR plugin in PyCharm. The plugin source code is available on the GitHub repo. When the plugin is installed, the ANTLR preview widow is applied at the bottom of the PyCharm IDE. In addition, the IDE can be recognized as \u201c.g4\u201d files and some other options added to the IDE. The main option is the ability to test a grammar rule and visualize the corresponding parse tree to that rule.</p> <p></p> <p>Figure 2. Installing the ANTLR plugin in the PyCharm IDE.</p> <p>In order to use the ANTLR preview tab, the ANTLR grammar should be opened in the PyCharm IDE. We then select a rule (typically the start rule) of our grammar, right-click on the rule, and select the \u201cTest Rule <code>rule_name</code>\u201d option from the opened menu, shown in Figure 3. We now write our sample input program in the left panel of the ANTLR preview, and the parse tree is shown in the right panel. </p> <p></p> <p>Figure 3. Test the grammar rule in the ANTLR PyCharm plugin.</p> <p>Figure 4 shows a simple Java class and the corresponding parse tree generated by the ANTLR. The leaves of the parse tree are program tokens, while the intermediate nodes are grammar rules that the evaluating program is derived from them. Also, the root of the tree is the grammar rule, which we selected to start parsing. It means that we can select and test every rule independently. However, a complete Java program can only parse from the start rule of the given grammar, i.e., the <code>compilaionUnit</code> rule.</p> <p></p> <p>Figure 4. Test the grammar rule in the ANTLR PyCharm plugin.</p> <p>It should be mentioned that the ANTLR Preview window is based on a grammar interpreter, not on the actual generated parser described in the previous section. It means that grammar attributes such as actions and predicates will not be evaluated during live preview because the interpreter is language agnostic. For the same reasons, if the generated parser and/or lexer classes extend a custom implementation of the base parser/lexer classes, the custom code will not be run during the live preview. </p> <p>In addition to the parse tree visualization, the ANTLR plugin provides facilities such as profiling, code generation, etc., described in here (Parr 2022b). For example, the profile tab shows the execution time of each rule in the parser for a given input string.</p> <p>I want to emphasize that visualizing the parse tree with the ANTLR plugin is really helpful when developing code and fixing bugs described in the next section of this tutorial.</p>"},{"location":"antlr_tutorials/antlr_basics/#traversing-the-parse-tree-programmatically","title":"Traversing the parse tree programmatically","text":"<p>ANTLR is not a simple parser generator. It provides a depth-first parse tree visiting and a callback mechanism called listener to implement the required program analysis or transformation passes. The depth-first search is performed by instantiating an object from the ANTLR <code>ParseTreeWalker</code> class and calling the walk method, which takes an instance of <code>ParseTree</code> as an input argument and traverses it.</p> <p>Obviously, if we visit the parse tree with the depth-first search algorithm, all program tokens are visited in the same order that they appeared in the source code file. However, the depth-first search contains additional information about when a node in the tree is visited and when the visiting all nodes in its subtree is finished. Therefore, we can add the required actions when visiting a node to perform a special task. For example, according to Figure 4, for counting the number of classes in a code snippet, we can define a counter variable, initialize it to zero, and increase it whenever the walker visits the \u201cclassDeclartion\u201d node. </p> <p>ANTLR provides two callback functions for each node in the parse tree. One is called by the walker when it is entered into a node, i.e., visit the node, but the children are not visited yet. Another is called when all nodes in the subtree of the visited node have been visited, and the walker is exiting the node. These callback functions are available in the listener class generated by the ANTLR for every rule in a given grammar. In our example for counting the number of classes, we implement all required logic in the body of <code>enterClassDeclartion</code> method of the <code>JavaLabledParserListener</code> class. We called these logic codes grammar\u2019s actions since, indeed, they are bunded to a grammar rule. </p> <p>It is worth noting that we can add these actions codes in the grammar file (<code>.g4</code> file) to form an attributed grammar. Embedding actions in grammar increase the efficiency of the analyzing process. However, when we need many complex actions, the listener mechanism provides a better way to implement them. Indeed, ANTLR 4 emphasizes separating the language applications from the language grammar by using the listener mechanism.</p> <p>Listing 1 shows the implementation program for counting the number of classes using the ANTLR listener mechanism. The <code>DesignMetrics</code> class inherits from <code>JavaLabeledParserListener</code> class which is the default listener class generated by ANTLR. We only implement the <code>enterClassDeclartion</code> method, which increases the value of the <code>__dsc</code> counter each time the walker visits a Java class.</p> <pre><code># module: JavaLabledParserListener.py\n\n__version__ = \"0.1.0\"\n__author__ = \"Morteza\"\n\nfrom antlr4 import *\nif __name__ is not None and \".\" in __name__:\n    from .JavaLabeledParser import JavaLabeledParser\nelse:\n    from JavaLabeledParser import JavaLabeledParser\n\nclass JavaLabeledParserListener(ParseTreeListener):\n    # \u2026\n    def enterClassDeclaration(self,\n                              ctx:JavaLabeledParser.ClassDeclarationContext):\n        pass\n    # \u2026\n\nclass DesignMetrics(JavaLabeledParserListener):\n    def __init__(self):\n        self.__dsc:int = 0  # Keep design size in classes\n\n    @property\n    def get_design_size(self):\n        return self.__dsc\n\n    def enterClassDeclaration(self,\n                              ctx:JavaLabeledParser.ClassDeclarationContext):\n        self.__dsc += 1\n</code></pre> <p>Listing 1: Programs that count the number of classes in a Java source code.</p>"},{"location":"antlr_tutorials/antlr_basics/#wiring-the-modules","title":"Wiring the modules","text":"<p>To complete our simple analysis task, first, the parse tree for a given input should be constructed. Then, the DesignMetrics class should be instantiated and passed to an object of ParseTreeWalker class. We created a driver module in Python beside the generated code by ANTLR to connect different parts of our program and complete our task. Listing 2 shows the implementation of the main driver for a program that counts the number of classes in Java source codes.</p> <pre><code># Module: main_driver.py\n\n__version__ = \"0.1.0\"\n__author__ = \"Morteza\"\nfrom antlr4 import *\nfrom JavaLabledLexer import JavaLabledLexer\nfrom JavaLeabledParser import JavaLabledParser\nfrom JavaLabledParserListener import DesignMetrics\n\ndef main(args):\n\n    # Step 1: Load input source into the stream object\n    stream = FileStream(args.file, encoding='utf8')\n\n    # Step 2: Create an instance of AssignmentStLexer\n    lexer = JavaLabledLexer(stream)\n\n    # Step 3: Convert the input source into a list of tokens\n    token_stream = CommonTokenStream(lexer)\n\n    # Step 4: Create an instance of the AssignmentStParser\n    parser = JavaLabledParser(token_stream)\n\n    # Step 5: Create parse tree\n    parse_tree = parser.compilationUnit()\n\n    # Step 6: Create an instance of DesignMetrics listener class\n    my_listener = DesignMetrics()\n\n    # Step 7: Create a walker to traverse the parse tree and callback our listener\n    walker = ParseTreeWalker()\n    walker.walk(t=parse_tree, listener=my_listener)\n\n    # Step 8: Getting the results\n    print(f'DSC={my_listener.get_design_size}')\n</code></pre> <p>Listing 2: Main driver module for the program in Listing 1</p>"},{"location":"antlr_tutorials/antlr_basics/#conclusion-and-remarks","title":"Conclusion and remarks","text":"<p>In this tutorial, we described the basic concepts regarding using the ANTLR tool to generate and walk phase three and implement custom program analysis applications with the help of the ANTLR listener mechanism. The most important point is that we used the real-world programming languages grammars to show the parsing and analyzing process. The discussed topics form the underlying concepts of our approach for automated refactoring used in CodART. Check out the ANTLR advanced tutorial to find out how we can use ANTLR for reliable and efficient program transformation.</p>"},{"location":"antlr_tutorials/antlr_basics/#references","title":"References","text":"<ul> <li> <p>[1] Parr T ANTLR (ANother Tool for Language Recognition). https://www.antlr.org. Accessed 10 Jan 2022a</p> </li> <li> <p>[2] Parr T IntelliJ Idea Plugin for ANTLR v4. https://github.com/antlr/intellij-plugin-v4. Accessed 10 Jan 2022b</p> </li> <li> <p>[3] Parr T, Fisher K (2011) LL(*): the foundation of the ANTLR parser generator. Proc 32nd ACM SIGPLAN Conf Program Lang Des Implement 425\u2013436. https://doi.org/http://doi.acm.org/10.1145/1993498.1993548</p> </li> </ul>"},{"location":"antlr_tutorials/antlr_slides/","title":"ANTLR basic tutorials (slides)","text":"<p>By: Morteza Zakeri</p> <p>Last update: May 1, 2022 </p>"},{"location":"antlr_tutorials/antlr_slides/#introduction-to-antlr-part-i","title":"Introduction to ANTLR: Part I","text":"Antlr part1 introduction  from Morteza Zakeri"},{"location":"antlr_tutorials/antlr_slides/#introduction-to-antlr-part-ii","title":"Introduction to ANTLR: Part II","text":"Antlr part2 getting_started_in_java  from Morteza Zakeri"},{"location":"antlr_tutorials/antlr_slides/#introduction-to-antlr-part-iii","title":"Introduction to ANTLR: Part III","text":"Antlr part3 getting_started_in_c_sharp  from Morteza Zakeri"},{"location":"assignments/","title":"Assignments","text":"<p>Visit writing and programming assignments.</p>"},{"location":"assignments/#exams","title":"Exams","text":""},{"location":"assignments/#midterms","title":"Midterms","text":"<ul> <li> <p> Tehran University - Fall 2023</p> </li> <li> <p> Amirkabir University - Fall 2024</p> </li> </ul>"},{"location":"assignments/#final","title":"Final","text":"<ul> <li> <p> Tehran University - Fall 2023</p> </li> <li> <p> Amirkabir University - Fall 2024</p> </li> </ul>"},{"location":"assignments/programming_assignment/","title":"Programming assignments","text":""},{"location":"assignments/programming_assignment/#current-semester","title":"Current semester","text":""},{"location":"assignments/programming_assignment/#archive","title":"Archive","text":""},{"location":"assignments/programming_assignment/#semester-fall-2018","title":"Semester Fall 2018","text":""},{"location":"assignments/programming_assignment/#semester-spring-2018","title":"Semester Spring 2018","text":""},{"location":"assignments/programming_assignment/#semester-fall-2017","title":"Semester Fall 2017","text":""},{"location":"assignments/writing_assignments/","title":"Writing assignments","text":""},{"location":"assignments/writing_assignments/#current-semester","title":"Current semester","text":""},{"location":"assignments/writing_assignments/#archive","title":"Archive","text":""},{"location":"assignments/writing_assignments/#semester-fall-2018-971","title":"Semester Fall 2018 (971)","text":"<ul> <li> HW1</li> <li> HW2</li> <li>HW3 (Dr. Parsa Questions)</li> </ul>"},{"location":"assignments/writing_assignments/#semester-spring-2018-962","title":"Semester Spring 2018 (962)","text":"<ul> <li> HW1</li> <li> HW2</li> <li> HW3</li> <li> HW4</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/01/31/the-minijava-to-c-compiler/","title":"The MiniJava to C Compiler","text":"<p>The MiniJava-C Compiler is a project designed to compile MiniJava code into C. MiniJava is a simplified version of Java that includes essential object-oriented features, making it ideal for educational purposes and lightweight application development. This compiler parses MiniJava code, performs semantic checks, and generates equivalent C code for execution.</p> <p>Note: This project was developed for educational purposes as part of the Compiler Design course at Amirkabir University of Technology (AUT).</p> <p>CheckOut Main Repository: MiniJava to C Compiler</p>"},{"location":"blog/2025/01/31/the-minijava-to-c-compiler/#compiler-phases","title":"Compiler Phases","text":"<ol> <li>Lexer:</li> <li>Converts the input MiniJava code into a stream of tokens.</li> <li>Identifies keywords, identifiers, literals, and symbols.</li> <li>Reports lexical errors if invalid characters are encountered.</li> <li>Uses SimpleJavaLexer</li> <li>Parser:</li> <li>Analyzes the token stream to construct an Abstract Syntax Tree (AST).</li> <li>Ensures the syntax conforms to the MiniJava grammar.</li> <li>Reports syntax errors with precise locations.</li> <li>Semantic Analyzer:</li> <li>Performs type checking and ensures the semantic correctness of the program.</li> <li>Validates variable declarations, method calls, and expression types.</li> <li>Reports semantic errors, such as type mismatches or undeclared variables.</li> <li>Code Generator:</li> <li>Translates the validated AST into equivalent C code.</li> <li>Handles variable initialization, method calls, control flow, and expressions.</li> <li>Produces optimized and readable C output.</li> </ol>"},{"location":"blog/2025/01/31/the-minijava-to-c-compiler/#features","title":"Features","text":"<ul> <li>Object-Oriented Programming Support :</li> <li>Classes and inheritance</li> <li>Method overriding</li> <li>Field access across inheritance chains</li> <li>this reference</li> <li>Control Structures :</li> <li><code>if</code>, <code>else</code> statements</li> <li><code>for</code>, <code>while</code> loops</li> <li>break and continue statements</li> <li>Arrays :</li> <li>Integer array support (int[])</li> <li>Array length property</li> <li>Array indexing</li> <li>Basics :</li> <li>Basic data types: int, boolean</li> <li>Classes and objects</li> <li>Method calls</li> <li>Conditional statements (if-else)</li> <li>Loops (while)</li> <li>Arithmetic operations (+, -, *, /, %)</li> <li>Logical operations (&amp;&amp;, ||, !)</li> <li>Relational operators (&lt;, &lt;=, &gt;, &gt;=, ==, !=)</li> <li>Variable assignments</li> </ul>"},{"location":"blog/2025/01/31/the-minijava-to-c-compiler/#example","title":"Example","text":"<pre><code>class Main {\n    public static void main(String[] args) {\n        Calculator calc = new Calculator();\n        int result = 2 + calc.add(4, calc.multiply(2, 4) / 2) * 4;\n        System.out.println(result);\n    }\n}\nclass Base {\n    public int add(int a, int b) {\n        return a + b;\n    }\n}\nclass Calculator extends Base {\n    public int multiply(int a, int b) {\n        return a * b;\n    }\n}\n</code></pre> <p>Compiles to:</p> <pre><code>// Main.c\nint main() {\n    Calculator *calc = $_new_Calculator();\n    int result;\n    int $_t_1 = calc-&gt;$_function_multiply(calc, 2, 4);\n    int $_t_2 = $_t_1 / 2;\n    int $_t_3 = calc-&gt;super.$_function_add(calc, 4, $_t_2);\n    int $_t_4 = $_t_3 * 4;\n    int $_t_5 = 2 + $_t_4;\n    result = $_t_5;\n    printf(\"%d\\n\", result);\n}\n\n// Base.h\nstruct Base {\n    int (*$_function_add)(void *, int, int);\n};\ntypedef struct Base Base;\nint Base_add(void *$this, int a, int b);\nBase *$_new_Base();\n\n// Base.c\nBase *$_new_Base() {\n    Base *self = (Base *) malloc(sizeof(Base));\n    self-&gt;$_function_add = Base_add;\n    return self;\n}\nint Base_add(void *$this, int a, int b) {\n    Base *super = (Base *) $this;\n    int $_t_0 = a + b;\n    return $_t_0;\n}\n\n// Calculator.h\nstruct Calculator {\n    Base super;\n    int (*$_function_multiply)(void *, int , int );\n};\ntypedef struct Calculator Calculator;\nint Calculator_multiply(void *$this, int a, int b);\nCalculator *$_new_Calculator();\n\n// Calculator.c\nCalculator *$_new_Calculator() {\n    Calculator *self = (Calculator *) malloc(sizeof(Calculator));\n    self-&gt;$_function_multiply = Calculator_multiply;\n    self-&gt;super.$_function_add = Base_add;\n    return self;\n}\nint Calculator_multiply(void *$this, int a, int b) {\n    Calculator *super = (Calculator *) $this;\n    int $_t_0 = a * b;\n    return $_t_0;\n}\n</code></pre> <p>Parse Tree: <pre><code>...\nClass{\n  Name: Calculator\n  Extends: Base\n  Fields: (0)\n  Methods: (1)\n    Method{Name: multiply, Type: int, Params: (Field{Name: a, Type: int}, Field{Name: b, Type: int})} {\n      CodeBlock\n        Return: \n          BinaryExpression (*) (Type:int)\n            Reference (Type:int): a\n            Reference (Type:int): b\n    }\n}\n</code></pre></p>"},{"location":"blog/2025/01/31/the-minijava-to-c-compiler/#implementation-details","title":"Implementation Details","text":"<ul> <li>Class Translation</li> <li>Classes become C structs</li> <li>Methods become function pointers</li> <li>Inheritance uses nested structs</li> <li>Inheritance validation</li> <li>Static type checking</li> <li>Method Dispatch</li> <li>Virtual method tables via function pointers</li> <li>$this pointer passed as first argument</li> <li>Method override verification</li> </ul>"},{"location":"calendar/","title":"Class Calendar","text":""},{"location":"calendar/#spring-2025","title":"Spring 2025","text":"<ul> <li>Amirkabir University of Technology</li> </ul>"},{"location":"calendar/#fall-2024","title":"Fall 2024","text":"<ul> <li> <p>Amirkabir University of Technology</p> </li> <li> <p>Sharif University of Technology</p> </li> </ul>"},{"location":"calendar/fall2024_aut/","title":"Course schedule","text":"Week Day Date Date   (Persian) Session Lecture Events W01 Sunday 1403/06/18 Session 01 Course   layout - Introduction Tuesday 1403/06/20 Session 02 Compiler basics (compiling steps, compilers types, and   architectures) W02 Sunday 1403/06/25 Session 03 Compiler basics (compiling   steps, compilers types, and architectures) Tuesday 1403/06/27 Session 04 Compilation   at a glance (A working example) W03 Sunday 1403/07/01 Session 05 Formal   languages and automata backgrounds Tuesday 1403/07/03 Session 06 Lexical   analysis W04 Sunday 1403/07/08 Session 07 Lexical   analysis - FLEX Tuesday 1403/07/10 Session 08 Syntax   analysis W05 Sunday 1403/07/15 Session 09 Top-down   parsing - Recursive decent parsers Tuesday 1403/07/17 Session 10 Top-down   parsing - LL (1) parsers W06 Sunday 1403/07/22 Session 11 Top-down   parsing - LL (*) parsers (ANTLR4 parser generator) Tuesday 1403/07/24 Session 12 Bottom-up   parsing - LR(0) and LR (1) W07 Sunday 1403/07/29 Session 13 Bottom-up   parsing - LALR(1) Tuesday 1403/08/01 Session 14 Bottom-up   parsing - SLR(1) W08 Sunday 1403/08/06 Session 15 Bottom-up   parser generators - YACC and BISON Tuesday 1403/08/08 Session 16 Bottom-up   parsing - CYK W09 Sunday 1403/08/13 Session 17 Midterm   Exam (paper-based) Tuesday 1403/08/15 Session 18 Program translation and   transformation techniques W10 Sunday 1403/08/20 Session 19 Syntax-directed   translation Tuesday 1403/08/22 Session 20 Model-driven   translation W11 Sunday 1403/08/27 Session 21 Type   checking and symbol tables Tuesday 1403/08/29 Session 22 Type   checking and symbol tables - Sci-tool Understand W12 Sunday 1403/09/04 Session 23 Intermediate   code generation -abstract syntax tress (ASTs) Tuesday 1403/09/06 Session 24 Intermediate   code generation - tree addresses code (TACs) W13 Sunday 1403/09/11 Session 25 Intermediate   code optimization -CFGs Tuesday 1403/09/13 Session 26 Intermediate   code optimization - Temporary variables minimization W14 Sunday 1403/09/18 Session 27 Runtime   environments and code generation Tuesday 1403/09/20 Session 28 Advanced   topics (compiler applications in software engineering) W15 Sunday 1403/09/25 Session 29 Students'   in-class presentations Tuesday 1403/09/27 Session 30 Course   conclusion W16 Sunday 1403/10/02 Tuesday 1403/10/04 W17 Sunday Tuesday W18"},{"location":"calendar/fall2024_sharifu/","title":"Course schedule","text":"Week Day Date Date   (Persian) Session Lecture Events W01 Saturday 1403/06/31 No class Monday 1403/07/02 Session 01 Course   layout - Introduction W02 Saturday 1403/07/07 Session 02 Compiler   basics (compiling steps, compilers types and architectures) Monday 1403/07/09 Session 03 Compilation   at a glance (A working example) W03 Saturday 1403/07/14 Session 04 Formal   languages and automata backgrounds Monday 1403/07/16 Session 05 Lexical   analysis W04 Saturday 1403/07/21 Session 06 Lexical   analysis - FLEX Monday 1403/07/23 Session 07 Syntax   analysis W05 Saturday 1403/07/28 Session 08 Top-down   parsing - Recursive decent parsers Monday 1403/07/30 Session 09 Top-down   parsing - LL (1) parsers W06 Saturday 1403/08/05 Session 10 Top-down   parsing - LL (*) parsers (ANTLR4 parser generator) Monday 1403/08/07 Session 11 Bottom-up   parsing - LR(0) and LR (1) W07 Saturday 1403/08/12 Session 12 Bottom-up   parsing - LALR(1) Monday 1403/08/14 Session 13 Bottom-up   parsing - SLR(1) W08 Saturday 1403/08/19 Session 14 Bottom-up   parser generators - YACC and BISON Monday 1403/08/21 Session 15 Bottom-up   parsing - CYK W09 Saturday 1403/08/26 Session 16 Midterm   Exam (paper-based) Monday 1403/08/28 Session 17 Program translation and   transformation techniques W10 Saturday 1403/09/03 Session 18 Syntax-directed   translation Monday 1403/09/05 Session 19 Model-driven   translation W11 Saturday 1403/09/10 Session 20 Type   checking and symbol tables Monday 1403/09/12 Session 21 Type   checking and symbol tables - Sci-tool Understand W12 Saturday 1403/09/17 Session 22 Intermediate   code generation -abstract syntax tress (ASTs) Monday 1403/09/19 Session 23 Intermediate   code generation - tree addresses code (TACs) W13 Saturday 1403/09/24 Session 24 Intermediate   code optimization -CFGs Monday 1403/09/26 Session 25 Intermediate   code optimization - Temporary variables minimization W14 Saturday 1403/10/01 Session 26 Runtime   environments and code generation Monday 1403/10/03 Session 27 Advanced   topics (compiler applications in software engineering) W15 Saturday 1403/10/08 Session 28 Students'   in-class presentations Monday 1403/10/10 Session 29 Course   conclusion W16 Saturday 1403/10/15 Monday 1403/10/17 W17 W18"},{"location":"calendar/spring2025_aut/","title":"Course schedule","text":"Week Day Date Date   (Persian) Session Lecture Events W01 Sunday 1403/11/07 Session 01 Course   layout - Introduction Sunday 1403/11/14 Session 02 Compiler basics (compiling steps, compilers types, and   architectures) W02 Tuesday 1403/11/16 Session 03 Compiler basics (compiling   steps, compilers types, and architectures) Sunday 1403/11/21 Session 04 Compilation   at a glance (A working example) W03 Tuesday 1403/11/23 Session 05 Formal   languages and automata backgrounds Sunday 1403/11/28 Session 06 Lexical   analysis W04 Tuesday 1403/11/30 Session 07 Lexical   analysis - FLEX Sunday 1403/12/05 Session 08 Syntax   analysis W05 Tuesday 1403/12/07 Session 09 Top-down   parsing - Recursive decent parsers Release First Homework : Lecture 1 &amp; 2 Sunday 1403/12/12 Session 10 Top-down   parsing - LL (1) parsers W06 Tuesday 1403/12/14 Session 11 Top-down   parsing - LL (*) parsers (ANTLR4 parser generator) Sunday 1403/12/19 Session 12 Bottom-up   parsing - LR(0) and LR (1) W07 Tuesday 1403/12/21 Session 13 Bottom-up   parsing - LALR(1) Sunday 1403/12/26 Session 14 Bottom-up   parsing - SLR(1) W08 Tuesday 1403/12/28 Session 15 Bottom-up   parser generators - YACC and BISON Release Second Homework : Lecture 3 &amp; 4 Sunday 1404/01/17 Session 16 Bottom-up   parsing - CYK W09 Tuesday 1404/01/19 Session 17 Midterm   Exam (paper-based) Sunday 1404/01/24 Session 18 Program translation and   transformation techniques W10 Tuesday 1404/01/26 Session 19 Syntax-directed   translation Sunday 1404/01/31 Session 20 Model-driven   translation W11 Tuesday 1404/02/02 Session 21 Type   checking and symbol tables Sunday 1404/02/07 Session 22 Type   checking and symbol tables - Sci-tool Understand Release Third Homework : Lecture 5 &amp; 6 W12 Tuesday 1404/02/09 Session 23 Intermediate   code generation -abstract syntax tress (ASTs) Sunday 1404/02/14 Session 24 Intermediate   code generation - tree addresses code (TACs) W13 Tuesday 1404/02/16 Session 25 Intermediate   code optimization -CFGs Sunday 1404/02/21 Session 26 Intermediate   code optimization - Temporary variables minimization Release Fourth Homework : Lecture 7 &amp; 8 W14 Tuesday 1404/02/23 Session 27 Runtime   environments and code generation Sunday 1404/02/28 Session 28 Advanced   topics (compiler applications in software engineering) W15 Tuesday 1404/02/30 Session 29 Students'   in-class presentations Sunday 1404/03/04 Session 30 Course   conclusion W16 Tuesday 1404/03/06 Sunday 1404/03/11 W17 Tuesday Sunday W18"},{"location":"language_applications/assignment_statement1main/","title":"Assignment statement grammar (version 1)","text":"<p>The main script for grammar AssignmentStatement1 (version 1)</p>"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--author","title":"author","text":"<p>Morteza Zakeri, https://m-zakeri.github.io</p>"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--date","title":"date","text":"<p>20201029</p>"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--required","title":"Required","text":"<ul> <li>Compiler generator:   ANTLR 4.x</li> <li>Target language(s):   Python 3.8.x</li> </ul>"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--changelog","title":"Changelog","text":""},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--v200","title":"v2.0.0","text":"<ul> <li>A lexer and parser for simple grammar without any attribute or listener</li> </ul>"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--refs","title":"Refs","text":"<ul> <li>Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/)</li> <li>Course website:   http://parsa.iust.ac.ir/courses/compilers/</li> <li>Laboratory website:   http://reverse.iust.ac.ir/</li> </ul>"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main.MyListener","title":"<code>MyListener</code>","text":"<p>               Bases: <code>AssignmentStatement1Listener</code></p> <p>A simple listener class</p> Source code in <code>language_apps/assignment_statement_v1/assignment_statement1main.py</code> <pre><code>class MyListener(AssignmentStatement1Listener):\n    \"\"\"\n    A simple listener class\n    \"\"\"\n\n    def __init__(self):\n        self._count = 0\n\n    def helper(self):\n        pass\n\n    def get_count(self):\n        return self._count\n\n    def exitFactor(self, ctx: AssignmentStatement1Parser.FactorContext):\n        # print('Dummy listener!')\n        # self._count += 1\n        pass\n\n    def enterExpr(self, ctx: AssignmentStatement1Parser.ExprContext):\n        if ctx.getChildCount() == 3:\n            if ctx.getChild(1).getText() == '+':\n                self._count += 1\n\n    def enterTerm(self, ctx: AssignmentStatement1Parser.TermContext):\n        pass\n\n    def exitNumber(self, ctx: AssignmentStatement1Parser.NumberContext):\n        pass\n</code></pre>"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main.main","title":"<code>main(args)</code>","text":"<p>The main driver to create and use the lexer and parser</p> <p>Args:</p> <pre><code>args (namespace):\n</code></pre> <p>return (None):</p> Source code in <code>language_apps/assignment_statement_v1/assignment_statement1main.py</code> <pre><code>def main(args):\n    \"\"\"\n    The main driver to create and use the lexer and parser\n\n    Args:\n\n        args (namespace):\n\n    return (None):\n\n    \"\"\"\n    # Step 1: Load input source into stream\n    stream = FileStream(args.file, encoding='utf8')\n    # input_stream = StdinStream()\n    # Step 2: Create an instance of AssignmentStLexer\n    lexer = AssignmentStatement1Lexer(stream)\n    # Step 3: Convert the input source into a list of tokens\n    token_stream = CommonTokenStream(lexer)\n    #\n    # quit()\n\n    # Step 4: Create an instance of the AssignmentStParser\n    parser = AssignmentStatement1Parser(token_stream)\n    # parser._interp.predictionMode = PredictionMode.SLL\n\n    # x = DescriptiveErrorListener()\n    # parser.addErrorListener()\n\n    # Step 5: Create parse tree\n    parse_tree = parser.start()\n\n    print(parse_tree.toStringTree())\n    # quit()\n\n    # Step 6: Create an instance of AssignmentStListener\n    my_listener = MyListener()\n    walker = ParseTreeWalker()\n    walker.walk(t=parse_tree, listener=my_listener)\n\n    print(f'Number of \"+\" operators: {my_listener.get_count()}')\n\n    # print(parse_tree.getText())\n    # quit()\n\n    # return\n    lexer.reset()\n    token = lexer.nextToken()\n    while token.type != Token.EOF:\n        print('Token text: ', token.text, 'Token line: ', token.line)\n        token = lexer.nextToken()\n</code></pre>"},{"location":"language_applications/assignment_statement2main/","title":"Assignment statement grammar (version 2)","text":""},{"location":"language_applications/assignment_statement2main/#tree-address-code-generation-pass","title":"Tree address code generation pass","text":"<p>ANTLR 4.x listener and visitor implementation for intermediate code generation (Three addresses code)</p> <p>@author: Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/) @date: 20201017</p> <ul> <li>Compiler generator:   ANTLR4.x</li> <li>Target language(s):     Python3.x,</li> </ul> <p>-Changelog: -- v2.1.0 --- Add support for AST intermediate representation using module <code>ast_pass</code> --- Change <code>compiler_pass</code> module to <code>three_address_code_pass</code> -- v2.0.0 --- Add attributes for grammar rules which are used to hold type and intermediate language_apps of rules.</p> <ul> <li>Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/)</li> <li>Course website:   http://parsa.iust.ac.ir/courses/compilers/</li> <li>Laboratory website:   http://reverse.iust.ac.ir/</li> </ul>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.three_address_code_pass.ThreeAddressCodeGenerator2Listener","title":"<code>ThreeAddressCodeGenerator2Listener</code>","text":"<p>               Bases: <code>AssignmentStatement2Listener</code></p> <p>Type checking and generating three address language_apps (optimizing number of temporary variables)</p> Source code in <code>language_apps/assignment_statement_v2/three_address_code_pass.py</code> <pre><code>class ThreeAddressCodeGenerator2Listener(AssignmentStatement2Listener):\n    \"\"\"\n    Type checking and generating three address language_apps (optimizing number of temporary variables)\n    \"\"\"\n\n    def __init__(self):\n        print('Listener2 call!')\n        self.temp_counter = 0\n\n    def create_temp(self):\n        self.temp_counter += 1\n        return 'T' + str(self.temp_counter)\n\n    def remove_temp(self):\n        self.temp_counter -= 1\n\n    def get_temp(self):\n        return 'T' + str(self.temp_counter)\n\n    @classmethod\n    def is_temp(cls, variable):\n        if variable[0] == 'T':\n            return True\n        return False\n\n    # ------------------\n    # Rule number\n    def exitNumber_float(self, ctx: AssignmentStatement2Parser.Number_floatContext):\n        ctx.type_attr = 'float'\n        ctx.value_attr = float(ctx.getText())\n\n    def exitNumber_int(self, ctx: AssignmentStatement2Parser.Number_intContext):\n        ctx.type_attr = 'int'\n        ctx.value_attr = int(ctx.getText())\n\n    # ------------------\n    # Rule factor\n    def exitFact_expr(self, ctx: AssignmentStatement2Parser.Fact_exprContext):\n        ctx.type_attr = ctx.expr().type_attr\n        ctx.value_attr = ctx.expr().value_attr\n\n    def exitFact_id(self, ctx: AssignmentStatement2Parser.Fact_idContext):\n\n        ctx.type_attr = 'string'\n        ctx.value_attr = ctx.getText()\n\n    def exitFact_number(self, ctx: AssignmentStatement2Parser.Fact_numberContext):\n        ctx.type_attr = ctx.number().type_attr\n        ctx.value_attr = ctx.number().value_attr\n\n    # ------------------\n    # Rule term\n    def exitTerm_fact_mutiply(self, ctx: AssignmentStatement2Parser.Term_fact_mutiplyContext):\n        if ctx.term().type_attr != ctx.factor().type_attr:\n            print('Semantic error: Cannot multiply {0} and {1}'.format(ctx.term().type_attr, ctx.factor().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.term().value_attr * ctx.factor().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.term().value_attr * ctx.factor().value_attr\n            else:\n                ctx.type_attr = 'string'\n                if self.is_temp(ctx.term().value_attr):\n                    ctx.value_attr = ctx.term().value_attr\n                    if self.is_temp(ctx.factor().value_attr):\n                        self.remove_temp()\n                elif self.is_temp(ctx.factor().value_attr):\n                    ctx.value_attr = ctx.factor().value_attr\n                else:\n                    ctx.value_attr = self.create_temp()\n            print('{0} = {1} * {2}'.format(ctx.value_attr, ctx.term().value_attr, ctx.factor().value_attr))\n\n    def exitTerm_fact_divide(self, ctx: AssignmentStatement2Parser.Term_fact_mutiplyContext):\n        if ctx.term().type_attr != ctx.factor().type_attr:\n            print('Semantic error: Cannot divide {0} and {1}'.format(ctx.term().type_attr, ctx.factor().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.term().value_attr / ctx.factor().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = int(ctx.term().value_attr / ctx.factor().value_attr)\n            else:\n                ctx.type_attr = 'string'\n                if self.is_temp(ctx.term().value_attr):\n                    ctx.value_attr = ctx.term().value_attr\n                    if self.is_temp(ctx.factor().value_attr):\n                        self.remove_temp()\n                elif self.is_temp(ctx.factor().value_attr):\n                    ctx.value_attr = ctx.factor().value_attr\n                else:\n                    ctx.value_attr = self.create_temp()\n            print('{0} = {1} / {2}'.format(ctx.value_attr, ctx.term().value_attr, ctx.factor().value_attr))\n\n    def exitFactor3(self, ctx: AssignmentStatement2Parser.Factor3Context):\n        ctx.type_attr = ctx.factor().type_attr\n        ctx.value_attr = ctx.factor().value_attr\n\n    # ------------------\n    # Rule expr\n    def exitExpr_term_plus(self, ctx: AssignmentStatement2Parser.Expr_term_plusContext):\n        if ctx.expr().type_attr != ctx.term().type_attr:\n            print('Semantic error: Cannot plus {0} and {1}'.format(ctx.expr().type_attr, ctx.term().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.expr().value_attr + ctx.term().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.expr().value_attr + ctx.term().value_attr\n            else:\n                ctx.type_attr = 'string'\n                if self.is_temp(ctx.expr().value_attr):\n                    ctx.value_attr = ctx.expr().value_attr\n                    if self.is_temp(ctx.term().value_attr):\n                        self.remove_temp()\n                elif self.is_temp(ctx.term().value_attr):\n                    ctx.value_attr = ctx.term().value_attr\n                else:\n                    ctx.value_attr = self.create_temp()\n            print('{0} = {1} + {2}'.format(ctx.value_attr, ctx.expr().value_attr, ctx.term().value_attr))\n\n    def exitExpr_term_minus(self, ctx: AssignmentStatement2Parser.Expr_term_minusContext):\n        if ctx.expr().type_attr != ctx.term().type_attr:\n            print('Semantic error: Cannot subtract {0} and {1}'.format(ctx.expr().type_attr, ctx.term().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.expr().value_attr - ctx.term().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.expr().value_attr - ctx.term().value_attr\n            else:\n                ctx.type_attr = 'string'\n                if self.is_temp(ctx.expr().value_attr):\n                    ctx.value_attr = ctx.expr().value_attr\n                    if self.is_temp(ctx.term().value_attr):\n                        self.remove_temp()\n                elif self.is_temp(ctx.term().value_attr):\n                    ctx.value_attr = ctx.term().value_attr\n                else:\n                    ctx.value_attr = self.create_temp()\n            print('{0} = {1} - {2}'.format(ctx.value_attr, ctx.expr().value_attr, ctx.term().value_attr))\n\n    def exitTerm4(self, ctx: AssignmentStatement2Parser.Term4Context):\n        ctx.type_attr = ctx.term().type_attr\n        ctx.value_attr = ctx.term().value_attr\n\n    # ------------------\n    # Rule expr\n    def exitAssign(self, ctx: AssignmentStatement2Parser.AssignContext):\n        ctx.type_attr = ctx.expr().type_attr\n        ctx.value_attr = ctx.expr().value_attr\n        print('Assign statement: \"{0} = {1}\"\\nAssign type: \"{2}\"'.format(ctx.ID().getText(), ctx.value_attr,\n                                                                         ctx.type_attr))\n</code></pre>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.three_address_code_pass.ThreeAddressCodeGenerator2Visitor","title":"<code>ThreeAddressCodeGenerator2Visitor</code>","text":"<p>               Bases: <code>AssignmentStatement2Visitor</code></p> <p>Type checking and generating three address language_apps (optimizing number of temporary variables) Utilizing ANTLR 4.x Visitor mechanism</p> Source code in <code>language_apps/assignment_statement_v2/three_address_code_pass.py</code> <pre><code>class ThreeAddressCodeGenerator2Visitor(AssignmentStatement2Visitor):\n    \"\"\"\n    Type checking and generating three address language_apps (optimizing number of temporary variables)\n    Utilizing ANTLR 4.x Visitor mechanism\n    \"\"\"\n\n    def __init__(self):\n        print('Visitor2 call!')\n        self.temp_counter = 0\n\n    def create_temp(self):\n        self.temp_counter += 1\n        return 'T' + str(self.temp_counter)\n\n    def remove_temp(self):\n        self.temp_counter -= 1\n\n    def get_temp(self):\n        return 'T' + str(self.temp_counter)\n\n    @classmethod\n    def is_temp(cls, variable):\n        if variable[0] == 'T':\n            return True\n        return False\n\n    def visitStart(self, ctx: AssignmentStatement2Parser.StartContext):\n        self.visit(tree=ctx.prog())\n\n    def visitProg(self, ctx: AssignmentStatement2Parser.ProgContext):\n        if ctx.getChildCount() == 2:\n            self.visit(tree=ctx.prog())\n        ctx.type_attr, ctx.value_attr = self.visit(tree=ctx.assign())\n        return ctx.type_attr, ctx.value_attr\n\n    def visitAssign(self, ctx: AssignmentStatement2Parser.AssignContext):\n        ctx.type_attr, ctx.value_attr = self.visit(tree=ctx.expr())\n        print('Assign statement: \"{0} = {1}\"\\nAssign type: \"{2}\"'.format(ctx.ID().getText(), ctx.value_attr,\n                                                                         ctx.type_attr))\n        return ctx.type_attr, ctx.value_attr\n\n    # ------------------\n    # Rule expr\n    def visitExpr_term_plus(self, ctx: AssignmentStatement2Parser.Expr_term_plusContext):\n        ctx.expr().type_attr, ctx.expr().value_attr = self.visit(tree=ctx.expr())\n        ctx.term().type_attr, ctx.term().value_attr = self.visit(tree=ctx.term())\n        if ctx.expr().type_attr != ctx.term().type_attr:\n            print('Semantic error: Cannot plus {0} and {1}'.format(ctx.expr().type_attr, ctx.term().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.expr().value_attr + ctx.term().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.expr().value_attr + ctx.term().value_attr\n            else:\n                ctx.type_attr = 'string'\n                if self.is_temp(ctx.expr().value_attr):\n                    ctx.value_attr = ctx.expr().value_attr\n                    if self.is_temp(ctx.term().value_attr):\n                        self.remove_temp()\n                elif self.is_temp(ctx.term().value_attr):\n                    ctx.value_attr = ctx.term().value_attr\n                else:\n                    ctx.value_attr = self.create_temp()\n            print('{0} = {1} + {2}'.format(ctx.value_attr, ctx.expr().value_attr, ctx.term().value_attr))\n            return ctx.type_attr, ctx.value_attr\n\n    def visitExpr_term_minus(self, ctx: AssignmentStatement2Parser.Expr_term_minusContext):\n        ctx.expr().type_attr, ctx.expr().value_attr = self.visit(tree=ctx.expr())\n        ctx.term().type_attr, ctx.term().value_attr = self.visit(tree=ctx.term())\n        if ctx.expr().type_attr != ctx.term().type_attr:\n            print('Semantic error: Cannot plus {0} and {1}'.format(ctx.expr().type_attr, ctx.term().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.expr().value_attr - ctx.term().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.expr().value_attr - ctx.term().value_attr\n            else:\n                ctx.type_attr = 'string'\n                if self.is_temp(ctx.expr().value_attr):\n                    ctx.value_attr = ctx.expr().value_attr\n                    if self.is_temp(ctx.term().value_attr):\n                        self.remove_temp()\n                elif self.is_temp(ctx.term().value_attr):\n                    ctx.value_attr = ctx.term().value_attr\n                else:\n                    ctx.value_attr = self.create_temp()\n            print('{0} = {1} - {2}'.format(ctx.value_attr, ctx.expr().value_attr, ctx.term().value_attr))\n            return ctx.type_attr, ctx.value_attr\n\n    def visitTerm4(self, ctx: AssignmentStatement2Parser.Term4Context):\n        ctx.type_attr, ctx.value_attr = self.visit(ctx.term())\n        return ctx.type_attr, ctx.value_attr\n\n    # ------------------\n    # Rule term\n    def visitTerm_fact_mutiply(self, ctx: AssignmentStatement2Parser.Term_fact_mutiplyContext):\n        ctx.term().type_attr, ctx.term().value_attr = self.visit(tree=ctx.term())\n        ctx.factor().type_attr, ctx.factor().value_attr = self.visit(tree=ctx.factor())\n        if ctx.term().type_attr != ctx.factor().type_attr:\n            print('Semantic error: Cannot multiply {0} and {1}'.format(ctx.term().type_attr, ctx.factor().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.term().value_attr * ctx.factor().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.term().value_attr * ctx.factor().value_attr\n            else:\n                ctx.type_attr = 'string'\n                if self.is_temp(ctx.term().value_attr):\n                    ctx.value_attr = ctx.term().value_attr\n                    if self.is_temp(ctx.factor().value_attr):\n                        self.remove_temp()\n                elif self.is_temp(ctx.factor().value_attr):\n                    ctx.value_attr = ctx.factor().value_attr\n                else:\n                    ctx.value_attr = self.create_temp()\n            print('{0} = {1} * {2}'.format(ctx.value_attr, ctx.term().value_attr, ctx.factor().value_attr))\n            return ctx.type_attr, ctx.value_attr\n\n    def visitTerm_fact_divide(self, ctx: AssignmentStatement2Parser.Term_fact_divideContext):\n        ctx.term().type_attr, ctx.term().value_attr = self.visit(tree=ctx.term())\n        ctx.factor().type_attr, ctx.factor().value_attr = self.visit(tree=ctx.factor())\n        if ctx.term().type_attr != ctx.factor().type_attr:\n            print('Semantic error: Cannot multiply {0} and {1}'.format(ctx.term().type_attr, ctx.factor().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.term().value_attr / ctx.factor().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = int(ctx.term().value_attr / ctx.factor().value_attr)\n            else:\n                ctx.type_attr = 'string'\n                if self.is_temp(ctx.term().value_attr):\n                    ctx.value_attr = ctx.term().value_attr\n                    if self.is_temp(ctx.factor().value_attr):\n                        self.remove_temp()\n                elif self.is_temp(ctx.factor().value_attr):\n                    ctx.value_attr = ctx.factor().value_attr\n                else:\n                    ctx.value_attr = self.create_temp()\n            print('{0} = {1} / {2}'.format(ctx.value_attr, ctx.term().value_attr, ctx.factor().value_attr))\n            return ctx.type_attr, ctx.value_attr\n\n    def visitFactor3(self, ctx: AssignmentStatement2Parser.Factor3Context):\n        ctx.type_attr, ctx.value_attr = self.visit(tree=ctx.factor())\n        return ctx.type_attr, ctx.value_attr\n\n    # ------------------\n    # Rule factor\n    def visitFact_expr(self, ctx: AssignmentStatement2Parser.Fact_exprContext):\n        return self.visit(tree=ctx.expr())\n\n    def visitFact_id(self, ctx: AssignmentStatement2Parser.Fact_idContext):\n        return 'string', ctx.ID().getText()\n\n    def visitFact_number(self, ctx: AssignmentStatement2Parser.Fact_numberContext):\n        return self.visit(tree=ctx.number())\n\n    # ------------------\n    # Rule number\n    def visitNumber_float(self, ctx: AssignmentStatement2Parser.Number_floatContext):\n        return 'float', float(ctx.FLOAT().getText())\n\n    def visitNumber_int(self, ctx: AssignmentStatement2Parser.Number_intContext):\n        return 'int', int(ctx.INT().getText())\n</code></pre>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.three_address_code_pass.ThreeAddressCodeGeneratorListener","title":"<code>ThreeAddressCodeGeneratorListener</code>","text":"<p>               Bases: <code>AssignmentStatement2Listener</code></p> <p>Type checking and generating three address language_apps (not optimized)</p> Source code in <code>language_apps/assignment_statement_v2/three_address_code_pass.py</code> <pre><code>class ThreeAddressCodeGeneratorListener(AssignmentStatement2Listener):\n    \"\"\"\n    Type checking and generating three address language_apps (not optimized)\n    \"\"\"\n\n    def __init__(self):\n        print('Listener call!')\n        self.temp_counter = 0\n\n    def create_temp(self):\n        self.temp_counter += 1\n        return 'T' + str(self.temp_counter)\n\n    # ------------------\n    # Rule number\n    def exitNumber_float(self, ctx: AssignmentStatement2Parser.Number_floatContext):\n        ctx.type_attr = 'float'\n        ctx.value_attr = float(ctx.getText())\n\n    def exitNumber_int(self, ctx: AssignmentStatement2Parser.Number_intContext):\n        ctx.type_attr = 'int'\n        ctx.value_attr = int(ctx.getText())\n\n    # ------------------\n    # Rule factor\n    def exitFact_expr(self, ctx: AssignmentStatement2Parser.Fact_exprContext):\n        ctx.type_attr = ctx.expr().type_attr\n        ctx.value_attr = ctx.expr().value_attr\n\n    def exitFact_id(self, ctx: AssignmentStatement2Parser.Fact_idContext):\n        ctx.type_attr = 'string'\n        ctx.value_attr = str(ctx.getText())\n\n    def exitFact_number(self, ctx: AssignmentStatement2Parser.Fact_numberContext):\n        ctx.type_attr = ctx.number().type_attr\n        ctx.value_attr = ctx.number().value_attr\n\n    # ------------------\n    # Rule term\n    def exitTerm_fact_mutiply(self, ctx: AssignmentStatement2Parser.Term_fact_mutiplyContext):\n        if ctx.term().type_attr != ctx.factor().type_attr:\n            print('Semantic error: Cannot multiply {0} and {1}'.format(ctx.term().type_attr, ctx.factor().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.term().value_attr * ctx.factor().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.term().value_attr * ctx.factor().value_attr\n            else:\n                ctx.type_attr = 'string'\n                ctx.value_attr = self.create_temp()\n            print('{0} = {1} * {2}'.format(ctx.value_attr, ctx.term().value_attr, ctx.factor().value_attr))\n\n    def exitTerm_fact_divide(self, ctx: AssignmentStatement2Parser.Term_fact_mutiplyContext):\n        if ctx.term().type_attr != ctx.factor().type_attr:\n            print('Semantic error: Cannot divide {0} and {1}'.format(ctx.term().type_attr, ctx.factor().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.term().value_attr / ctx.factor().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = int(ctx.term().value_attr / ctx.factor().value_attr)\n            else:\n                ctx.type_attr = 'string'\n                ctx.value_attr = self.create_temp()\n            print('{0} = {1} / {2}'.format(ctx.value_attr, ctx.term().value_attr, ctx.factor().value_attr))\n\n    def exitFactor3(self, ctx: AssignmentStatement2Parser.Factor3Context):\n        ctx.type_attr = ctx.factor().type_attr\n        ctx.value_attr = ctx.factor().value_attr\n\n    # ------------------\n    # Rule expr\n    def exitExpr_term_plus(self, ctx: AssignmentStatement2Parser.Expr_term_plusContext):\n        if ctx.expr().type_attr != ctx.term().type_attr:\n            print('Semantic error: Cannot plus {0} and {1}'.format(ctx.expr().type_attr, ctx.term().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.expr().value_attr + ctx.term().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.expr().value_attr + ctx.term().value_attr\n            else:\n                ctx.type_attr = 'string'\n                ctx.value_attr = self.create_temp()\n            print('{0} = {1} + {2}'.format(ctx.value_attr, ctx.expr().value_attr, ctx.term().value_attr))\n\n    def exitExpr_term_minus(self, ctx: AssignmentStatement2Parser.Expr_term_minusContext):\n        if ctx.expr().type_attr != ctx.term().type_attr:\n            print('Semantic error: Cannot subtract {0} and {1}'.format(ctx.expr().type_attr, ctx.term().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.expr().value_attr - ctx.term().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.expr().value_attr - ctx.term().value_attr\n            else:\n                ctx.type_attr = 'string'\n                ctx.value_attr = self.create_temp()\n            print('{0} = {1} - {2}'.format(ctx.value_attr, ctx.expr().value_attr, ctx.term().value_attr))\n\n    def exitTerm4(self, ctx: AssignmentStatement2Parser.Term4Context):\n        ctx.type_attr = ctx.term().type_attr\n        ctx.value_attr = ctx.term().value_attr\n\n    # ------------------\n    # Rule expr\n    def exitAssign(self, ctx: AssignmentStatement2Parser.AssignContext):\n        ctx.type_attr = ctx.expr().type_attr\n        ctx.value_attr = ctx.expr().value_attr\n        print('Assign statement: \"{0} = {1}\"\\nAssign type: \"{2}\"'.format(ctx.ID().getText(), ctx.value_attr,\n                                                                         ctx.type_attr))\n</code></pre>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.three_address_code_pass.ThreeAddressCodeGeneratorVisitor","title":"<code>ThreeAddressCodeGeneratorVisitor</code>","text":"<p>               Bases: <code>AssignmentStatement2Visitor</code></p> <p>Type checking and generating three address language_apps (not optimized regarding to the number of temporary variables) Utilizing ANTLR 4.x Visitor mechanism</p> Source code in <code>language_apps/assignment_statement_v2/three_address_code_pass.py</code> <pre><code>class ThreeAddressCodeGeneratorVisitor(AssignmentStatement2Visitor):\n    \"\"\"\n    Type checking and generating three address language_apps (not optimized regarding to the number of temporary variables)\n    Utilizing ANTLR 4.x Visitor mechanism\n    \"\"\"\n\n    def __init__(self):\n        print('Visitor call!')\n        self.temp_counter = 0\n\n    def create_temp(self):\n        self.temp_counter += 1\n        return 'T' + str(self.temp_counter)\n\n    def visitStart(self, ctx: AssignmentStatement2Parser.StartContext):\n        self.visit(tree=ctx.prog())\n\n    def visitProg(self, ctx: AssignmentStatement2Parser.ProgContext):\n        if ctx.getChildCount() == 2:\n            self.visit(tree=ctx.prog())\n        ctx.type_attr, ctx.value_attr = self.visit(tree=ctx.assign())\n        return ctx.type_attr, ctx.value_attr\n\n    def visitAssign(self, ctx: AssignmentStatement2Parser.AssignContext):\n        ctx.type_attr, ctx.value_attr = self.visit(tree=ctx.expr())\n        print('Assign statement: \"{0} = {1}\"\\nAssign type: \"{2}\"'.format(ctx.ID().getText(), ctx.value_attr,\n                                                                         ctx.type_attr))\n        return ctx.type_attr, ctx.value_attr\n\n    # ------------------\n    # Rule expr\n    def visitExpr_term_plus(self, ctx: AssignmentStatement2Parser.Expr_term_plusContext):\n        ctx.expr().type_attr, ctx.expr().value_attr = self.visit(tree=ctx.expr())\n        ctx.term().type_attr, ctx.term().value_attr = self.visit(tree=ctx.term())\n        if ctx.expr().type_attr != ctx.term().type_attr:\n            print('Semantic error: Cannot plus {0} and {1}'.format(ctx.expr().type_attr, ctx.term().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.expr().value_attr + ctx.term().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.expr().value_attr + ctx.term().value_attr\n            else:\n                ctx.type_attr = 'string'\n                ctx.value_attr = self.create_temp()\n            print('{0} = {1} + {2}'.format(ctx.value_attr, ctx.expr().value_attr, ctx.term().value_attr))\n            return ctx.type_attr, ctx.value_attr\n\n    def visitExpr_term_minus(self, ctx: AssignmentStatement2Parser.Expr_term_minusContext):\n        ctx.expr().type_attr, ctx.expr().value_attr = self.visit(tree=ctx.expr())\n        ctx.term().type_attr, ctx.term().value_attr = self.visit(tree=ctx.term())\n        if ctx.expr().type_attr != ctx.term().type_attr:\n            print('Semantic error: Cannot plus {0} and {1}'.format(ctx.expr().type_attr, ctx.term().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.expr().value_attr - ctx.term().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.expr().value_attr - ctx.term().value_attr\n            else:\n                ctx.type_attr = 'string'\n                ctx.value_attr = self.create_temp()\n            print('{0} = {1} - {2}'.format(ctx.value_attr, ctx.expr().value_attr, ctx.term().value_attr))\n            return ctx.type_attr, ctx.value_attr\n\n    def visitTerm4(self, ctx: AssignmentStatement2Parser.Term4Context):\n        ctx.type_attr, ctx.value_attr = self.visit(ctx.term())\n        return ctx.type_attr, ctx.value_attr\n\n    # ------------------\n    # Rule term\n    def visitTerm_fact_mutiply(self, ctx: AssignmentStatement2Parser.Term_fact_mutiplyContext):\n        ctx.term().type_attr, ctx.term().value_attr = self.visit(tree=ctx.term())\n        ctx.factor().type_attr, ctx.factor().value_attr = self.visit(tree=ctx.factor())\n        if ctx.term().type_attr != ctx.factor().type_attr:\n            print('Semantic error: Cannot multiply {0} and {1}'.format(ctx.term().type_attr, ctx.factor().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.term().value_attr * ctx.factor().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = ctx.term().value_attr * ctx.factor().value_attr\n            else:\n                ctx.type_attr = 'string'\n                ctx.value_attr = self.create_temp()\n            print('{0} = {1} * {2}'.format(ctx.value_attr, ctx.term().value_attr, ctx.factor().value_attr))\n            return ctx.type_attr, ctx.value_attr\n\n    def visitTerm_fact_divide(self, ctx: AssignmentStatement2Parser.Term_fact_divideContext):\n        ctx.term().type_attr, ctx.term().value_attr = self.visit(tree=ctx.term())\n        ctx.factor().type_attr, ctx.factor().value_attr = self.visit(tree=ctx.factor())\n        if ctx.term().type_attr != ctx.factor().type_attr:\n            print('Semantic error: Cannot multiply {0} and {1}'.format(ctx.term().type_attr, ctx.factor().type_attr))\n            quit(-1)\n        else:\n            if ctx.term().type_attr == 'float':\n                ctx.type_attr = 'float'\n                ctx.value_attr = ctx.term().value_attr / ctx.factor().value_attr\n            elif ctx.term().type_attr == 'int':\n                ctx.type_attr = 'int'\n                ctx.value_attr = int(ctx.term().value_attr / ctx.factor().value_attr)\n            else:\n                ctx.type_attr = 'string'\n                ctx.value_attr = self.create_temp()\n            print('{0} = {1} / {2}'.format(ctx.value_attr, ctx.term().value_attr, ctx.factor().value_attr))\n            return ctx.type_attr, ctx.value_attr\n\n    def visitFactor3(self, ctx: AssignmentStatement2Parser.Factor3Context):\n        ctx.type_attr, ctx.value_attr = self.visit(tree=ctx.factor())\n        return ctx.type_attr, ctx.value_attr\n\n    # ------------------\n    # Rule factor\n    def visitFact_expr(self, ctx: AssignmentStatement2Parser.Fact_exprContext):\n        return self.visit(tree=ctx.expr())\n\n    def visitFact_id(self, ctx: AssignmentStatement2Parser.Fact_idContext):\n        return 'string', ctx.ID().getText()\n\n    def visitFact_number(self, ctx: AssignmentStatement2Parser.Fact_numberContext):\n        return self.visit(tree=ctx.number())\n\n    # ------------------\n    # Rule number\n    def visitNumber_float(self, ctx: AssignmentStatement2Parser.Number_floatContext):\n        return 'float', float(ctx.FLOAT().getText())\n\n    def visitNumber_int(self, ctx: AssignmentStatement2Parser.Number_intContext):\n        return 'int', int(ctx.INT().getText())\n</code></pre>"},{"location":"language_applications/assignment_statement2main/#abstract-syntax-tree-ast-generation-pass","title":"Abstract syntax tree (AST) generation pass","text":"<p>ANTLR 4.x listener and visitor implementation for intermediate code generation (abstract syntax trees)</p> <p>@author: Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/) @date: 20201117</p> <ul> <li>Compiler generator:   ANTRL4.x</li> <li>Target language(s):   Python3.x,</li> </ul> <p>-Changelog: -- v2.1.0 --- Add support for AST visualization with dummy nodes --- Add support for AST intermediate representation using module <code>ast_pass</code> --- Change <code>compiler_pass</code> module to <code>three_address_code_pass</code> -- v2.0.0 --- Add attributes for grammar rules which are used to hold type and intermediate language_apps of rules.</p> <ul> <li>Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/)</li> <li>Course website:   http://parsa.iust.ac.ir/courses/compilers/</li> <li>Laboratory website:   http://reverse.iust.ac.ir/</li> </ul>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.abstract_syntax_tree_pass.ASTListener","title":"<code>ASTListener</code>","text":"<p>               Bases: <code>AssignmentStatement2Listener</code></p> Source code in <code>language_apps/assignment_statement_v2/abstract_syntax_tree_pass.py</code> <pre><code>class ASTListener(AssignmentStatement2Listener):\n    \"\"\"\n\n    \"\"\"\n    def __init__(self):\n        self.ast = AST()  # Data structure for holding the abstract syntax tree\n        self.q = queue.Queue()  # Use to print and visualize AST\n        self.g = nx.DiGraph()  # Use to visualize AST\n        # self.q.empty()\n        # print('Q=', )\n\n    def print_tree(self, node=None, level=1):\n        if node is None:\n            # print()\n            return\n        # if not self.q.empty():\n        #     print('Parent:', self.q.get().value)\n        # print('\\t'*level, end='')\n        print()\n        while node is not None:\n            current_node = node\n            print(current_node.value, end='')  # alt+196 = \u2500\u2500\u2500, alt+178=\u2593\n            if node.child is not None:\n                # self.q.put(node)\n                self.g.add_edge(current_node, node.child, edge_type='C', color='red')\n                self.q.put(node.child)\n            else:\n                tn = TreeNode(value='\u2593', child=None, brother=None)\n                self.g.add_edge(current_node, tn, edge_type='C', color='red')\n            node = node.brother\n            if node is not None:\n                print('\\t\u2500\u2500\u2500\\t', end='')\n                self.g.add_edge(current_node, node, edge_type='B', color='blue')\n            else:\n                tn = TreeNode(value='\u2593', child=None, brother=None)\n                self.g.add_edge(current_node, tn, edge_type='B', color='blue')\n\n        if not self.q.empty():\n            self.print_tree(node=self.q.get(), level=level + 1)\n\n    def print_tree2(self, node=None):\n        pass\n\n    def exitAssign(self, ctx: AssignmentStatement2Parser.AssignContext):\n        idPntr = self.ast.make_node(value=ctx.ID().getText(), child=None, brother=ctx.expr().value_attr)\n        assPntr = self.ast.make_node(value=\":=\", child=idPntr, brother=None)\n        ctx.value_attr = assPntr\n        self.ast.root = assPntr\n        self.print_tree(node=self.ast.root, level=1)\n\n    def exitExpr_term_plus(self, ctx: AssignmentStatement2Parser.Expr_term_plusContext):\n        self.ast.add_brother(ctx.expr().value_attr, ctx.term().value_attr)\n        exprPntr = self.ast.make_node(value=\"+\", child=ctx.expr().value_attr, brother=None)\n        ctx.value_attr = exprPntr\n\n    def exitExpr_term_minus(self, ctx: AssignmentStatement2Parser.Expr_term_plusContext):\n        self.ast.add_brother(ctx.expr().value_attr, ctx.term().value_attr)\n        exprPntr = self.ast.make_node(value=\"-\", child=ctx.expr().value_attr, brother=None)\n        ctx.value_attr = exprPntr\n\n    def exitTerm4(self, ctx: AssignmentStatement2Parser.Term4Context):\n        ctx.value_attr = ctx.term().value_attr\n\n    # ----------------------\n    def exitTerm_fact_mutiply(self, ctx: AssignmentStatement2Parser.Term_fact_mutiplyContext):\n        self.ast.add_brother(ctx.term().value_attr, ctx.factor().value_attr)\n        termPntr = self.ast.make_node(value=\"*\", child=ctx.term().value_attr, brother=None)\n        ctx.value_attr = termPntr\n\n    def exitTerm_fact_divide(self, ctx: AssignmentStatement2Parser.Term_fact_divideContext):\n        self.ast.add_brother(ctx.term().value_attr, ctx.factor().value_attr)\n        termPntr = self.ast.make_node(value=\"/\", child=ctx.term().value_attr, brother=None)\n        ctx.value_attr = termPntr\n\n    def exitFactor3(self, ctx: AssignmentStatement2Parser.Factor3Context):\n        ctx.value_attr = ctx.factor().value_attr\n\n    # ---------------------\n    def exitFact_expr(self, ctx: AssignmentStatement2Parser.Fact_exprContext):\n        ctx.value_attr = ctx.expr().value_attr\n\n    def exitFact_id(self, ctx: AssignmentStatement2Parser.Fact_idContext):\n        idPntr = self.ast.make_node(value=ctx.ID().getText(), child=None, brother=None)\n        ctx.value_attr = idPntr\n\n    def exitFact_number(self, ctx: AssignmentStatement2Parser.Fact_numberContext):\n        ctx.value_attr = ctx.number().value_attr\n\n    # ----------------------\n    def exitNumber_float(self, ctx: AssignmentStatement2Parser.Number_floatContext):\n        numberPntr = self.ast.make_node(value=ctx.FLOAT().getText(), child=None, brother=None)\n        ctx.value_attr = numberPntr\n\n    def exitNumber_int(self, ctx: AssignmentStatement2Parser.Number_intContext):\n        numberPntr = self.ast.make_node(value=ctx.INT().getText(), child=None, brother=None)\n        ctx.value_attr = numberPntr\n</code></pre>"},{"location":"language_applications/assignment_statement2main/#main-driver","title":"Main driver","text":"<p>Main script for grammar AssignmentStatement2 (version 2) Contains attributes for holding rule type and rule intermediate representations  (AST and Three-addresses codes)</p>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--author","title":"author","text":"<p>Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/)</p>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--date","title":"date","text":"<p>20201029</p> <ul> <li>Compiler generator:   ANTLR 4.x</li> <li>Target language(s):   Python 3.8.x</li> </ul>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--install-pygraphviz","title":"Install pygraphviz","text":"<p>To draw AST as a binary tree you need to install the pygraphviz 1- download and install graphviz (for Windows/ Linux) 2- add graphviz to system path 3- install pygraphviz using the following command  python -m pip install --global-option=build_ext  --global-option=\"-IC:\\Program Files\\Graphviz\\include\" --global-option=\"-LC:\\Program Files\\Graphviz\\lib\" pygraphviz</p>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--changelog","title":"Changelog","text":""},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--v211","title":"v2.1.1","text":"<ul> <li>Add visualization with Graphviz.</li> </ul>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--v210","title":"v2.1.0","text":"<ul> <li>Add support for AST intermediate representation using module <code>ast_pass</code></li> <li>Change <code>compiler_pass</code> module to <code>three_address_code_pass</code></li> </ul>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--v200","title":"v2.0.0","text":"<ul> <li>Add attributes for grammar rules which are used to hold type and intermediate language_apps of rules.</li> </ul>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--refs","title":"Refs","text":"<ul> <li>Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/)</li> <li>Course website: http://parsa.iust.ac.ir/courses/compilers/</li> <li>Laboratory website: http://reverse.iust.ac.ir/</li> </ul>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main.draw","title":"<code>draw(g=None)</code>","text":"<p>Draw abstract syntax tree</p> <p>Args:</p> <pre><code>g (nx.DiGraph) :\n</code></pre> <p>Returns:</p> <pre><code>None\n</code></pre> Source code in <code>language_apps/assignment_statement_v2/assignment_statement2main.py</code> <pre><code>def draw(g: nx.DiGraph = None):\n    \"\"\"\n\n    Draw abstract syntax tree\n\n    Args:\n\n        g (nx.DiGraph) :\n\n    Returns:\n\n        None\n\n    \"\"\"\n\n    pos = graphviz_layout(\n        G=g,\n        prog='dot',\n        # prog='circo',\n    )\n    # pos = hierarchy_pos(G=g,)\n\n    # pos = nx.kamada_kawai_layout(G=g)\n    # pos = nx.bipartite_layout(G=g, nodes=g.nodes)\n    # pos = nx.spectral_layout(G=g)\n    # pos = nx.spiral_layout(G=g)\n    # pos = nx.spiral_layout(G=g)\n\n    colors = [g[u][v]['color'] for u, v in g.edges]\n    nx.draw(g,\n            with_labels=False,\n            node_size=500,\n            node_color='black',\n            edge_color=colors,\n            pos=pos,\n            )\n    edge_labels = nx.get_edge_attributes(g, 'edge_type')\n    # print('#', edge_labels)\n    nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels, )\n\n    node_labels = {}\n    for node in g.nodes():\n        # set the node name as the key and the label as its value\n        node_labels[node] = node.value\n    nx.draw_networkx_labels(g, pos, node_labels, font_size=12, font_color='w')\n    plt.savefig('../../docs/figs/ast4.png')\n    plt.show()\n</code></pre>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main.draw_graphviz","title":"<code>draw_graphviz(g=None)</code>","text":"<p>Visualize abstract syntax tree with Graphviz</p> <p>Arges:</p> <pre><code> g (nx.DiGraph): The abstract syntax tree to be converted to the dot file\n</code></pre> <p>Returns:</p> <pre><code>None\n</code></pre> <p>References:</p> <pre><code>[1] https://graphviz.org/Gallery/directed/psg.html\n</code></pre> Source code in <code>language_apps/assignment_statement_v2/assignment_statement2main.py</code> <pre><code>def draw_graphviz(g: nx.DiGraph = None):\n    \"\"\"\n\n    Visualize abstract syntax tree with Graphviz\n\n    Arges:\n\n         g (nx.DiGraph): The abstract syntax tree to be converted to the dot file\n\n    Returns:\n\n        None\n\n    References:\n\n        [1] https://graphviz.org/Gallery/directed/psg.html\n\n    \"\"\"\n    pydot_graph = nx.drawing.nx_pydot.to_pydot(g)\n    # nx.drawing.nx_pydot.write_dot(func_graph, self.cfg_path + str(self.domain_name) + '.dot')\n\n    pydot_graph2 = pydot.Dot(\"\", graph_type=\"digraph\")\n    nid = 0\n    for u, v in g.edges:\n        if u.value == u'\\u2593':\n            # u.value = 'NULL'\n            node_u = pydot.Node(name=f'node_id_{nid}', label=u.value, shape='box')\n            nid += 1\n        else:\n            node_u = pydot.Node(name=u.value, label=u.value, shape='box')\n        if v.value == u'\\u2593':\n            # v.value = 'NULL'\n            node_v = pydot.Node(name=f'node_id_{nid}', label=v.value, shape='box')\n            nid += 1\n        else:\n            node_v = pydot.Node(name=v.value, label=v.value, shape='box')\n        print(u.value, v.value)\n\n        # edge_obj_dict = dict()\n        # edge_obj_dict.update({'color': g[u][v]['color']})\n        # edge_obj_dict.update({'label': g[u][v]['edge_type']})\n        edge_ = pydot.Edge(src=node_u, dst=node_v, color=g[u][v]['color'], label=g[u][v]['edge_type'])\n        pydot_graph2.add_node(node_u)\n        pydot_graph2.add_node(node_v)\n        pydot_graph2.add_edge(edge_)\n\n    pydot_graph2.write('../../docs/figs/ast2gv.dot', encoding='utf-8', )\n    # pydot_graph2.write_png('../../docs/figs/ast2.png')\n    result = subprocess.run(\n        ['dot', '-Tpng',\n         '../../docs/figs/ast2gv.dot',\n         '-o',\n         '../../docs/figs/ast2gv.png'\n         ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE\n    )\n    print(result.returncode)\n    error_ = result.stderr.decode('utf-8')\n    print(error_)\n</code></pre>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main.hierarchy_pos","title":"<code>hierarchy_pos(G, root=None, width=1.0, vert_gap=0.2, vert_loc=0, xcenter=0.5)</code>","text":"<p>From Joel's answer at https://stackoverflow.com/a/29597209/2966723. Licensed under Creative Commons Attribution-Share Alike</p> <p>If the graph is a tree this will return the positions to plot this in a hierarchical layout.</p> <p>Args:</p> <pre><code>G (nx.Graph): the graph (must be a tree)\n\nroot (nx.Node): the root node of current branch\n\n- if the tree is directed and this is not given, the root will be found and used\n\n- if the tree is directed and this is given, then the positions will be just for the descendants of this node.\n\n- if the tree is undirected and not given,then a random choice will be used.\n\nwidth (float): horizontal space allocated for this branch - avoids overlap with other branches\n\nvert_gap (float): gap between levels of hierarchy\n\nvert_loc (float): vertical location of root\n\nxcenter (float): horizontal location of root\n</code></pre> Source code in <code>language_apps/assignment_statement_v2/assignment_statement2main.py</code> <pre><code>def hierarchy_pos(G, root=None, width=1., vert_gap=0.2, vert_loc=0, xcenter=0.5):\n    \"\"\"\n    From Joel's answer at https://stackoverflow.com/a/29597209/2966723.\n    Licensed under Creative Commons Attribution-Share Alike\n\n    If the graph is a tree this will return the positions to plot this in a\n    hierarchical layout.\n\n    Args:\n\n        G (nx.Graph): the graph (must be a tree)\n\n        root (nx.Node): the root node of current branch\n\n        - if the tree is directed and this is not given, the root will be found and used\n\n        - if the tree is directed and this is given, then the positions will be just for the descendants of this node.\n\n        - if the tree is undirected and not given,then a random choice will be used.\n\n        width (float): horizontal space allocated for this branch - avoids overlap with other branches\n\n        vert_gap (float): gap between levels of hierarchy\n\n        vert_loc (float): vertical location of root\n\n        xcenter (float): horizontal location of root\n    \"\"\"\n\n    if not nx.is_tree(G):\n        raise TypeError('cannot use hierarchy_pos on a graph that is not a tree')\n\n    if root is None:\n        if isinstance(G, nx.DiGraph):\n            root = next(iter(nx.topological_sort(G)))  # allows back compatibility with nx version 1.11\n        else:\n            root = random.choice(list(G.nodes))\n\n    def _hierarchy_pos(G, root, width=1., vert_gap=0.2, vert_loc=0, xcenter=0.5, pos=None, parent=None):\n        \"\"\"\n\n        see hierarchy_pos docstring for most arguments\n\n        pos: a dict saying where all nodes go if they have been assigned\n        parent: parent of this branch. - only affects it if non-directed\n\n        \"\"\"\n\n        if pos is None:\n            pos = {root: (xcenter, vert_loc)}\n        else:\n            pos[root] = (xcenter, vert_loc)\n        children = list(G.neighbors(root))\n        if not isinstance(G, nx.DiGraph) and parent is not None:\n            children.remove(parent)\n        if len(children) != 0:\n            dx = width / len(children)\n            nextx = xcenter - width / 2 - dx / 2\n            for child in children:\n                nextx += dx\n                pos = _hierarchy_pos(\n                    G,\n                    child,\n                    width=dx,\n                    vert_gap=vert_gap,\n                    vert_loc=vert_loc - vert_gap, xcenter=nextx,\n                    pos=pos, parent=root\n                )\n        return pos\n\n    return _hierarchy_pos(G, root, width, vert_gap, vert_loc, xcenter)\n</code></pre>"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main.main","title":"<code>main(args)</code>","text":"<p>Create lexer and parser and execute AST listener</p> <p>Args:</p> <pre><code>args (Arg):\n</code></pre> <p>Returns:</p> <pre><code>None\n</code></pre> Source code in <code>language_apps/assignment_statement_v2/assignment_statement2main.py</code> <pre><code>def main(args):\n    \"\"\"\n    Create lexer and parser and execute AST listener\n\n    Args:\n\n        args (Arg):\n\n    Returns:\n\n        None\n\n    \"\"\"\n\n    # Step 1: Load input source into stream\n    stream = FileStream(args.file, encoding='utf8')\n    print('Input language_apps:\\n{0}'.format(stream))\n    print('Result:')\n\n    # Step 2: Create an instance of AssignmentStLexer\n    lexer = AssignmentStatement2Lexer(stream)\n\n    # Step 3: Convert the input source into a list of tokens\n    token_stream = CommonTokenStream(lexer)\n\n    # Step 4: Create an instance of the AssignmentStParser\n    parser = AssignmentStatement2Parser(token_stream)\n\n    # Step 5: Create parse tree\n    parse_tree = parser.start()\n\n\n    # Step 6: Create an instance of AssignmentStListener\n    code_generator_listener = ThreeAddressCodeGeneratorListener()\n    # code_generator_listener = ThreeAddressCodeGenerator2Listener()\n    ast_generator = ASTListener()\n\n    # Step 7(a): Walk parse tree with a customized listener (Automatically)\n    walker = ParseTreeWalker()\n    walker.walk(t=parse_tree, listener=code_generator_listener)  # or\n</code></pre>"},{"location":"language_applications/assignment_statement3main/","title":"Assignment statement grammar (version 3)","text":"<p>Main script for grammar AssignmentStatement3 (version 3) Contains semantic rules to perform type checking and semantic routines to generate intermediate representation (three addresses codes)</p>"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--author","title":"author","text":"<p>Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/)</p>"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--date","title":"date","text":"<p>20201028</p>"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--required","title":"Required","text":"<ul> <li>Compiler generator:   ANTLR 4.x</li> <li>Target language(s):   Python 3.8.x</li> </ul>"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--changelog","title":"Changelog","text":""},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--v30","title":"v3.0","text":"<ul> <li>Add semantic rules to perferm type checking</li> <li>Add semantic routines to generate intermediate representation (three addresses codes)</li> </ul>"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--refs","title":"Refs","text":"<ul> <li>Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/)</li> <li>Course website:   http://parsa.iust.ac.ir/courses/compilers/</li> <li>Laboratory website:   http://reverse.iust.ac.ir/</li> </ul>"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main.main","title":"<code>main(args)</code>","text":"<p>Create lexer and parser for language application</p> <p>Args:</p> <pre><code>args (string): command line arguments\nreturn (None):\n</code></pre> Source code in <code>language_apps/assignment_statement_v3/assignment_statement3main.py</code> <pre><code>def main(args):\n    \"\"\"\n    Create lexer and parser for language application\n\n    Args:\n\n        args (string): command line arguments\n        return (None):\n    \"\"\"\n\n    # Step 1: Load input source into stream\n    stream = FileStream(args.file, encoding='utf8')\n    # input_stream = StdinStream()\n    print('Input stream:')\n    print(stream)\n    print('Compiler result:')\n\n    # Step 2: Create an instance of AssignmentStLexer\n    lexer = AssignmentStatement3Lexer(stream)\n    # Step 3: Convert the input source into a list of tokens\n    token_stream = CommonTokenStream(lexer)\n    # Step 4: Create an instance of the AssignmentStParser\n    parser = AssignmentStatement3Parser(token_stream)\n    # Step 5: Create parse tree\n    parse_tree = parser.start()\n    # Step 6: Create an instance of AssignmentStListener\n    # my_listener = MyListener()\n    # walker = ParseTreeWalker()\n    # walker.walk(t=parse_tree, listener=my_listener)\n\n    quit()\n\n    lexer.reset()\n    token = lexer.nextToken()\n    while token.type != Token.EOF:\n        print('Token text: ', token.text, 'Token line: ', token.line)\n        token = lexer.nextToken()\n</code></pre>"},{"location":"language_applications/assignment_statement4main/","title":"Assignment statement grammar (version 4)","text":"<p>Main script for grammar AssignmentStatement4 (version 4) Contains semantic rules to perform type checking and semantic routines to generate intermediate representation (three addresses codes) Also, generates intermediate representation (three addresses codes) with minimum number of 'temp' variables</p>"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--author","title":"author","text":"<p>Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/)</p>"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--date","title":"date","text":"<p>20201029</p>"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--required","title":"Required","text":"<ul> <li>Compiler generator:   ANTLR 4.x</li> <li>Target language(s):   Python 3.8.x</li> </ul>"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--changelog","title":"Changelog","text":""},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--v40","title":"v4.0","text":"<ul> <li>Generate intermediate representation (three addresses codes) with minimum number of 'temp' variables</li> </ul>"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--v30","title":"v3.0","text":"<ul> <li>Add semantic rules to perferm type checking</li> <li>Add semantic routines to generate intermediate representation (three addresses codes)</li> </ul>"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--refs","title":"Refs","text":"<ul> <li>Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/)</li> <li>Course website:   http://parsa.iust.ac.ir/courses/compilers/</li> <li>Laboratory website:   http://reverse.iust.ac.ir/</li> </ul>"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main.main","title":"<code>main(args)</code>","text":"<p>Create lexer and parser for language application</p> <p>Args:</p> <pre><code>args (string): command line arguments\nreturn (None):\n</code></pre> Source code in <code>language_apps/assignment_statement_v4/assignment_statement4main.py</code> <pre><code>def main(args):\n    \"\"\"\n    Create lexer and parser for language application\n\n    Args:\n\n        args (string): command line arguments\n        return (None):\n    \"\"\"\n\n    # Step 1: Load input source into stream\n    stream = FileStream(args.file, encoding='utf8')\n    # input_stream = StdinStream()\n    print('Input stream:')\n    print(stream)\n    print('Compiler result:')\n\n    # Step 2: Create an instance of AssignmentStLexer\n    lexer = AssignmentStatement4Lexer(stream)\n    # Step 3: Convert the input source into a list of tokens\n    token_stream = CommonTokenStream(lexer)\n    # Step 4: Create an instance of the AssignmentStParser\n    parser = AssignmentStatement4Parser(token_stream)\n    # Step 5: Create parse tree\n    parse_tree = parser.start()\n    # Step 6: Create an instance of AssignmentStListener\n    # my_listener = MyListener()\n    # walker = ParseTreeWalker()\n    # walker.walk(t=parse_tree, listener=my_listener)\n\n    quit()\n    lexer.reset()\n    token = lexer.nextToken()\n    while token.type != Token.EOF:\n        print('Token text: ', token.text, 'Token line: ', token.line)\n        token = lexer.nextToken()\n</code></pre>"},{"location":"language_applications/main/","title":"Main","text":"<p>There are four language application in this repository </p> <ul> <li>assignment_statement_v1</li> <li>assignment_statement_v2</li> <li>assignment_statement_v3</li> <li>assignment_statement_v4</li> </ul> <p>The main module of IUST Compiler project.</p> <p>Refer to <code>language_apps</code> package to find classroom code snippets.</p>"},{"location":"language_applications/main/#main.Main","title":"<code>Main</code>","text":"<p>Welcome to Compiler course This file contains the main script for all code snippets</p> Source code in <code>main.py</code> <pre><code>class Main:\n    \"\"\"Welcome to Compiler course\n    This file contains the main script for all code snippets\n    \"\"\"\n\n    @classmethod\n    def print_welcome(cls, name) -&gt; None:\n        \"\"\"\n        Print welcome message\n        :param name:\n        :return:\n        \"\"\"\n        print(f'Welcome to our dragon course {name}.')\n\n    def tokenize_name(self, method_name):\n        method_name = 'getSdfsdfsdtudentNsdfdsfumber'\n\n        identifier_parts = list()\n        # First: split based-on CamelCase\n        matches = re.finditer('.+?(?:(?&lt;=[a-z])(?=[A-Z])|(?&lt;=[A-Z])(?=[A-Z][a-z])|$)', method_name)\n        camel_cases = [m.group(0) for m in matches]\n\n        # Second: split based-on underscore character '_'\n        for case in camel_cases:\n            case = case.lower()\n            case = case.split('_')\n            identifier_parts.extend(case)\n\n        print(f'Method name tokens {camel_cases}.')\n</code></pre>"},{"location":"language_applications/main/#main.Main.print_welcome","title":"<code>print_welcome(name)</code>  <code>classmethod</code>","text":"<p>Print welcome message :param name: :return:</p> Source code in <code>main.py</code> <pre><code>@classmethod\ndef print_welcome(cls, name) -&gt; None:\n    \"\"\"\n    Print welcome message\n    :param name:\n    :return:\n    \"\"\"\n    print(f'Welcome to our dragon course {name}.')\n</code></pre>"},{"location":"lectures/","title":"\ud83c\udf93 Compiler Design Course Pamphlets \ud83d\udcda","text":"<p>Welcome to our online pamphlets for the Compiler Design Course! \ud83c\udf89</p> <p>This repository contains a collection of pamphlets that I've created to help others understand some of the fundamental concepts in compiler design. I've taken the time to edit and rewrite these pamphlets in a more friendly and engaging way, making them easier to understand and more approachable to beginners. \ud83d\udcdd</p>"},{"location":"lectures/#-table-of-contents","title":"\ud83d\udcd6 Table of Contents","text":"<ol> <li> <p>Introduction</p> </li> <li> <p>Compiler Basic</p> </li> <li> <p>Lexical Analysis</p> </li> <li> <p>Syntax Analysis</p> </li> <li> <p>Top down</p> </li> <li> <p>Bottom up</p> </li> <li> <p>Translation Methods</p> </li> <li> <p>Semantic Analysis</p> </li> <li> <p>Intermediate-Code Generation</p> </li> <li> <p>Machine-Independent Optimizations</p> </li> <li> <p>Run-time Environments and Code</p> </li> </ol> <p>Embark on this educational journey, where you will not only gain theoretical knowledge but also acquire practical skills in building and understanding compilers. Let's explore the intricate world of compiler design together.</p>"},{"location":"lectures/#-contributing","title":"\ud83d\udc69\u200d\ud83d\udcbb Contributing","text":"<p>Your contributions are welcomed to these pamphlets! If you have any suggestions or improvements, please feel free to submit a pull request. I'm always looking for ways to improve these pamphlets and make them more helpful to others. \ud83d\ude4c</p>"},{"location":"lectures/00_Introduction/","title":"Introduction","text":""},{"location":"lectures/00_Introduction/#what-is-compiler","title":"What is Compiler?","text":"<p>A compiler is a software program that takes source code written in a high-level programming language and converts it into a lower-level language that can be executed by a machine. The output of this process is often referred to as object code or machine code.  The purpose of a compiler is to enable the execution of a program on a specific hardware platform.</p> <p>a simplified overview of how a compiler works:</p> <ol> <li> <p>Source Code: Programmers write human-readable code in a high-level programming language like C, C++, Java, or Python.</p> </li> <li> <p>Compilation: The source code is fed into a compiler. The compiler analyzes the code, performs various optimizations, and generates an equivalent machine code or intermediate code.</p> </li> <li> <p>Object Code/Executable: The output of the compiler is typically a binary file or set of files containing machine code or intermediate code. This can be directly executed by the computer's hardware or by a virtual machine, depending on the programming language.</p> </li> <li> <p>Linking (optional): In some languages, the compilation process involves linking, where multiple compiled files or libraries are combined to create the final executable.</p> </li> </ol> <p>The process of compiling code involves several steps:</p> <ol> <li> <p>Lexical Analysis: The compiler splits the source code into lexemes, which are individual code fragments that represent specific patterns in the code. The lexemes are then tokenized in preparation for syntax and semantic analyses.</p> </li> <li> <p>Syntax Analysis: The compiler verifies that the code's syntax is correct, based on the rules for the source language. This process is also referred to as parsing. During this step, the compiler typically creates abstract syntax trees that represent the logical structures of specific code elements.</p> </li> <li> <p>Semantic Analysis: The compiler verifies the validity of the code's logic. This step goes beyond syntax analysis by validating the code's accuracy. For example, the semantic analysis might check whether variables have been assigned the right types or have been properly declared.</p> </li> <li> <p>Intermediate Code Generation: After the code passes through all three analysis phases, the compiler generates an intermediate representation (IR) of the source code. The IR code makes it easier to translate the source code into a different format. However, it must accurately represent the source code in every respect, without omitting any functionality.</p> </li> <li> <p>Optimization: The compiler optimizes the IR code in preparation for the final code generation. The type and extent of optimization depends on the compiler. Some compilers let users configure the degree of optimization.</p> </li> <li> <p>Output Code Generation: The compiler generates the final output code, using the optimized IR code.</p> </li> </ol> <p>Compilers are important because they enable developers to write code in high-level programming languages, which are easier to understand and more human-readable than machine code. They also provide portability, as the machine code generated can be run on many different operating systems and hardware architectures. Additionally, compilers can provide programmer security by preventing memory-related errors, such as buffer overflows, by analyzing and optimizing the code.</p> <p>However, it's important to note that the code produced by a compiler is platform-dependent. This means that compiled code produces a machine-readable and machine-specific executable file that only the particular type of machine is able to execute. For example, code compiled on a Windows machine won\u2019t run on a Mac or Linux system without being recompiled.</p> <p>The main advantages of using a compiler include:</p> <ul> <li> <p>Performance: Compiled code often runs faster than interpreted code because it's already translated into machine code.</p> </li> <li> <p>Platform Independence: In some cases, the compiled code can be executed on different platforms without modification, especially if it's compiled to an intermediate code that's interpreted by a virtual machine.</p> </li> </ul>"},{"location":"lectures/00_Introduction/#compiled-code-vs-interpreted-code","title":"Compiled Code VS Interpreted Code","text":"<p>Compiled code and interpreted code represent two different approaches to executing computer programs. Here are the key differences between them:</p>"},{"location":"lectures/00_Introduction/#compiled-code","title":"Compiled Code:","text":"<ol> <li>Translation Process:</li> <li>Compilation: The entire source code is translated into machine code or an intermediate code by a compiler before execution.</li> <li> <p>Output: The result of compilation is often an executable file containing machine code that can be directly executed by the computer's hardware.</p> </li> <li> <p>Execution:</p> </li> <li>Direct Execution: The compiled code is executed directly by the computer's processor.</li> <li> <p>Performance: Generally, compiled code tends to be faster than interpreted code because it is pre-translated into machine code.</p> </li> <li> <p>Examples:</p> </li> <li> <p>Common examples of compiled languages include C, C++, and Rust.</p> </li> <li> <p>Portability:</p> </li> <li>Compiled code is often less portable than interpreted code, as the compiled binary may be specific to the architecture or platform for which it was compiled.</li> </ol>"},{"location":"lectures/00_Introduction/#interpreted-code","title":"Interpreted Code:","text":"<ol> <li>Translation Process:</li> <li>Interpretation: The source code is translated and executed line by line or statement by statement by an interpreter.</li> <li> <p>Output: No separate compilation step is required, and the source code is directly interpreted during execution.</p> </li> <li> <p>Execution:</p> </li> <li>Interpretation: The interpreter reads and executes the source code directly without generating an intermediate machine code or binary file.</li> <li> <p>Performance: Interpreted code can be slower than compiled code because it is translated on-the-fly during execution.</p> </li> <li> <p>Examples:</p> </li> <li> <p>Examples of interpreted languages include Python, JavaScript, and Ruby.</p> </li> <li> <p>Portability:</p> </li> <li>Interpreted code is often more portable as it can be run on any system with the appropriate interpreter. However, the interpreter itself needs to be available for each platform.</li> </ol> <p>in summary, A compiler translates a programming language (source language) into executable code (target language)</p> <p></p>"},{"location":"lectures/01_Compiler-Basic/","title":"Compiler Basic","text":""},{"location":"lectures/01_Compiler-Basic/#compiler-big-picture","title":"Compiler big picture","text":""},{"location":"lectures/01_Compiler-Basic/#goals","title":"goals","text":"<p>1- Correctness: generate correct machine codes 2- Effectiveness: optimized machine codes 3- Efficiency: reasonable translation time (&lt; \ud835\udc42(n^3))</p> <p>Correctness is crucial. It gets hard to debug a code with a broken compiler.</p> <p>Verified compilers: \u2219 Some compilers have been proven to generate correct code! (X. Leroy, Formal Certification of a Compiler Back End, POPL \u201906) \u2219 Requires formal methods.</p>"},{"location":"lectures/01_Compiler-Basic/#formalization","title":"formalization","text":"<p>In compiler design, it's essential to precisely describe the input (source code) and output (machine code) of a compiler.</p> <p>Input: Source code (programming language) Output: Assembly (machine) code Let's see how a programming language is described: \u2219 Tow essential ingredients of a language: 1- Syntax: which strings of symbols are valid expressions in the language? \u2219 Example (Java syntax): Uses a semicolon (\";\") to separate two statements 2- Semantics: what do valid expressions actually mean, or how do they behave?    \u2219 Example (Java semantics): What should be printed?  <pre><code>int z = 0;\nSystem.out.println(\"z+1=\" + z++);\n</code></pre></p>"},{"location":"lectures/01_Compiler-Basic/#describing-programming-language-syntax","title":"Describing Programming Language Syntax","text":"<p>Programming languages have two crucial components: syntax and semantics.</p>"},{"location":"lectures/01_Compiler-Basic/#syntax","title":"Syntax","text":"<p>Syntax defines which strings of symbols are valid expressions in the language. </p>"},{"location":"lectures/01_Compiler-Basic/#example-c","title":"Example (C++):","text":"<p><pre><code>int @; // Invalid statement\nint z = 0;  // Valid statement\nstd::cout &lt;&lt; \"z+1=\" &lt;&lt; z++ &lt;&lt; std::endl;  // Valid statement\n</code></pre> There are multiple ways to define a syntax such as listing all valid programs but this way leads to so many long valid programs and is not practical, natrual languages are easy to underestand but they are nformal, imprecise, vague, tedious, and repetitive. The best chois right now is using formal languages (and automata): \u2219 Branch of CS that formalizes the properties of \u201clanguages\u201d over strings and their syntax. \u2219 Pros: Well-documenting what programs a compiler should accept or reject.  \u2219 Pros: Developing compiler phases (lexer, parser, code generation) formally and automatically.</p> <p>You can see an example below using a BNF grammar: <pre><code>&lt;Statement&gt;       ::= &lt;AssignmentSt&gt;\n                   | &lt;ForST&gt;\n                   | '{' &lt;StatementList&gt; '}'\n                   | \u03b5\n\n&lt;AssignmentSt&gt;    ::= &lt;Id&gt; '=' &lt;Expr&gt;\n\n&lt;ForST&gt;           ::= 'for' &lt;Id&gt; '=' &lt;Expr&gt; 'to' &lt;Expr&gt; &lt;DoPart&gt;\n\n&lt;DoPart&gt;          ::= 'do' &lt;Statement&gt;\n\n&lt;StatementList&gt;   ::= &lt;Statement&gt; ';' &lt;StatementList&gt;\n                   | &lt;Statement&gt;\n</code></pre></p>"},{"location":"lectures/01_Compiler-Basic/#semantics","title":"Semantics","text":"<p>Semantics defines the meaning or behavior of valid expressions. We will discuss this aspect in the next chapters.</p>"},{"location":"lectures/01_Compiler-Basic/#compilation-steps","title":"Compilation Steps","text":"<p>Compiling a program involves several steps, and it's crucial to understand the process.</p> <p>Intermediate Representations (IRs):    - Compiler uses different program Intermediate Representations.    - These IRs facilitate necessary program manipulations (analysis, optimization, code generation).</p>"},{"location":"lectures/01_Compiler-Basic/#example-compilation-steps-in-c","title":"Example Compilation Steps in C++:","text":"<pre><code>if (b==0) a=b;\n</code></pre>"},{"location":"lectures/01_Compiler-Basic/#compiler-architecture","title":"Compiler Architecture:","text":""},{"location":"lectures/01_Compiler-Basic/#compiler-phases","title":"Compiler Phases","text":"<ul> <li> <p>Parser: Responsible for analyzing the syntactic structure of the source code and generating a parse tree or abstract syntax tree (AST).</p> </li> <li> <p>Scanner: Tokenizes the source code, breaking it down into a sequence of tokens for the parser to analyze. </p> </li> <li> <p>Type Checker: Ensures that the types of expressions and statements are consistent and adherent to the language's rules.</p> </li> <li> <p>Code Generators: Translate the intermediate representation (IR) or AST into machine code or another target language.</p> </li> <li> <p>Error Manager: Handles and reports errors that occur during the compilation process.</p> </li> </ul> <p>Note: The order of these components may vary depending on the specific compiler design</p> <p>Note: The parser acts as a \"driver\" in the compiler design, meaning it plays a central role in coordinating the compilation process.</p> <p>Note: It analyzes the syntax of the source code, constructs a hierarchical representation (such as a parse tree), and passes this structure to subsequent stages of the compiler.</p>"},{"location":"lectures/01_Compiler-Basic/#steps-in-system-context","title":"Steps in System Context:","text":"<ul> <li>Preprocessing: Expanding macros and collecting program sources.</li> <li>Compilation: Parsing the preprocessed source code and generating an intermediate representation.</li> <li>Linking: Joining together object files and resolving external references.</li> <li>Loading: Mapping virtual addresses to physical address space.</li> </ul>"},{"location":"lectures/01_Compiler-Basic/#unresolved-references","title":"Unresolved references:","text":"<p> This image illustrates how unresolved references (like function calls to external symbols) are handled during compilation and linking in a typical system:                  </p> <p>source code The function abs(int) is declared, not defined \u2014 it\u2019s assumed to be defined elsewhere, perhaps in another file or library.</p> <p>This introduces an unresolved reference during compilation.</p> <p>assembly code  The compiler translates abs(x) into a call _abs instruction.</p> <p>Since _abs is not defined in this file, the call is marked as an external symbol.</p> <p>The call instruction uses a relative address, which can't be resolved yet \u2014 it depends on where _abs will be located.</p> <p>object code  The E8 opcode is the CALL instruction in x86-64, which takes a 4-byte relative offset.</p> <p>The offset (00 00 00 00) is a placeholder \u2014 the actual address is unknown during object code generation.</p> <p>The linker will fill in the correct offset when _abs is resolved.</p> <p>Note: The compiler emits a relocation entry for _abs in the object file. The linker later:</p> <ol> <li> <p>Finds the address of _abs in the linked modules or libraries.</p> </li> <li> <p>Calculates the relative offset.</p> </li> <li> <p>Fills in the 4-byte placeholder in the call instruction.</p> </li> </ol>"},{"location":"lectures/01_Compiler-Basic/#output-of-the-linker","title":"Output of the linker:","text":"<p>  The linker at the end merges all the object codes form all the source codes into one unit as the final program to be loaded into memory.</p>"},{"location":"lectures/01_Compiler-Basic/#memory-layout-of-programs","title":"Memory layout of programs:","text":"<p>The memory layout of a program is typically divided into distinct segments:</p> <p>Text/Code Segment: Contains compiled machine code (instructions).</p> <p>Initialized Data Segment: Stores global and static variables with defined initial values.</p> <p>BSS (Uninitialized Data Segment): Holds global and static variables without initial values.</p> <p>Heap: Used for dynamically allocated memory (malloc, new). Grows upward.</p> <p>Stack: Stores function call frames, local variables, and return addresses. Grows downward.</p> <p>Command Line Arguments: Placed at the top of memory.</p> <p>The Static Memory Layout (code, data, BSS) is fixed at compile/link time, while the Dynamic Memory Layout (heap and stack) changes at runtime.</p>"},{"location":"lectures/01_Compiler-Basic/#compile-passes","title":"Compile Passes","text":"<p>Definition: A compile pass is a stage in the compilation process where the source program undergoes transformations specified by its phases, producing intermediate output.</p> <ul> <li>Single-pass Compiler: Scans the complete source code only once. Example: Pascal compiler.</li> <li>Multi-pass Compiler: Processes the source code multiple times, improving the code pass by pass until the final pass emits the final code.</li> </ul>"},{"location":"lectures/01_Compiler-Basic/#compiler-errors","title":"Compiler Errors","text":"<p>A good compiler assists the programmer in locating and tracking down errors:</p> <ul> <li>Compile-time Errors: Occur during program compilation.</li> <li>Lexical errors</li> <li>Syntactic errors</li> <li> <p>Semantic errors</p> </li> <li> <p>Run-time Errors: Occur while the program is running after being successfully compiled.</p> </li> <li>Crashes</li> <li>Logical errors</li> </ul>"},{"location":"lectures/01_Compiler-Basic/#types-of-errors-in-compilers","title":"Types of Errors in Compilers","text":"<p>Common programming errors can occur at various levels:</p> <ol> <li>Lexical Errors: Misspellings of identifiers, keywords, or operators.</li> <li>Syntactic Errors: Misplaced semicolons, extra or missing braces.</li> <li>Semantic Errors: Type mismatches between operators and operands.</li> <li>Logical Errors: Incorrect reasoning by the programmer.</li> </ol>"},{"location":"lectures/01_Compiler-Basic/#error-recovery","title":"Error Recovery","text":"<p>Definition: Error recovery is a process that takes action against errors to reduce negative effects as much as possible.</p> <p>Common error-recovery strategies for parser error handling:</p> <ol> <li>Panic-mode</li> <li>Phrase-level</li> <li>Error-productions</li> <li>Global-correction</li> </ol>"},{"location":"lectures/01_Compiler-Basic/#compiler-types","title":"Compiler Types","text":"<ul> <li>Decompiler: Translates from a low-level language to a higher-level one.</li> <li>Cross-compiler: Produces code for a different <code>CPU</code> or operating system.</li> <li>Transpiler (Source-to-Source Compiler): Translates between <code>high-level</code> languages.</li> <li>Bootstrap Compiler: Temporary compiler used for compiling a more permanent or better-optimized compiler.</li> <li>Compiler-compiler: Produces a compiler (or part of one) in a generic and reusable way (e.g., <code>ANTLR</code>, <code>FLEX</code>, <code>YACC</code>, <code>BISON</code>).</li> </ul>"},{"location":"lectures/01_Compiler-Basic/#compiler-alternatives","title":"Compiler alternatives","text":""},{"location":"lectures/01_Compiler-Basic/#interpreter","title":"Interpreter","text":"<p>An interpreter is another computer program like compiler that executes instructions written in a programming language immediately statement by statement.</p>"},{"location":"lectures/01_Compiler-Basic/#interpreter-vs-compiler","title":"Interpreter vs compiler","text":"<p>Interpreter: 1. Program execution is a part of interpretation process. 2. No compile time but more runtime. 3. Optimizations are not as robust as compilers. 4. Python, Ruby    </p> <p>Compiler:  1. Program execution is separate from the compilation. 2. Compiler time but faster execution 3. Robust optimization (sees the entire code upfront). 4. C, C++</p> <p>Note: Some language implementations provide both: Java, C#.</p>"},{"location":"lectures/01_Compiler-Basic/#ahead-of-time-aot-compilation","title":"Ahead-of-time (AOT) Compilation","text":"<p>A traditional compiler where translation occurs in a separate step from execution.</p>"},{"location":"lectures/01_Compiler-Basic/#just-in-time-jit-compilation","title":"Just-In-Time (JIT) Compilation","text":""},{"location":"lectures/01_Compiler-Basic/#definition","title":"Definition","text":"<p>The JIT compiler, or dynamic translator, reads bytecodes (in a bytecode-compiled system) in many sections (or in full, rarely) and compiles them dynamically into machine code.</p> <ul> <li>This can be done per-file, per-function, or even on any arbitrary code fragment.</li> <li>As a result, the program can run faster.</li> <li>Provides lazy/late compiling.</li> <li>The code can be compiled when it is about to be executed and then cached and reused later without needing to be recompiled.</li> </ul>"},{"location":"lectures/01_Compiler-Basic/#advantages-of-jit-compilation","title":"Advantages of JIT Compilation","text":"<ol> <li>Improved Performance:</li> <li> <p>JIT compilation can result in faster execution of the program since machine code is generated dynamically.</p> </li> <li> <p>Lazy Compilation:</p> </li> <li> <p>Compilation is deferred until the code is about to be executed. This allows for better optimization decisions based on runtime information.</p> </li> <li> <p>Caching and Reuse:</p> </li> <li> <p>Once code is compiled, it can be cached and reused in subsequent executions, reducing the need for repeated compilation.</p> </li> <li> <p>Adaptability:</p> </li> <li>JIT compilation allows for adaptability to the runtime environment, optimizing the code based on the specific characteristics of the executing system.</li> </ol> <p></p> <p>Just-In-Time (JIT) compilation is a technique employed in the field of compiler design to enhance the runtime performance of programs. Unlike traditional ahead-of-time (AOT) compilation, where source code is translated into machine code before execution, JIT compilation occurs dynamically during program execution. This approach combines elements of interpretation and compilation, seeking to leverage the advantages of both.</p> <p>In JIT compilation, the source code is initially translated into an intermediate representation, often referred to as bytecode or an intermediate language. This intermediate code is not directly executed by the hardware but is designed to be more portable and platform-independent than the original source code.</p> <p>During runtime, as the program is executed, the JIT compiler analyzes the intermediate code and translates it into machine code that is specific to the underlying hardware architecture. This process occurs on-the-fly, just before the corresponding code is executed, hence the term \"just-in-time.\" The generated machine code is then executed directly by the hardware, resulting in potentially improved performance compared to interpreting the original source or intermediate code.</p>"},{"location":"lectures/01_Compiler-Basic/#use-cases","title":"Use Cases","text":"<ul> <li>Bytecode-Compiled Systems:</li> <li> <p>Commonly used in virtual machines that execute bytecode, such as Java Virtual Machine (JVM) or Common Language Runtime (CLR) in .NET.</p> </li> <li> <p>Execution Speed Optimization:</p> </li> <li> <p>JIT compilation is employed to improve the execution speed of programs, especially in environments where interpretation of high-level code would be too slow.</p> </li> <li> <p>Memory Efficiency:</p> </li> <li>By compiling only the necessary portions of code during runtime, JIT compilation can contribute to more efficient memory usage.</li> </ul>"},{"location":"lectures/01_Compiler-Basic/#bootstrapping","title":"Bootstrapping <p>Definition: Bootstrapping is a technique where a simple program initiates a more complex system of programs. In the context of compilers, it often involves the process of writing a compiler for a programming language using another compiler written in the same language. This self-compiling process allows for the creation of more sophisticated compilers.</p> <p>Example: Consider the BIOS (Basic Input/Output System), which initializes and tests hardware, peripherals, and external memory devices when a computer boots. In compiler design, bootstrapping might involve writing a compiler for a language <code>A</code> using a compiler written in the same language <code>A</code>.</p>","text":""},{"location":"lectures/01_Compiler-Basic/#t-diagrams","title":"T-Diagrams:","text":"<p>Definition: T-diagrams are a graphical notation used to represent the relationships between different programming languages in the context of compiler design. The notation takes the form of <code>A ---(C)---&gt; B</code>, where a compiler written in language <code>C</code> processes source code in language <code>A</code> and produces executable code in language <code>B</code>.</p>"},{"location":"lectures/01_Compiler-Basic/#full-bootstrapping","title":"Full Bootstrapping:","text":"<p>Goal: The goal of full bootstrapping is to implement a compiler for language <code>X</code> on a machine <code>M</code>, given the presence of a language <code>C</code> compiler/assembler on the same machine.</p> <p>Steps:</p> <ol> <li>Write Compiler for Subset <code>X</code> in <code>C</code>:</li> <li>Begin by writing a compiler for a small part (subset) of language <code>X</code> using the language <code>C</code>. Compile this on machine <code>M</code>, where the <code>C</code> compiler already exists.</li> </ol> <p></p> <ol> <li>Compile Compiler for Sub <code>X</code>:</li> <li>Use the existing language <code>C</code> compiler to compile the compiler written in Step 1. This creates a compiler that can process the subset language <code>Sub X</code>.</li> </ol> <p></p> <ol> <li>Translate Subset Compiler (Sub X):</li> <li>Translate the subset compiler (Sub X) written in Step 1 into the subset language (Sub X). This is a necessary step in the process.</li> </ol> <p></p> <ol> <li>Compile Subset Compiler (Sub X):</li> <li>Compile the subset compiler (Sub X) from Step 3 using the compiler built in Step 2. This further refines the compiler for <code>Sub X</code>.</li> </ol> <p></p> <ol> <li>Extend Subset Language (Sub X):</li> <li>Extend the subset language (Sub X) compiler from Step 3 into a compiler for the full language <code>X</code>. Importantly, this step is still performed using only the subset language <code>Sub X</code>.</li> </ol> <p></p> <ol> <li>Compile Full Language Compiler:</li> <li>Finally, compile the full language <code>X</code> compiler using the compiler built in Step 4. At this point, you have a compiler for the full language <code>X</code> implemented on machine <code>M</code>.</li> </ol> <p></p> <p>This comprehensive process demonstrates the self-sustaining nature of bootstrapping, where each step builds upon the capabilities of the previous one, ultimately leading to the creation of a compiler for a more complex language.</p>"},{"location":"lectures/01_Compiler-Basic/#half-bootstrapping","title":"Half Bootstrapping:","text":"<p>Half bootstrapping in compiler design refers to a process where a new compiler for a language is created by leveraging an existing compiler for the same language on a different machine. This differs from full bootstrapping, which involves building a compiler from scratch without any prior compiler for that language. Half bootstrapping is also known as cross-compilation. \u2219 It is easier than the full bootstrapping. \u2219 To build the bootstrapped compiler, we need a cross compiler which could be run on the machine N to produce the compiler for the machine M.</p>"},{"location":"lectures/01_Compiler-Basic/#example","title":"example:","text":"<p>. We have a compiler for language A in machine M but we want to develope one compiler for machine N. We have:</p> <p></p> <p>step 1: implement A -&gt; N compiler in A. </p> <p>step 2: compiler on M. </p>"},{"location":"lectures/01_Compiler-Basic/#simple-bootstrapping","title":"Simple Bootstrapping:","text":"<p>Process: If a compiler or interpreter already exists for the language <code>A</code> on the target machine <code>M</code>, the process of bootstrapping is simplified.</p>"},{"location":"lectures/02_Lexical-Analysis/","title":"Lexical Analysis","text":""},{"location":"lectures/02_Lexical-Analysis/#lexical-analysis-goal","title":"Lexical Analysis Goal","text":"<p>Goal: The primary objective of lexical analysis is to partition an input string into meaningful elements called tokens.</p> <p>Tasks of Lexical Analyzer: 1. Recognize substrings corresponding to tokens. 2. Return tokens with their categories.</p> <p>Main Tasks: - Read input characters of the source program. - Group them into lexemes. - Produce, as output, a sequence of tokens for each lexeme in the source program.</p> <p>Output: - The output of lexical analysis is a stream of tokens, which serves as the input to the parser.</p>"},{"location":"lectures/02_Lexical-Analysis/#formal-languages","title":"Formal Languages","text":"<p>Definition: - A language over $ \u03a3 $ is a subset of $ \u03a3 $ (set of all words over $ \u03a3 $ ). - Alphabet $ \u03a3: A $ finite set of elements. - For the lexer: Characters. - For the parser: Token classes/symbol types. - Words (strings): Sequences of elements from the alphabet \u03a3. - Example: If $ \u03a3 = {\ud835\udc4e, \ud835\udc4f} $, then $ \u03a3 = {\\epsilon, \ud835\udc4e, \ud835\udc4f, \ud835\udc4e\ud835\udc4e, \ud835\udc4e\ud835\udc4f, \ud835\udc4f\ud835\udc4e, \ud835\udc4f\ud835\udc4f, \ud835\udc4e\ud835\udc4e\ud835\udc4e, \ud835\udc4e\ud835\udc4e\ud835\udc4f, \ud835\udc4e\ud835\udc4f\ud835\udc4e, ...} $ - Example of an infinite language over $\u03a3: \ud835\udc3f1 = {\ud835\udc4e\ud835\udc4f, \ud835\udc4e\ud835\udc4f\ud835\udc4e\ud835\udc4f, \ud835\udc4e\ud835\udc4f\ud835\udc4e\ud835\udc4f\ud835\udc4e\ud835\udc4f, ...} = {{(\ud835\udc4e\ud835\udc4f)}^\ud835\udc5b | \ud835\udc5b \u2265 1} $</p>"},{"location":"lectures/02_Lexical-Analysis/#basic-operations-on-formal-languages","title":"Basic operations on formal languages","text":""},{"location":"lectures/02_Lexical-Analysis/#_1","title":"","text":""},{"location":"lectures/02_Lexical-Analysis/#formal-languages-description-notations","title":"Formal Languages Description Notations","text":"<ol> <li>Sets: $ \ud835\udc3f1 = { {}\ud835\udc4e^{\ud835\udc5b} \ud835\udc4f | \ud835\udc5b \u2265 0 } $</li> <li>Grammars: <code>&lt;\ud835\udc34&gt; ::= \ud835\udc4e\ud835\udc34 | \ud835\udc4f</code></li> <li>Automata:</li> </ol>"},{"location":"lectures/02_Lexical-Analysis/#_2","title":"<ol> <li>Regular Expressions (Regex): Used only for regular languages.</li> <li>Example: \ud835\udc4e*</li> </ol>","text":""},{"location":"lectures/02_Lexical-Analysis/#formal-grammars","title":"Formal Grammars","text":"<p>Definition: - \ud835\udc3a = {\ud835\udc41, \u03a3, \ud835\udc43, \ud835\udc46}, where   - \ud835\udc41: A finite set \ud835\udc41 of nonterminal symbols, disjoint from the strings formed from \ud835\udc3a.   - \u03a3: A finite set of terminal symbols, disjoint from \ud835\udc41.   - \ud835\udc43: A finite set \ud835\udc43 of production rules, each rule of the form \ud835\udefc \u2192 \ud835\udefd.   - \ud835\udc46: A distinguished symbol \ud835\udc46 \u2208 \ud835\udc41, the start symbol.</p> <p>Convention: - Use small letters for terminals and capital letters for non-terminals or variables when writing grammar production rules.</p>"},{"location":"lectures/02_Lexical-Analysis/#regular-grammars","title":"Regular Grammars","text":"<p>Definition: - A grammar \ud835\udc3a = (\ud835\udc41, \u03a3, \ud835\udc43, \ud835\udc46) is right-linear if all productions are of the form:   - \ud835\udc34 \u2192 \ud835\udc65\ud835\udc35 | \ud835\udc65 | \ud835\udf16, where \ud835\udc34, \ud835\udc35 \u2208 \ud835\udc41 and \ud835\udc65 \u2208 \u03a3*</p> <p>Definition: - A grammar is left-linear if all productions are of the form:   - \ud835\udc34 \u2192 \ud835\udc35\ud835\udc65 | \ud835\udc65 | \ud835\udf16, where \ud835\udc34, \ud835\udc35 \u2208 \ud835\udc41 and \ud835\udc65 \u2208 \u03a3*</p> <p>Regular Grammar: - A regular grammar is one that is either right-linear or left-linear. </p>"},{"location":"lectures/02_Lexical-Analysis/#deterministic-finite-acceptor-dfa","title":"Deterministic Finite Acceptor (DFA)","text":"<p>Definition: A deterministic finite acceptor or <code>DFA</code> is defined by the quintuple $$ [ M = (Q, \\Sigma, \\delta, q_0, F) ] $$ where - ( Q ) is a finite set of internal states, - ( \\Sigma ) is a finite set of symbols called the input alphabet, - ( \\delta: Q \\times \\Sigma \\rightarrow Q ) is a total function called the transition function, - ( q_0 \\in Q ) is the initial state, - ( F \\subseteq Q ) is a set of final states.</p>"},{"location":"lectures/02_Lexical-Analysis/#non-deterministic-finite-acceptor-nfa","title":"Non-deterministic Finite Acceptor (NFA)","text":"<p>Definition: A non-deterministic finite acceptor or <code>NFA</code> is defined by the quintuple $$ [ M = (Q, \\Sigma, \\delta, q_0, F) ] $$ where - ( Q ) is a finite set of internal states, - ( \\Sigma ) is a finite set of symbols called the input alphabet, - ( \\delta: Q \\times (\\Sigma \\cup \\{\\varepsilon\\}) \\rightarrow 2^Q ) is a total function called the transition function, - ( q_0 \\in Q ) is the initial state, - ( F \\subseteq Q ) is a set of final states.</p>"},{"location":"lectures/02_Lexical-Analysis/#nfa-vs-dfa","title":"NFA vs. DFA","text":"<p>DFA Transition Function: ( \\delta: Q \\times \\Sigma \\rightarrow Q )</p> <p>NFA Transition Function: ( \\delta: Q \\times (\\Sigma \\cup \\{\\varepsilon\\}) \\rightarrow 2^Q )</p> <ul> <li><code>NFA</code> can have multiple transitions for one input in a given state.</li> <li><code>NFA</code> can have no transition for an input in a given state.</li> <li><code>NFA</code> can make a transition without consuming an input symbol (\u03bb or \u03b5-transition).</li> </ul>"},{"location":"lectures/02_Lexical-Analysis/#_3","title":"","text":""},{"location":"lectures/02_Lexical-Analysis/#computations-of-a-dfa","title":"Computations of a DFA","text":"<ul> <li>For each input string, there is exactly one path in a <code>DFA</code> (O(n)). $$ [ L(M) = { w \\in \\Sigma^ : \\delta^(q_0, w) \\in F } ] $$ </li> </ul>"},{"location":"lectures/02_Lexical-Analysis/#_4","title":"","text":""},{"location":"lectures/02_Lexical-Analysis/#computations-of-an-nfa-and-language-acceptance","title":"Computations of an NFA and Language Acceptance","text":"<ul> <li>For an input string, there are multiple possible computation paths in an <code>NFA</code> (O(2^n)). $$[ L(M) = { w \\in \\Sigma^ : \\delta^(q_0, w) \\cap F = \\emptyset } ] $$</li> </ul>"},{"location":"lectures/02_Lexical-Analysis/#_5","title":"","text":""},{"location":"lectures/02_Lexical-Analysis/#nfa-vs-dfa-implementation","title":"NFA vs. DFA Implementation","text":"<ul> <li><code>DFA</code>s are generally simpler to implement due to their deterministic nature.  #  <li><code>NFA</code>s may require additional mechanisms to handle non-deterministic transitions.</li> <li>Simulation of <code>NFA</code> requires tracking multiple possible states simultaneously.  #  <p>example: Construct NFA that accepts all strings of the form 0^k where \ud835\udc58 is a multiple of 2 or 3.</p> <p>solution: Step 1: Understand the Language We are working with strings made only of 0s, and we want to accept them if:</p> <p>k mod 2 = 0 or k mod 3 = 0 Accept: 00, 000, 0000, 000000, ...</p> <p>Step 2: Use Modulo Arithmetic We know that:</p> <p>Multiples of 2 mean the number of 0s is even.</p> <p>Multiples of 3 mean the number of 0s is divisible by 3.</p> <p>So we can build two machines:</p> <p>One that accepts if k % 2 == 0</p> <p>One that accepts if k % 3 == 0</p> <p>Then, combine both machines using \u03b5-transitions to form a union.</p> <p>Step 3: Design NFA for Multiples of 2 We build a 2-state loop:</p> <p>That means:</p> <p>Every 2 zeros, we return to the accepting state.</p> <p>Accepts: 00, 0000, 000000, ... Step 4: Design NFA for Multiples of 3</p> <p>That means:</p> <p>Every 3 zeros, we return to the accepting state.</p> <p>Accepts: 000, 000000, 000000000, ... </p> <p>Step 5: Add a Start State and Combine with \u03b5-transitions We introduce a new start state (state 0), and:</p> <p>Add \u03b5-transition to state 1 (begin the multiple-of-2 path)</p> <p>Add \u03b5-transition to state 3 (begin the multiple-of-3 path)</p> <p>That allows the NFA to nondeterministically choose either path.</p>"},{"location":"lectures/02_Lexical-Analysis/#_6","title":"","text":""},{"location":"lectures/02_Lexical-Analysis/#regular-expressions","title":"Regular expressions <p>Regular expressions are just a notation for some particular operations on languages.</p>","text":""},{"location":"lectures/02_Lexical-Analysis/#_7","title":"","text":""},{"location":"lectures/02_Lexical-Analysis/#_8","title":"<p>\u2219 Regular expressions are used to describe a regular languge (pattern) for a computing machine.  \u2219 Extended Regular-Expression Notation: A number of additional operators may appear as shorthands in regular expressions, to make it easier to express patterns (will be discussed later).</p>","text":""},{"location":"lectures/02_Lexical-Analysis/#manual-construction-of-lexers","title":"Manual Construction of Lexers","text":""},{"location":"lectures/02_Lexical-Analysis/#recognition-of-tokens","title":"Recognition of Tokens","text":"<p>The manual construction of a lexical analyzer involves several steps:</p> <ol> <li>Describe Lexical Patterns:</li> <li> <p>Define regular expressions (RE) to describe the lexical pattern of each token type.</p> </li> <li> <p>Construct NFAs:</p> </li> <li> <p>Create Non-deterministic Finite Automata (NFAs) for each regular expression.</p> </li> <li> <p>Convert NFAs to DFAs:</p> </li> <li> <p>Convert the NFAs to Deterministic Finite Automata (DFAs) for efficiency.</p> </li> <li> <p>Minimize DFA States:</p> </li> <li> <p>Minimize the number of states in the DFAs where possible.</p> </li> <li> <p>Construct Transition Diagrams:</p> </li> <li> <p>Build lexical analyzer transition diagrams from the DFAs.</p> </li> <li> <p>Implement Transition Diagrams:</p> </li> <li>Translate the transition diagrams into actual code for the lexical analyzer.</li> </ol>"},{"location":"lectures/02_Lexical-Analysis/#most-common-tokens","title":"Most common tokens","text":""},{"location":"lectures/02_Lexical-Analysis/#_9","title":"","text":""},{"location":"lectures/02_Lexical-Analysis/#transition-diagrams-notations","title":"Transition Diagrams: Notations","text":"<p>As an intermediate step, patterns are converted into stylized flowcharts called \"transition diagrams.\" These diagrams incorporate DFAs for recognizing tokens. If it's necessary to retract the forward pointer one position (i.e., the lexeme doesn't include the symbol that got us to the accepting state), a '*' is placed near that accepting state.</p> <p>To create a transition diagram for a lexical analyzer from regular-expression patterns, follow these steps:</p> <p>1. Define States: Represent each state as a node (circle) that summarizes the condition of the input scanned so far, tracking characters between the lexemeBegin and forward pointers. 2. Add Edges: Draw directed edges between states, labeled with a symbol or set of symbols. Each edge indicates a transition from one state to another based on the next input symbol. Ensure the diagram is deterministic (at most one edge per symbol from any state). 3. Designate Start State: Mark one state as the start state with an incoming edge labeled \"start.\" This is where scanning begins before any input is read. 4. Identify Accepting States: Mark states that indicate a lexeme has been found with a double circle. These are final states where a token and optional attribute value may be returned to the parser. 5. Handle Retraction (if needed): If the forward pointer needs to retract one position (i.e., the lexeme excludes the last symbol), place a * near the accepting state. 6. Attach Actions: Associate any required actions (e.g., returning a token) with accepting states. The resulting diagram visually represents the process of scanning input to match patterns, guiding the lexical analyzer to recognize tokens deterministically.</p>"},{"location":"lectures/02_Lexical-Analysis/#transition-diagram-examples","title":"Transition Diagram Examples:","text":"<ol> <li>Relational Operations (RELOPs):</li> <li>Diagram for recognizing relational operators like <code>&lt;</code>, <code>&lt;&gt;</code>, <code>=</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>==</code>, etc.</li> </ol> <ol> <li>Reserved Words and Identifiers:</li> <li>Diagram for recognizing reserved words and identifiers in the source code.</li> </ol> <ol> <li>Unsigned Numbers:</li> <li>Diagram for recognizing unsigned numerical values.</li> </ol> <p><pre><code>TOKEN getRelop()\n{\n   TOKEN retToken = new(RELOP);\n   while(1) { /* repeat character processing until a returnor failure occurs */\n      switch(state) {\n         case 0: c = nextChar();\n         if ( c == '&lt;' ) state = 1;\n         else if ( c == '=' ) state = 5;\n         else if ( c == '&gt;' ) state = 6;\n         else fail(); /* lexeme is not a relop */\n         break;\n         case 1: ...\n         ...\n         case 8: retract();\n         retToken.attribute = GT;\n         return(retToken);\n      }\n   }\n}\n</code></pre> Here we show the action for state 8. Because state 8 bears a *, we must retract the input pointer one position (i.e., put c back on the input stream). That task is accomplished by the function retract(). Since state 8represents the recognition of lexeme &gt;, we set the second component of the returned ob ject, which we suppose is named attribute, to GT, the code for this operator.</p>"},{"location":"lectures/02_Lexical-Analysis/#lexer-input-and-output","title":"Lexer Input and Output:","text":"<p>The lexical analyzer takes the source code as input and produces a stream of tokens as output. This token stream is then passed to the parser for further syntactic analysis.</p> <p></p>"},{"location":"lectures/02_Lexical-Analysis/#static-scope-and-block-structure","title":"Static Scope and Block Structure:","text":"<ul> <li>The scope of a declaration is implicitly determined by where it appears in the program.</li> <li>Code blocks group declarations and statements, often delimited by braces <code>{}</code> or keywords like <code>begin</code> and <code>end</code>.</li> </ul>"},{"location":"lectures/02_Lexical-Analysis/#static-scope-and-block-structure-in-c","title":"Static scope and block structure in C++","text":""},{"location":"lectures/02_Lexical-Analysis/#white-spaces","title":"White Spaces:","text":"<ul> <li>Whitespaces are defined as tokens using space characters (' '), tabs ('\\t), and end-of-line characters ('\\r', '\\n').</li> <li>In most languages, whitespaces and comments can occur between any two tokens and are generally ignored by the parser.</li> </ul>"},{"location":"lectures/02_Lexical-Analysis/#comments","title":"Comments:","text":"<ul> <li>Comments are detected and discarded by the lexer.</li> <li>They can be single-line or multi-line.</li> <li>Lexical analyzers always find the next non-whitespace, non-comment token.  </li> </ul> <p>example: Design a transition diagram to identify multi-line comments in Java (C++) programs. <pre><code>/* This is a\n* multi-line comments */\n</code></pre> solution: S0 (Start State): Initial state. Transition on / to S1. S1: Seen /, expecting * to start the comment. Transition on * to S2. Transition on any other character (not ) back to S0 (or a failure state, not shown for simplicity).  S2: Inside the comment, after seeing /. Transition on * to S3 (potential end of comment). Transition on any other character (including newlines) back to S2 (stay in comment). S3: Seen * inside comment, checking for / to end. Transition on / to S4 (accepting state, comment complete). Transition on any other character (including ) back to S2 (continue comment).     S4 (Accepting State): Comment complete (seen /). Double circle, no retraction needed (lexeme includes */). Action: Return token (e.g., COMMENT) to the parser. </p>"},{"location":"lectures/02_Lexical-Analysis/#_10","title":"","text":""},{"location":"lectures/02_Lexical-Analysis/#lexical-errors-and-error-recovery","title":"Lexical Errors and Error Recovery:","text":"<ul> <li>Lexical errors occur when no token pattern matches the remaining input.</li> <li>A \"panic mode\" recovery strategy involves deleting characters until a well-formed token is found.</li> <li>Other recovery actions include deleting, inserting, replacing, or transposing characters.</li> </ul>"},{"location":"lectures/02_Lexical-Analysis/#panic-mode-recovery","title":"Panic Mode Recovery","text":"<p>Panic mode recovery is one of the error recovery strategies used in compiler design. It is commonly used by most parsing methods. In this strategy, when an error is discovered, the parser discards input symbols one at a time until it finds a designated set of synchronizing tokens. These tokens are delimiters such as semicolons or ends, which indicate the end of an input statement.</p> <p>Here is a simple example of how panic mode recovery works:</p> <pre><code>int a, 5abcd, sum, $2;\n</code></pre> <p>In this case, the parser would discard the input symbols one at a time until it finds a synchronizing token (like a semicolon). However, this strategy may lead to semantic or runtime errors in further stages. The panic mode recovery process can be implemented in a high-level parsing function. This function is responsible for detecting parsing errors and re-synchronizing the input stream by skipping tokens until a suitable spot to resume parsing is found. For a grammar that ends statements with semicolons, the semicolon becomes the synchronizing token. Here is an example of a top-level parsing function that uses panic mode recovery:</p> <pre><code>static int doParsing(void){\n  initialize errorcounter to zero\n\n  WHILE TYPEOFTOKEN is not EOF DO\n     SWITCH TYPEOFTOKEN\n     CASE ID:       -- ID is in the FIRST set of assignment()\n        returnStatus = assignment()\n        break\n     CASE PRINT:    -- PRINT is in the FIRST set of print()\n        returnStatus = print()\n        break\n     CASE ...\n         -- Other cases can go here, for other statement types\n         break\n     DEFAULT:\n        eprintf(\"File %s Line %ld: Expecting %s or %s;\"\n           \" found: %s '%s'\",\n              filename,\n              LINENUMBER,\n              tokenType(ID),\n              tokenType(PRINT),\n              tokenType(TYPEOFTOKEN),\n              LEXEMESTR );\n        returnStatus = FALSE\n        break\n     END SWITCH\n\n     IF returnStatus is FALSE THEN\n        CALL panic()\n        increment errorcounter\n     ENDIF\n  END WHILE\n  return errorcounter\n}\n</code></pre> <p>In this example, each parsing function is a Boolean function. Each parsing function may succeed, in which case we continue parsing, or fail, in which case we stop parsing and return the failure indication to our parent function.</p>"},{"location":"lectures/02_Lexical-Analysis/#lexical-analysis-challenges","title":"Lexical Analysis Challenges:","text":"<p>\u2219 In Fortran, whitespace is insignificant:      \u2219 VAR1 is exactly the same as VA R1. \u2219 Philosophy: removing all the whitespace should not change the meaning of the program. :-D  \u2219 How does it make the task of scanner difficult? \u2219 DO 5 I = 1,25 Here DO is a keyword representing a loop. \u2219 DO 5 I = 1.25 Here DO5I is a variable assigned an integer (there is no loop).</p>"},{"location":"lectures/02_Lexical-Analysis/#lookahead","title":"Lookahead:","text":"<p>Lookahead is required to decide where one token ends and the next token begins. We would like to minimize lookahead. \u2219 Lookahead is always required, e.g., we need lookahead to disambiguate between == and =.  \u2219 PL/1: keywords are not reserved. \u2219 IF ELSE THEN THEN = ELSE; ELSE ELSE = THEN \u2219 This makes lexical analysis a bit more difficult \u2013 need to decide what is a variable name and what is a keyword, and so need to look at what\u2019s going on in the rest of the expression. \u2219 There are examples where PL/1 may require unbounded lookahead!</p>"},{"location":"lectures/02_Lexical-Analysis/#automatic-construction-of-lexers","title":"Automatic construction of Lexers","text":""},{"location":"lectures/02_Lexical-Analysis/#lexer-construction-steps","title":"Lexer Construction Steps","text":"<p>Input: Token Specifications - A list of regular expressions (RE) in priority order that define the patterns of tokens in a programming language.</p> <p>Output: Lexer - A program that reads an input stream and breaks it up into tokens based on the specified regular expressions.</p> <p>Algorithm:</p> <ol> <li>Convert REs into NFAs:</li> <li>Transform regular expressions into Non-deterministic Finite Automata (NFAs).</li> <li> <p>Each regular expression corresponds to an NFA that recognizes the language defined by that expression.</p> </li> <li> <p>Convert NFAs to DFA:</p> </li> <li>Convert NFAs to Deterministic Finite Automata (DFAs) for efficiency.</li> <li> <p>Create a DFA that accepts the same language as the original NFA.</p> </li> <li> <p>Convert DFA into Transition Table:</p> </li> <li>Create a transition table that represents the DFA.</li> <li>The table indicates the next state for each combination of current state and input symbol.</li> </ol>"},{"location":"lectures/02_Lexical-Analysis/#the-lexflex-tool","title":"The Lex/Flex Tool","text":"<p>Flex, also known as Fast Lexical Analyzer Generator, is a tool used for generating lexical analyzers, also known as \"scanners\" or \"lexers\". It was written in C around 1987 by Vern Paxson and is often used in conjunction with other tools like Berkeley Yacc parser generator or GNU Bison parser generator.</p>"},{"location":"lectures/02_Lexical-Analysis/#structure-of-lex-programs","title":"Structure of Lex Programs","text":"<p>The structure of a lex program is as follows:</p> <pre><code>{% manifest constants %}\ndeclarations\n%%\ntranslation rules\n%%\nauxiliary functions\n</code></pre> <ul> <li> <p>Manifest constants: These are enclosed in <code>{}</code> brackets and can include definitions for variables, regular definitions, and manifest constants.</p> </li> <li> <p>Declarations: These are the variable declarations that are used in the program.</p> </li> <li> <p>Translation rules: Each rule has the form: <code>Pattern { Action }</code>. Patterns are regular expressions, and actions are code fragments, typically in C.</p> </li> <li> <p>Auxiliary functions: These are additional functions that can be compiled separately and loaded with the lexical analyzer.</p> </li> </ul>"},{"location":"lectures/02_Lexical-Analysis/#lexical-analysis_1","title":"Lexical Analysis","text":"<p>Lexical analysis, or lexing, is the process of converting a sequence of characters into a sequence of tokens. The lexical analyzer takes in a stream of input characters and returns a stream of tokens.</p>"},{"location":"lectures/02_Lexical-Analysis/#running-a-lex-program","title":"Running a Lex Program","text":"<p>To run a lex program, you need to follow these steps:</p> <ol> <li> <p>Write an input file that describes the lexical analyzer to be generated. This file should be named <code>lex.l</code> and written in the lex language. The lex compiler transforms <code>lex.l</code> into a C program, in a file that is always named <code>lex.yy.c</code>.</p> </li> <li> <p>Compile the <code>lex.yy.c</code> file into an executable file. This can be done using the C compiler.</p> </li> <li> <p>Run the executable file. The output file will take a stream of input characters and produce a stream of tokens.</p> </li> </ol> <p>Here is an example of how to run a lex program:</p> <pre><code>flex filename.l # or flex filename.lex depending on the extension file is saved with\ngcc lex.yy.c\n./a.out\n</code></pre> <p>Then provide the input to the program if it is required. Press <code>Ctrl+D</code> or use some rule to stop taking inputs from the user.</p>"},{"location":"lectures/02_Lexical-Analysis/#advantages-and-disadvantages-of-flex","title":"Advantages and Disadvantages of Flex","text":"<p>Flex has several advantages:</p> <ul> <li>Efficiency: Flex-generated lexical analyzers are very fast and efficient, which can improve the overall performance of the programming language.</li> <li>Portability: Flex is available on many different platforms, making it easy to use on a wide range of systems.</li> <li>Flexibility: Flex is very flexible and can be customized to support many different types of programming languages and input formats.</li> <li>Easy to Use: Flex is relatively easy to learn and use, especially for programmers with experience in regular expressions.</li> </ul> <p>However, there are also some disadvantages:</p> <ul> <li>Steep Learning Curve: While Flex is relatively easy to use, it can have a steep learning curve for programmers who are not familiar with regular expressions.</li> <li>Limited Error Detection: Flex-generated lexical analyzers can only detect certain types of errors, such as syntax errors and misspelled keywords.</li> <li>Limited Support for Unicode: Flex has limited support for Unicode, which can make it difficult to work with non-ASCII characters.</li> <li>Code Maintenance: Flex-generated code can be difficult to maintain over time, especially as the programming language evolves and changes. This can make it challenging to keep the lexical analyzer up to date with the latest version of the language.</li> </ul> <p></p>"},{"location":"lectures/02_Lexical-Analysis/#lex-architecture","title":"Lex Architecture","text":"<ul> <li>Describes how Lex works in terms of token recognition.</li> <li>Lex takes a set of regular expressions and corresponding actions to create a lexer.</li> </ul> <p>How does lex work?</p> <p> Flex (a tool for generating lexical analyzers) works by converting a token specification into a deterministic finite automaton (DFA) to recognize patterns in input text and perform corresponding actions. Here's how it operates:</p> <p>1. Token Specification: The input to Flex includes a set of patterns (regular expressions) and associated actions. In the image, the token specification defines: ab \u2192 {Action 1} aab \u2192 {Action 2} a+ (one or more as) \u2192 {Action 3} 2. NFA Construction: Flex initially constructs a nondeterministic finite automaton (NFA) from the regular expressions. The NFA (left diagram) includes states (e.g., q0 to q9) with \u03b5-transitions (no input) and labeled transitions (e.g., a, b). Multiple paths (e.g., from q0 to q3 via \u03b5 and a, or to q7 via b) allow non-deterministic choices. 3. Conversion to DFA: The NFA is converted into a DFA (right diagram) to ensure determinism, where each state (e.g., s0 to s4) has at most one transition per input symbol (a or b). The DFA includes: Start state s0. Accepting states (e.g., s2 for aab). A trap state to handle invalid inputs. Transitions based on the input alphabet \u03a3 = {a, b}, as shown in the transition table. 4. Transition Table: The table defines state transitions: s0 \u2192 s1 on a, error on b. s1 \u2192 s3 on a, s2 on b. s2, s3, s4 lead to errors or self-loops (e.g., s4 \u2192 s4 on a). This guides the DFA through the input string.  5 .Input Processing: For an input like aab: Start at s0 \u2192 s1 (on a) \u2192 s3 (on a) \u2192 s2 (on b). s2 is an accepting state, triggering {Action 2} for aab. 6. Action Execution: When the DFA reaches an accepting state matching the longest prefix of the input (e.g., aab over ab), Flex executes the corresponding action (e.g., returning a token to the parser). Flex automates this process, generating C code for a lexical analyzer that uses the DFA to scan input, match patterns, and execute actions efficiently. The algorithm for each part is described in the following sections.</p>"},{"location":"lectures/02_Lexical-Analysis/#regular-expression-to-nfa","title":"Regular Expression to NFA","text":"<ul> <li>Illustrates the process of converting a regular expression to a Non-deterministic Finite Automaton (NFA).</li> <li>Each construct in the regular expression corresponds to a state transition in the NFA.</li> </ul>"},{"location":"lectures/02_Lexical-Analysis/#regular-expressions-re","title":"Regular Expressions (RE):","text":"<p>A regular expression is a concise way to describe a set of strings. It consists of: - Alphabet: A set of symbols (characters). - Operators: Specify operations to combine and manipulate sets of strings. - Special Symbols: Representing operations like concatenation, union, and closure.</p>"},{"location":"lectures/02_Lexical-Analysis/#nondeterministic-finite-automaton-nfa","title":"Nondeterministic Finite Automaton (NFA):","text":"<p>An NFA is a type of finite automaton that allows multiple transitions from a state on a given input symbol. It has states, transitions, and an initial and final state.</p>"},{"location":"lectures/02_Lexical-Analysis/#steps-to-convert-re-to-nfa","title":"Steps to Convert RE to NFA:","text":"<ol> <li>Base Cases: </li> <li>Empty String (\ud835\udf16): Create an NFA with two states (initial and final) and an \ud835\udf16 transition between them.</li> <li>Single Symbol (a): Create an NFA with two states, one initial and one final, with a transition labeled by the symbol.</li> </ol> <ol> <li>Concatenation (AB):</li> <li>If RE is AB, create NFAs for A and B.</li> <li> <p>Connect the final state of A to the initial state of B.</p> </li> <li> <p>Union (A | B):</p> </li> <li>If RE is A | B, create NFAs for A and B.</li> <li>Create a new initial state with \ud835\udf16 transitions to the initial states of A and B.</li> <li> <p>Create a new final state with \ud835\udf16 transitions from the final states of A and B.</p> </li> <li> <p>Kleene Closure (A^*): </p> </li> <li> <p>If RE is A*, create an NFA for A.</p> </li> <li>Add a new initial state with \ud835\udf16 transitions to the initial state of A and a \ud835\udf16 transition from the final state of A to the initial state of A.</li> </ol>"},{"location":"lectures/02_Lexical-Analysis/#example","title":"Example:","text":"<p>Let's convert the regular expression <code>(a|b)*abb</code> to an NFA:</p> <ol> <li>Base Cases:</li> <li><code>a</code>: NFA1 (States: 2, Initial: 1, Final: 2, Transition: 1-&gt;2 (a))</li> <li><code>b</code>: NFA2 (States: 2, Initial: 1, Final: 2, Transition: 1-&gt;2 (b))</li> <li> <p>\ud835\udf16: NFA\ud835\udf16 (States: 2, Initial: 1, Final: 2, Transition: 1-&gt;2 (\ud835\udf16))</p> </li> <li> <p>Concatenation (ab):</p> </li> <li> <p>Concatenate NFA1 and NFA2.</p> </li> <li> <p>Union (a|b):</p> </li> <li>Create a new initial state with \ud835\udf16 transitions to the initial states of NFA1 and NFA2.</li> <li> <p>Create a new final state with \ud835\udf16 transitions from the final states of NFA1 and NFA2.</p> </li> <li> <p>Kleene Closure ((a|b)*):</p> </li> <li>Add a new initial state with \ud835\udf16 transitions to the initial state of the union NFA.</li> <li> <p>Add a \ud835\udf16 transition from the final state of the union NFA to its initial state.</p> </li> <li> <p>Concatenation with <code>abb</code>:</p> </li> <li>Concatenate the Kleene Closure NFA with the NFA for <code>abb</code>.</li> </ol> <p>The resulting NFA accepts the language described by the regular expression.</p> <p></p>"},{"location":"lectures/02_Lexical-Analysis/#converting-nfa-to-dfa","title":"Converting NFA to DFA","text":"<ul> <li>For every NFA there exists an equivalent DFA that accepts the same set of strings.</li> <li>Demonstrates the algorithm to convert NFAs to DFAs.</li> <li>Each set of possible states in the NFA becomes one state in the DFA, resulting in a more efficient representation.  </li> </ul> <p>Algorithm: 1. Keep track of a set of all possible states in which the automaton could be, 2. View this finite set as one state of new automaton, 3. When processing if we see a set exactly the same as a set constructed earlier we mark it.</p> <p>example: </p> <ol> <li>Identify the NFA: </li> <li>The NFA has states {q0, q1, q2, q3, q4}, with q0 as the start state.  Transitions include:  </li> <li>q0 \u2192 q1 on a, q0 \u2192 q4 on b.</li> <li>q1 \u2192 q2 on a, q1 \u2192 q3 on \u03b5, q1 \u2192 q4 on a,b.</li> <li>q2 \u2192 q3 on b, q2 \u2192 q2 on \u03b5.</li> <li>q3 is an accepting state (marked with \u221a).</li> <li>q4 \u2192 q4 on a,b. \u03b5-transitions allow non-deterministic moves without input.</li> <li>Initialize DFA States: </li> <li>Start with the \u03b5-closure of the NFA's start state (q0). The \u03b5-closure of q0 (denoted {q0}) includes q0 and any states reachable via \u03b5-transitions.</li> <li>Compute \u03b5-closure({q0}): From q0, no \u03b5-transitions, so \u03b5-closure({q0}) = {q0}.</li> <li>Create Initial DFA State:</li> <li>The first DFA state is {q0}, the \u03b5-closure of the NFA start state.</li> <li>Mark {q0} as an unmarked state to process.</li> <li>Compute Transitions for Each Input Symbol:</li> <li>For each unmarked DFA state and each input symbol (a, b): Find the set of NFA states reachable by that symbol. Take the \u03b5-closure of that set to form a new DFA state.</li> <li>For {q0}: On a: Move to {q1} (from q0 \u2192 q1), then \u03b5-closure({q1}) = {q1, q2, q3} (since q1 \u2192 q3 via \u03b5, and q2 \u2192 q3 via \u03b5 from q2). On b: Move to {q4} (from q0 \u2192 q4), then \u03b5-closure({q4}) = {q4}.</li> <li>New DFA states: {q1, q2, q3} and {q4}.</li> <li>Process New States:</li> <li>Mark {q0} as processed.</li> <li>For {q1, q2, q3}: On a: Move to {q1, q2, q3} (q1 \u2192 q2, q2 \u2192 q3 via \u03b5), then \u03b5-closure = {q1, q2, q3}. On b: Move to {q3} (q2 \u2192 q3), then \u03b5-closure = {q3}.</li> <li>For {q4}: On a or b: Move to {q4}, then \u03b5-closure = {q4}.</li> <li>New states: {q3}, with {q1, q2, q3} and {q4} to process.</li> <li>Repeat Until All States Are Processed:</li> <li>For {q3} (accepting since q3 is accepting): On a or b: No transitions, so no new states.</li> <li>Mark all states ({q1, q2, q3}, {q4}, {q3}) as processed.</li> <li>Final DFA states: {q0}, {q1, q2, q3}, {q4}, {q3}.</li> <li>Define Transitions and Accepting States:</li> <li>Transitions (as shown): {q0} \u2192 {q1, q2, q3} on a. {q0} \u2192 {q4} on b. {q1, q2, q3} \u2192 {q1, q2, q3} on a. {q1, q2, q3} \u2192 {q3} on b. {q4} \u2192 {q4} on a,b. {q3} has no outgoing transitions.  </li> <li>Accepting states: {q1, q2, q3} and {q3} (contain q3).</li> </ol> <p></p>"},{"location":"lectures/02_Lexical-Analysis/#dfa-minimization","title":"DFA Minimization","text":"<p>DFA minimization is the process of converting a given Deterministic Finite Automaton (DFA) to an equivalent DFA with the minimum number of states. This process is also known as the optimization of DFA and uses a partitioning algorithm.  The steps to minimize a DFA are as follows:</p> <ol> <li> <p>Partitioning: Divide the set of states (Q) into two sets. One set will contain all final states and the other set will contain non-final states. This partition is called P0.</p> </li> <li> <p>Initialization: Initialize k = 1.</p> </li> <li> <p>Finding Pk: Find Pk by partitioning the different sets of Pk-1. In each set of Pk-1, take all possible pairs of states. If two states of a set are distinguishable, split the sets into different sets in Pk.</p> </li> <li> <p>Stopping Condition: Stop when Pk = Pk-1 (No change in partition).</p> </li> <li> <p>Merging States: All states of one set are merged into one. The number of states in the minimized DFA will be equal to the number of sets in Pk.</p> </li> </ol> <p>Two states (qi, qj) are distinguishable in partition Pk if for any input symbol a, \u03b4(qi, a) and \u03b4(qj, a) are in different sets in partition Pk-1.</p> <p>Here's a pseudocode example of Hopcroft's algorithm, one of the algorithms for DFA minimization:</p> <pre><code>P := {F, Q \\ F}\nW := {F, Q \\ F}\nwhile (W is not empty) do\n   choose and remove a set A from W\n   for each c in \u03a3 do\n       let X be the set of states for which a transition on c leads to a state in A\n       for each set Y in P for which X \u2229 Y is nonempty and Y \\ X is nonempty do\n           replace Y in P by the two sets X \u2229 Y and Y \\ X\n           if Y is in W\n               replace Y in W by the same two sets\n           else\n               if |X \u2229 Y| &lt;= |Y \\ X|\n                  add X \u2229 Y to W\n               else\n                  add Y \\ X to W\n</code></pre> <p>This algorithm starts with a partition that is too coarse: every pair of states that are equivalent according to the Nerode congruence belong to the same set in the partition, but pairs that are inequivalent might also belong to the same set. It gradually refines the partition into a larger number of smaller sets, at each step splitting sets of states into pairs of subsets that are necessarily inequivalent.</p> <p>Advantages of DFA minimization include reduced complexity, optimal space utilization, improved performance, and language equivalence. However, it also has some disadvantages such as increased computational complexity, additional design and analysis effort, loss of readability, and it's limited to deterministic automata.</p> <p>In Sammary: Intuition: - Two DFA states are equivalent if all subsequent behavior from those states is the same.</p> <p>Procedure: 1. Create a table of state pairs. 2. Mark cells where one state is final and the other is non-final. 3. Mark pairs where transitions on the same symbol lead to marked pairs. 4. Repeat step 3 until no unmarked pairs remain. 5. Merge unmarked states to achieve a minimized DFA.</p> <p>example: </p>"},{"location":"lectures/02_Lexical-Analysis/#resolving-ambiguities-in-lexers","title":"Resolving Ambiguities in Lexers","text":"<p>Regular Expression Ambiguity: Ambiguity arises when regular expressions can match input in multiple ways. - Consider the following rules for example: \u2219 Keywords: if, else, while, for, ... \u2219 Identifiers: letter (letter | digit)* - What is the desired output for elsevier? \u2219 ID(elsevier) \u2219 KEYWORD(else) + ID(vier)   </p> <p>Conflict Resolution in Lex: 1. Longest/Maximal Match Rule:    - Prefer a longer prefix over a shorter one. 2. Declaration Priority:    - If the longest prefix matches multiple patterns, prefer the one listed first in the Lex program.</p> <p>example: Consider the following specification of tokens: 1. b(bc)^* 2. c^*(ba)^* 3. acb 4. a^+ 5. acb*  </p> <p>Tokenize the given input: aaabbcbcbaacbcaacbcba.</p> <p></p>"},{"location":"lectures/02_Lexical-Analysis/#the-flex-manual","title":"The Flex Manual","text":"<ul> <li>An essential reference for using the Flex tool.</li> <li>It provides detailed information on the Flex tool and its capabilities.</li> <li>Refer to the provided appendix slide for in-depth insights.</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/","title":"Syntax Analysis","text":""},{"location":"lectures/03_Syntax-Analysis/#_1","title":"<p>When an input string (source code or a program in some language) is given to a compiler, the compiler processes it in several phases, starting from <code>lexical analysis</code> (As mentioned scans the input and divides it into tokens) to target code generation.</p> <p><code>Syntax analysis</code> or <code>parsing</code> constitutes the second phase within the compiler's workflow. This chapter delves into fundamental concepts crucial for constructing a parser. </p>","text":""},{"location":"lectures/03_Syntax-Analysis/#syntax-analysis-aim-and-scope","title":"Syntax Analysis Aim and Scope","text":"<ul> <li>Syntax analysis validates the syntax of the source code written in a programming language using a component called parser.</li> <li>Testing for membership whether \ud835\udc64 (a source code) belongs to \ud835\udc3f(\ud835\udc3a) (a programming language with grammar G) is just a \u201cyes\u201d or \u201cno\u201d answer.</li> <li>However the syntax analyzer in a compiler must</li> <li>generate the syntax tree,</li> <li>handle errors gracefully if string is not in the language.</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#syntax-analysis-goal","title":"Syntax analysis goal","text":"<ul> <li>The parser uses the stream of the tokens produced by the lexical analyzer to create a tree-like intermediate representation that depicts the grammatical structure of the token stream.</li> <li>The parser reports any syntax errors in an intelligible fashion and recovers from commonly occurring errors to continue processing the remainder of the program.</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#syntax-analysis-prerequisites","title":"Syntax analysis prerequisites","text":"<ul> <li>What we need for syntax analysis:</li> <li>An expressive description technique: describe the syntax,</li> <li>An acceptor mechanism: determine if input token stream satisfies the syntax description.</li> <li>For lexical analysis:</li> <li>Regular expressions describe tokens,</li> <li>Finite Automata is acceptor for regular expressions.</li> <li>Why not use regular expressions (on tokens) to specify programming language syntax?</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#limitation-of-regular-expressions-for-syntax-analysis","title":"Limitation of regular expressions for syntax analysis","text":"<ul> <li>General-purpose programming languages like C, C++, C#, Java, etc. are not regular languages, so they cannot be described by regular expressions.</li> <li>Consider nested constructs (blocks, expressions, statements):</li> <li>((a+b)^*(c-d)/e)</li> <li>if (a&gt;0) { if (a==b) {print(a);}}</li> <li>The sytnax of the \u2019{\u2019 construct in the second code snippet can be described with a language like \ud835\udc3f ={\ufe00\ud835\udc4e^\ud835\udc5b\ud835\udc4f^\ud835\udc5b|\ud835\udc5b \u2265 0}\ufe00 which is a context-free language not regular.</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#non-context-free-language-constructs","title":"Non-context-free language constructs","text":"<ul> <li>Some constructs found in typical programming languages cannot be specified using CFG grammars alone.</li> <li>Declaration of identifiers before their use: \ud835\udc3f1 ={\ufe00\ud835\udc64\ud835\udc50\ud835\udc64|\ud835\udc64 \u2208 (\ud835\udc4e|\ud835\udc4f)^*}\ufe00</li> <li>Checking that the number of formal parameters in the declaration of a function agrees with the number of actual parameters: \ud835\udc3f2 ={\ufe00\ud835\udc4e^\ud835\udc5b\ud835\udc4f^\ud835\udc5a\ud835\udc50^\ud835\udc5b\ud835\udc51^\ud835\udc5a|\ud835\udc5b \u2265 1 \u2227 \ud835\udc5a \u2265 1}</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#syntax-analysis-scope","title":"Syntax analysis scope","text":"<ul> <li>What syntax analysis cannot do:</li> <li>To check whether variables are of types on which operations are allowed,</li> <li>To check whether a variable has been declared before use,</li> <li>To check whether a variable has been initialized</li> <li>These issues will be handled in semantic analysis phase. example</li> <li>Parsing only checks syntax correctness.</li> <li>Several important inspections are deferred until later phases.</li> <li>e.g. semantic analysis is responsible for type checking</li> <li>A program with the correct syntax (but incorrect semantics): <pre><code>int x = true; // type not agree\nint y; // variable not initialized\nx = (y &lt; z); // variable not declared\n</code></pre></li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#context-free-grammars","title":"Context-free grammars","text":"<p>As previously explored, a <code>lexical analyzer</code> proficiently identifies tokens using regular expressions and pattern rules. However, its capacity is constrained when it comes to scrutinizing the syntax of a given sentence, particularly in tasks involving balancing tokens like parentheses. To overcome this limitation, <code>syntax analysis</code> employs <code>context-free grammar</code> (CFG), a construct recognized by push-down automata.</p> <p><code>Syntax analysis</code>, often referred to as <code>parsing</code>, is a critical phase in compiler design where the compiler assesses whether the source code aligns with the grammatical rules of the programming language. Typically occurring as the second stage in the compilation process, following <code>lexical analysis</code>, the primary objective is to generate a parse tree or <code>abstract syntax tree</code> (AST). This hierarchical representation mirrors the grammatical structure of the program encapsulated in the source code.</p>"},{"location":"lectures/03_Syntax-Analysis/#programming-languages-grammar","title":"Programming languages grammar","text":"<ul> <li>Context-free grammar (CFG) is used instead of regular grammar (expressions) to precisely describe the syntactic properties of the programming languages.</li> <li>A specification of the balanced-parenthesis language using context-free grammar: <pre><code>\ud835\udc46 \u2192 (\ud835\udc46)\n\ud835\udc46 \u2192 \ud835\udc46\ud835\udc46 | \u03b5\n</code></pre></li> <li>If a grammar accepts a string, there is a derivation of that string using the rules of the grammar: <pre><code>\ud835\udc46 \u21d2 \ud835\udc46\ud835\udc46 \u21d2 (\ud835\udc46)\ud835\udc46 \u21d2 ((\ud835\udc46))\ud835\udc46 \u21d2 (())\ud835\udc46 \u21d2 (())(\ud835\udc46) \u21d2 (())()\n</code></pre></li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#context-free-grammars-for-programming-languages","title":"Context-free grammars for programming languages","text":"<p>A context-free grammar (CFG) is a type of grammar where every production rule is of the form <code>A \u2192 \u03b1</code>, where <code>A</code> is a single non-terminal and <code>\u03b1</code> is a string of terminals and/or non-terminals.  definition <pre><code>A grammar \ud835\udc3a ={\ufe00\ud835\udc49, \ud835\udc47, \ud835\udc46, \ud835\udc43}\ufe00   \nis said to be context-free if all productions in \ud835\udc43 have the form \ud835\udc3f\ud835\udc3b\ud835\udc46 \u2192 \ud835\udc45\ud835\udc3b\ud835\udc46, where \ud835\udc3f\ud835\udc3b\ud835\udc46 \u2208 \ud835\udc49 \ud835\udc4e\ud835\udc5b\ud835\udc51 \ud835\udc45\ud835\udc3b\ud835\udc46 \u2208$(\ufe00\ud835\udc49 \u222a \ud835\udc47)\ufe00^*$  \n\ud835\udc49 : A finite set \ud835\udc49 of nonterminal symbols, that is disjoint with the strings formed from \ud835\udc3a, syntactic variables   \n\ud835\udc47: A finite set of terminal symbols that is disjoint from \ud835\udc49 , token or \ud835\udf16  \n\ud835\udc46: A distinguished symbol \ud835\udc46 \u2208 \ud835\udc49 that is the start symbol, also\ncalled the sentence symbol.\n</code></pre> - Production rule specifies how non-terminals can be expanded. - A derivation in \ud835\udc3a starts from the starting symbol \ud835\udc46. - Each step replaces a non-terminal with one of its right hand sides. - Language L(G) of a grammar G: \u2219 The set of all strings of terminals derived from the start symbol. \ud835\udc46 \u2192 \ud835\udc4e\ud835\udc46\ud835\udc4f | \ud835\udc46\ud835\udc46 | \u03b5 \u2219 \ud835\udc3f ={\ufe00\ud835\udc64 \u2208 {\ud835\udc4e, \ud835\udc4f}^*: \ud835\udc5b\ud835\udc4e(\ud835\udc64) = \ud835\udc5b\ud835\udc4f(\ud835\udc64) \u2227 \ud835\udc5b\ud835\udc4e(\ud835\udc63) \u2265 \ud835\udc5b\ud835\udc4f(\ud835\udc63), \ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52 \ud835\udc63 \ud835\udc56\ud835\udc60 \ud835\udc4e\ud835\udc5b\ud835\udc66 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc53 \ud835\udc56\ud835\udc65 \ud835\udc5c\ud835\udc53 \ud835\udc64}\ufe00.  </p> <ul> <li>Every regular grammar is context-free, so a regular language is also a context-free one.</li> </ul> <p>So, let's to Learn...</p> <p>CFG, on the other hand, is a superset of Regular Grammar, as depicted below:</p> <p></p> <ul> <li> <p>Inductively build a production rule for each regular expression operator: </p> </li> <li> <p>where \u2219 \ud835\udc3a1 is grammar for \ud835\udc451 with the start symbol \ud835\udc461, \u2219 \ud835\udc3a2 is grammar for \ud835\udc452 with the start symbol \ud835\udc462.</p> </li> </ul> <p>example  Show that \ud835\udc3f ={\ufe00\ud835\udc4e^\ud835\udc5b\ud835\udc4f^\ud835\udc5a: \ud835\udc5b \u2260 \ud835\udc5a}\ufe00 is a contex free language. solution  We want to construct a CFG for the language \ud835\udc3f={\ud835\udc4e^\ud835\udc5b\ud835\udc4f^\ud835\udc5a\u2223\ud835\udc5b\u2260\ud835\udc5a}  We can split this into two disjoint CFLs:  1. \ud835\udc3f={\ud835\udc4e^\ud835\udc5b\ud835\udc4f^\ud835\udc5a\u2223\ud835\udc5b&gt;\ud835\udc5a}  2. \ud835\udc3f={\ud835\udc4e^\ud835\udc5b\ud835\udc4f^\ud835\udc5a\u2223\ud835\udc5b&lt;\ud835\udc5a}</p> <p>then L=L1 \u222a L2.  Since context-free languages are closed under union, if both  \ud835\udc3f1 and \ud835\udc3f2 are CFLs, then so is L.</p> <p> </p> <p>All productions are of the correct form \ud835\udc34\u2192\ud835\udefc, so this is a context-free grammar.</p> <p>Important Points</p> <p>Here are a few key points to remember about associativity:</p> <ul> <li>All operators with the same precedence have the same associativity. This is necessary because it helps the compiler decide the order of operations when an expression has two operators of the same precedence.</li> <li>The associativity of postfix and prefix operators is different. The associativity of postfix is left to right, while the associativity of prefix is right to left.</li> <li>The comma operator has the lowest precedence among all operators. It's important to use it carefully to avoid unexpected results.</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#precedence","title":"Precedence","text":"<p>Precedence is like a rule that helps us decide which operation to perform first when two different operators share a common operand. For example, in the expression <code>2+3*4</code>, both addition and multiplication are operators that share the operand <code>3</code>. </p> <p>By setting precedence among operators, we can easily decide which operation to perform first. Mathematically, multiplication (*) has precedence over addition (+), so the expression <code>2+3*4</code> will always be interpreted as <code>(2 + (3 * 4))</code>.</p>"},{"location":"lectures/03_Syntax-Analysis/#left-recursion","title":"Left Recursion","text":"<p>Left recursion is a situation where a grammar has a non-terminal that appears as the left-most symbol in its own derivation. This can cause problems for top-down parsers, which start parsing from the start symbol and can get stuck in an infinite loop when they encounter the same non-terminal in their derivation.</p> <p>For example, consider the following grammar:</p> <ol> <li><code>A =&gt; A\u03b1 | \u03b2</code></li> <li><code>S =&gt; A\u03b1 | \u03b2</code> <code>A =&gt; Sd</code></li> </ol> <p>The first example is an example of immediate left recursion, where <code>A</code> is any non-terminal symbol and <code>\u03b1</code> represents a string of non-terminals. The second example is an example of indirect left recursion.</p> <p></p> <p>By understanding and managing precedence and left recursion, we can make sure that our compiler can correctly parse and evaluate expressions.</p>"},{"location":"lectures/03_Syntax-Analysis/#understanding-associativity","title":"Understanding Associativity","text":"<p>Associativity is like a rule that helps us decide the order of operations when an operand has operators on both sides. If the operation is left-associative, the operand will be taken by the left operator. If it's right-associative, the right operator will take the operand. </p> <p>Left Associative Operations</p> <p>Operations like Addition, Multiplication, Subtraction, and Division are left associative. This means that when an expression contains more than one of these operations, the operations are performed from left to right. </p> <p>For example, if we have an expression like <code>id op id op id</code>, it will be evaluated as <code>(id op id) op id</code>. To illustrate, consider the expression <code>(id + id) + id</code>.</p> <p>Right Associative Operations</p> <p>Operations like Exponentiation are right associative. This means that when an expression contains more than one of these operations, the operations are performed from right to left. </p> <p>For example, if we have an expression like <code>id op (id op id)</code>, it will be evaluated as <code>id op (id op id)</code>. To illustrate, consider the expression <code>id ^ (id ^ id)</code>.</p>"},{"location":"lectures/03_Syntax-Analysis/#parsing-and-ambiguity","title":"Parsing and Ambiguity","text":""},{"location":"lectures/03_Syntax-Analysis/#leftmost-and-rightmost-derivations","title":"Leftmost and rightmost derivations","text":"<p>In the world of compiler design, there are two types of derivations that we often encounter: left-most and right-most derivations. These are like the two sides of a coin, each with its own unique characteristics.  - In a CFG grammar that is not linear, a derivation may involve sentential forms with more than one variable. - In such cases, we have a choice in the order in which variables are replaced.  </p> <p>Example of Derivation (Parse) Trees</p> <p>Consider the following grammar and string:</p> <ul> <li>Grammar: <code>E \u2192 E + E | E * E | -E | (E) | id</code></li> <li>String: <code>-(id + id)</code></li> </ul> <p>The leftmost derivation for this grammar and string is:</p> <pre><code>E \u21d2 -E \u21d2 -(E) \u21d2 -(E + E) \u21d2 -(id + E) \u21d2 -(id + id)\n</code></pre> <p>The rightmost derivation for the same grammar and string is:</p> <pre><code>E \u21d2 -E \u21d2 -(E) \u21d2 -(E + E) \u21d2 -(E + id) \u21d2 -(id + id)\n</code></pre> <p>Both derivations result in the same parse tree. Production Rules</p> <p>Let's start with some production rules. These are like the recipes that our compiler follows to understand and process the input string. Here are some example production rules:</p> <pre><code>E \u2192 E + E\nE \u2192 E * E\nE \u2192 id \n</code></pre> <p>And here's the input string that we'll be working with: <code>id + id * id</code></p> <p>Left-most Derivation</p> <p>Now, let's see how the compiler would process this input string using a left-most derivation. This is like saying, \"Hey compiler, let's start from the left and work our way to the right.\" Here's how it looks:</p> <pre><code>E \u2192 E * E\nE \u2192 E + E * E\nE \u2192 id + E * E\nE \u2192 id + id * E\nE \u2192 id + id * id\n</code></pre> <p>Notice that the left-most non-terminal is always processed first. It's like the compiler is saying, \"I'll handle the leftmost thing first, then move on to the next one on the left.\"</p> <p>Right-most Derivation</p> <p>Now, let's see how the compiler would process the same input string using a right-most derivation. This is like saying, \"Hey compiler, let's start from the right and work our way to the left.\" Here's how it looks:</p> <pre><code>E \u2192 E + E\nE \u2192 E + E * E\nE \u2192 E + E * id\nE \u2192 E + id * id\nE \u2192 id + id * id\n</code></pre> <p>And that's it! We've now explored both left-most and right-most derivations. Remember, these are just the two sides of a coin. Depending on the parsing strategy that the compiler uses, it might prefer one side over the other.</p>"},{"location":"lectures/03_Syntax-Analysis/#understanding-parse-trees","title":"Understanding Parse Trees","text":"<p>Parse trees are like a roadmap for your compiler. They are graphical representations of a derivation, showing how strings are derived from the start symbol. The start symbol becomes the root of the parse tree, and it's great to visualize this process.</p> <p>A parse tree is a tree structure that represents the syntactic structure of a string according to some grammar. In the context of a CFG, a parse tree is a derivation or parse tree for <code>G</code> if and only if it has the following properties:</p> <ul> <li>The root is labeled <code>S</code>.</li> <li>Every leaf has a label from <code>T \u222a {\u03b5}</code>.</li> <li>Every interior vertex (a vertex that is not a leaf) has a label from <code>V</code>.</li> <li>If a vertex has label <code>A \u2208 V</code>, and its children are labeled (from left to right) <code>a1, a2, ..., an</code>, then <code>P</code> must contain a production of the form <code>A \u2192 a1a2...an</code>.</li> <li>A leaf labeled <code>\u03b5</code> has no siblings, that is, a vertex with a child labeled <code>\u03b5</code> can have no other children.</li> </ul> <p>Let's take a look at an example using the left-most derivation of <code>a + b * c</code>.</p> <p>The Left-most Derivation</p> <p>For example for write parse tree for this left-most derivation:</p> <pre><code>E \u2192 E * E\nE \u2192 E + E * E\nE \u2192 id + E * E\nE \u2192 id + id * E\nE \u2192 id + id * id\n</code></pre> <p>Step-by-Step Parse Tree Construction</p> <p>Now, let's build the parse tree step-by-step:</p> <p>step 1: <code>E \u2192 E * E</code></p> <p></p> <p>step 2: <code>E \u2192 E + E * E</code></p> <p></p> <p>step 3: <code>E \u2192 id + E * E</code></p> <p></p> <p>step 4: <code>E \u2192 id + id * E</code></p> <p></p> <p>step 5: <code>E \u2192 id + id * id</code></p> <p></p> <p>Parse Tree Characteristics</p> <p>In a parse tree, we have:</p> <ul> <li>All leaf nodes are terminals.</li> <li>All interior nodes are non-terminals.</li> <li>In-order traversal gives the original input string.</li> <li>The parse tree shows the associativity and precedence of operators. The deepest sub-tree is traversed first, so the operator in that sub-tree gets precedence over the operator in the parent nodes.</li> <li>There is a many-to-one relationship between derivations and parse trees.</li> <li>Indeed, no information on order of derivation steps is associated with the final parse tree.</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#parse-trees-and-abstract-syntax-tree-ast","title":"Parse trees and abstract syntax tree (AST)","text":"<p>An AST does not include inessential punctuation and delimiters (braces, semicolons, parentheses, etc.). </p>"},{"location":"lectures/03_Syntax-Analysis/#ambiguity-in-grammar","title":"Ambiguity in Grammar","text":"<p>A grammar is said to be ambiguous if it has more than one parse tree for at least one string. - An ambiguous grammar is one that produces more than one leftmost derivation or more than one rightmost derivation for the same sentence. - Note: One leftmost and one rightmost derivation for a word is not sufficient. example  consider the following grammar: <pre><code>\ud835\udc38 \u2192 \ud835\udc38 + \ud835\udc38 | \ud835\udc38 * \ud835\udc38 | \u2212 \ud835\udc38 | (\ud835\udc38) | \ud835\udc56\ud835\udc51 \n</code></pre> solution There is a word (string) id+id*id generated by this grammar that has two different parse trees: </p>"},{"location":"lectures/03_Syntax-Analysis/#ambiguity-detection","title":"Ambiguity detection","text":"<p>The problem of deciding whether a given CFG is ambiguous is undecidable. - Bad news: \u2219 There is no general algorithm to remove ambiguity from a CFG. - More bad new: \u2219 Some CFL\u2019s have only ambiguous CFG\u2019s.</p> <p>CFL \ud835\udc3f is inherently ambiguous if all grammars for \ud835\udc3f are ambiguous. Inherent ambiguity: example One possible ambiguous grammar for  L ={\ufe000^\ud835\udc561^\ud835\udc572^\ud835\udc58: \ud835\udc56 = \ud835\udc57 \u2228 \ud835\udc57 = \ud835\udc58}\ufe00:  There are two derivations of every string with equal numbers of 0\u2019s, 1\u2019s and 2\u2019s: <pre><code>\ud835\udc46 \u21d2 \ud835\udc34\ud835\udc35 \u21d2 01\ud835\udc35 \u21d2 012\n\ud835\udc46 \u21d2 \ud835\udc36\ud835\udc37 \u21d2 0\ud835\udc37 \u21d2 012\n</code></pre></p> <p>Notes on detecting ambiguous grammar The following forms of CFGs are ambiguous: 1. \ud835\udc34 \u2192 \ud835\udefc\ud835\udc34 | \ud835\udc34\ud835\udefd (left recursive and right recursive simultaneously) 2. \ud835\udc34 \u2192 \ud835\udc35, \ud835\udc35 \u2192 \ud835\udc36, \ud835\udc36 \u2192 \ud835\udc65, ..., \ud835\udc65 \u2192 \ud835\udc34 3. \ud835\udc34 \u2192 \ud835\udc34\ud835\udc34, \ud835\udc34 \u2192 \ud835\udc34\ud835\udc34\ud835\udc34, ... 4. \ud835\udc34 \u2192 \ud835\udc34\ud835\udefc\ud835\udc34</p>"},{"location":"lectures/03_Syntax-Analysis/#ambiguity-problems-for-compilers","title":"Ambiguity problems for compilers","text":"<ul> <li>Ambiguity is problematic because meaning of the programs can be incorrect.</li> <li>Example 1: Dangling-else problem \u2219 Here \"other\"stands for any other statement.  </li> <li>Consider statement: \u2219 if \ud835\udc381 then if \ud835\udc382 then \ud835\udc461 else \ud835\udc462  </li> <li>Results for \ud835\udc381 : false, \ud835\udc382 : true, \ud835\udc461 : \ud835\udc67 := 10, and \ud835\udc462 : \ud835\udc67 := 0 \u2219 Top tree: \ud835\udc67 does not set. \u2219 Bottom tree: \ud835\udc67 = 0</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#techniques-for-eliminating-ambiguity","title":"Techniques for eliminating ambiguity","text":"<ul> <li>There is no algorithm to convert automatically any ambiguous grammar to an unambiguous grammar accepting the same language. Techniques for eliminating ambiguity from some CFGs:</li> <li>Rewriting the grammar (cleanest way) . Sometimes an ambiguous grammar can be rewritten to eliminate the ambiguity (a completely new grammar).</li> <li>Applying disambiguating rules \u2219 On grammar, \u2219 At parsing time.</li> <li>A combination of the above techniques</li> </ul> <p>example</p> <p>Unambiguous, with precedence and associativity rules honored:</p> <ul> <li> <p>Ambiguous: E -&gt; E + E | E * E | (E) | num | id</p> </li> <li> <p>Unambiguous: E -&gt; E + T | T T -&gt; T * F | F F -&gt; (E) | num | id</p> </li> </ul> <p>For another example for operation(+, -, *, /, ^), we have:</p> <p>E -&gt; E + T | T</p> <p>T -&gt; T * F | T * F | F</p> <p>F -&gt; G ^ F | G</p> <p>G -&gt; num | id | (E)</p> <pre><code>                     E\n                    /|\\\n                   / | \\\n                  /  |  \\\n                 E   +   T\n                 |      /|\\ \n                 |     / | \\\n                num   T  *  F\n                 |    |     |\n                 1    F     G\n                     /|\\    |\n                    G ^ F  num\n                    |   |   | \n                   num  G   3\n                    |   |\n                    2  num\n                        |\n                        3\n</code></pre>"},{"location":"lectures/03_Syntax-Analysis/#programming-languages-grammars","title":"Programming Languages Grammars","text":""},{"location":"lectures/03_Syntax-Analysis/#writing-a-grammar","title":"Writing a grammar","text":"<ul> <li>The sequences of tokens accepted by a parser form a superset of the programming language.</li> <li>Subsequent phases of the compiler must analyze the output of the parser to ensure compliance with rules that are not checked by the parser.</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#lexical-versus-syntactic-analysis","title":"Lexical versus syntactic analysis","text":"<p>Why use regular expressions to define the lexical syntax of a language? 1. Separating the syntactic structure of a language into lexical and nonlexical parts provides a convenient way of modularizing the front end of a compiler into two manageable-sized components. 2. The lexical rules of a language are frequently quite simple, and to describe them we do not need a notation as powerful as grammars. 3. Regular expressions generally provide a more concise and easier-to-understand notation for tokens than grammars. 4. More efficient lexical analyzers can be constructed automatically from regular expressions than from arbitrary grammars.</p> <ul> <li>There are no firm guidelines as to what to put into the lexical rules, as opposed to the syntactic rules.</li> <li>Regular expressions are most useful for describing the structure of constructs such as identifiers, constants, keywords, and white spaces.</li> <li>Grammars, on the other hand, are most useful for describing nested structures such as balanced parentheses, matching begin-end\u2019s, corresponding if-then-else\u2019s, and so on.</li> </ul> <p>A grammar for a subset of Java statements </p>"},{"location":"lectures/03_Syntax-Analysis/#antlr","title":"ANTLR","text":"<p>ANTLR (ANother Tool for Language Recognition) is a powerful parser generator used to build interpreters, compilers, and translators for domain-specific languages and structured text. It takes a grammar written in a special syntax and generates code\u2014typically in Java, but also supporting languages like Python, C#, and JavaScript\u2014that can recognize and process inputs according to that grammar. ANTLR supports lexical analysis (tokenizing) and syntactic analysis (parsing), allowing developers to define tokens, parser rules, and even error recovery strategies. Its output includes a lexer and parser that produce parse trees, which can be traversed using listeners or visitors to perform semantic analysis or code generation. ANTLR is widely used in academia and industry due to its flexibility, modern features, and strong tooling support.</p>"},{"location":"lectures/03_Syntax-Analysis/#describing-programming-language-grammar-in-antlr","title":"Describing programming language grammar in ANTLR","text":""},{"location":"lectures/03_Syntax-Analysis/#antlr-input-and-output","title":"ANTLR input and output","text":""},{"location":"lectures/03_Syntax-Analysis/#parsing","title":"Parsing","text":"<ul> <li>Process of determination whether a string can be generated by a grammar.</li> <li>It is the process of determining how a string of terminals can be generated by a grammar.</li> <li>A syntax analyzer or parser is a program that performs syntax analysis.</li> <li>A parser takes as input tokens from the lexical analyzer and treats the token names as terminal symbols of a context-free grammar.</li> <li>The parse tree may be constructed figuratively (by going through the corresponding derivation steps) or literally.  </li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#types-of-parsing","title":"Types of Parsing","text":"<p>Syntax analyzers follow production rules defined by means of context-free grammar. The way the production rules are implemented (derivation) divides parsing into two types : top-down parsing and bottom-up parsing. </p>"},{"location":"lectures/03_Syntax-Analysis/#top-down-parsinggoal-driven","title":"Top-down Parsing(goal driven)","text":"<p>When the parser starts constructing the parse tree from the start symbol and then tries to transform the start symbol to the input, it is called top-down parsing.</p> <ul> <li>Start from the start non-terminal,</li> <li>Grow parse tree downwards to match the input word,</li> <li>Easier to understand and program manually.</li> </ul> <p>Recursive descent parsing : It is a common form of top-down parsing. It is called recursive as it uses recursive procedures to process the input. Recursive descent parsing suffers from backtracking.</p> <p>Backtracking : It means, if one derivation of a production fails, the syntax analyzer restarts the process using different rules of same production. This technique may process the input string more than once to determine the right production.</p>"},{"location":"lectures/03_Syntax-Analysis/#bottom-up-parsingdata-driven","title":"Bottom-up Parsing(data driven)","text":"<p>As the name suggests, bottom-up parsing starts with the input symbols and tries to construct the parse tree up to the start symbol. - Start from the input word, - Build up parse tree which has start non-terminal as root, - More powerful and used by most parser generators.</p> <p>Example: Input string : a + b * c</p> <p>Production rules:</p> <pre><code>S \u2192 E\nE \u2192 E + T\nE \u2192 E * T\nE \u2192 T\nT \u2192 id\n</code></pre> <p>Let us start bottom-up parsing <pre><code>a + b * c\n</code></pre></p> <p>Read the input and check if any production matches with the input:</p> <pre><code>a + b * c\nT + b * c\nE + b * c\nE + T * c\nE * c\nE * T\nE\nS\n</code></pre>"},{"location":"lectures/03_Syntax-Analysis/#directional-methods","title":"Directional methods","text":"<ul> <li>Process the input symbol by symbol from Left-to-right,</li> <li>Advantage: parsing starts and makes progress before the last symbol of the input is seen,</li> <li>Example: LL and LR parsers.</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#non-directional-methods","title":"Non-directional methods","text":"<ul> <li>Allow access to input in an arbitrary order,</li> <li>Disadvantage: Require the entire input to be in memory before parsing can start,</li> <li>Advantage: Allow more flexible grammars than directional parsers,</li> <li>Example: CYK parser</li> </ul>"},{"location":"lectures/03_Syntax-Analysis/#top-down-vs-bottom-up","title":"Top-down vs. bottom-up","text":""},{"location":"lectures/03_Syntax-Analysis/#parsing-complexity","title":"Parsing complexity","text":""},{"location":"lectures/04_Top-down/","title":"Top Dawn Parsing","text":""},{"location":"lectures/04_Top-down/#_1","title":"<p>We have learnt in the last chapter that the top-down parsing technique parses the input, and starts constructing a parse tree from the root node gradually moving down to the leaf nodes. The types of top-down parsing are depicted below:</p> <pre><code>                             Top-Down\n                                |\n                        Recursive Descent\n                        |               |\n                  Back-tracking  Non back-tracking\n                                        |\n                                 Predictive Parser\n                                        |\n                                    LL Parser\n</code></pre>","text":""},{"location":"lectures/04_Top-down/#recursive-descent-parsing","title":"Recursive Descent Parsing","text":"<p>Recursive descent is a top-down parsing technique that constructs the parse tree from the top and the input is read from left to right. It uses procedures for every terminal and non-terminal entity. This parsing technique recursively parses the input to make a parse tree, which may or may not require back-tracking. But the grammar associated with it (if not left factored) cannot avoid back-tracking. A form of recursive-descent parsing that does not require any back-tracking is known as predictive parsing.</p> <p>This parsing technique is regarded recursive as it uses context-free grammar which is recursive in nature. - A typical procedure for a nonterminal in a top-down parser is as follows:</p>"},{"location":"lectures/04_Top-down/#_2","title":"<p>This code represents a recursive descent parser for a top-down parsing approach, specifically implementing a procedure A() for a nonterminal symbol A. Here's how it works: 1. Choose a Production: The parser selects one production rule for nonterminal A, expressed as A \u2192 X\u2081X\u2082\u2026X\u2096, where X\u1d62 are symbols (terminals or nonterminals). 2. terate Over Symbols: For each symbol X\u1d62 in the production (from 1 to k): - If X\u1d62 is a Nonterminal: The parser recursively calls the procedure for X\u1d62 (e.g., X\u1d62()), allowing it to process the corresponding grammar rule. - If X\u1d62 is a Terminal and Matches Input: If X\u1d62 matches the current input symbol a, the parser advances the input to the next symbol. - Otherwise: If X\u1d62 does not match the input (and is not a nonterminal), an error is reported. 3. Recursive Nature: The process repeats for each nonterminal encountered, building a parse tree from the top down by expanding nonterminals into their productions and matching terminals with the input.  </p> <p>This method assumes a predictive parser with a single production choice per nonterminal for simplicity, and it fails if the input cannot be matched according to the chosen production. </p> <p>problem: The pseudocode does not specify how the parser selects which production rule for A (e.g., A \u2192 X\u2081X\u2082\u2026X\u2096) to apply when there are multiple possible rules. Without a clear criterion or strategy (e.g., a parsing table or lookahead), the choice is arbitrary, making the parser nondeterministic. This can lead to different parse results for the same input, depending on the chosen production, which is undesirable for a reliable top-down parser.</p> <p>solution: Use the lookahead symbol: the first, i.e., leftmost, terminal of the input string.</p> <p>example:</p>","text":""},{"location":"lectures/04_Top-down/#_3","title":"","text":""},{"location":"lectures/04_Top-down/#_4","title":"<p>The match(terminal t) function is used in a top-down parser to verify and consume terminals from the input:</p> <ul> <li>It checks if the current lookahead symbol (lookahead) matches the expected terminal t.</li> <li>If they match, it advances the input by calling nextTerminal() to set lookahead to the next input symbol.</li> <li>If they do not match, it reports a \"syntax error\" and does not advance the input.</li> </ul> <p>This function ensures the parser stays synchronized with the input stream, failing when the input deviates from the expected grammar.</p>","text":""},{"location":"lectures/04_Top-down/#back-tracking","title":"Back-tracking","text":""},{"location":"lectures/04_Top-down/#back-tracking-problem-in-general-recursive-descent-parsers","title":"Back-tracking problem in general recursive-descent parsers","text":"<ul> <li>General recursive-descent may require backtracking; that is, it may require repeated scans over the input. \u2219 Backtracking: The selection of a production for a nonterminal may involve trial-and-error; that is, we may have to try a production and backtrack to try another production if the first is found to be unsuitable.  </li> <li>A left-recursive grammar can cause a recursive-descent parser, even one with backtracking, to go into an in nite loop. \u2219 A \u2192 AX\u2081X\u2082\u2026X\u2096; Top- down parsers start from the root node (start symbol) and match the input string against the production rules to replace them (if matched). To understand this, take the following example of CFG: <pre><code>S \u2192 rXd | rZd\nX \u2192 oa | ea\nZ \u2192 ai\n</code></pre> For an input string: read, a top-down parser, will behave like this:</li> </ul> <p>It will start with <code>S</code> from the production rules and will match its yield to the left-most letter of the input, i.e. <code>r</code>. The very production of <code>S</code> <code>(S \u2192 rXd)</code> matches with it. So the <code>top-down</code> parser advances to the next input letter (i.e. <code>e</code>). The parser tries to expand non-terminal <code>X</code> and checks its production from the left <code>(X \u2192 oa)</code>. It does not match with the next input symbol. So the top-down parser backtracks to obtain the next production rule of <code>X</code>, <code>(X \u2192 ea)</code>.</p> <p>Now the parser matches all the input letters in an ordered manner. The string is accepted.</p> <p></p>"},{"location":"lectures/04_Top-down/#predictive-parser","title":"Predictive Parser","text":"<p>Predictive parser is a recursive descent parser, which has the capability to predict which production is to be used to replace the input string. The predictive parser does not suffer from backtracking.</p> <p>To accomplish its tasks, the predictive parser uses a look-ahead pointer, which points to the next input symbols. To make the parser back-tracking free, the predictive parser puts some constraints on the grammar and accepts only a class of grammar known as LL(k) grammar. - Predictive parsing relies on information about the first k symbols that can be generated by a production body. - Allow parser to \"lookahead\" k number of tokens from the input. \u2219 LL(1): Parser can only look at current token. \u2219 LL(2): Parser can only look at current token and the token follows it. \u2219 LL(k): Parser can look at k tokens from input.</p> <p></p> <p>Predictive parsing uses a stack and a parsing table to parse the input and generate a parse tree. Both the stack and the input contains an end symbol $ to denote that the stack is empty and the input is consumed. The parser refers to the parsing table to take any decision on the input and stack element combination.</p> <p></p> <p>In recursive descent parsing, the parser may have more than one production to choose from for a single instance of input, whereas in predictive parser, each step has at most one production to choose. There might be instances where there is no production matching the input string, making the parsing procedure to fail.</p>"},{"location":"lectures/04_Top-down/#ll-parser","title":"LL Parser","text":"<p>An LL Parser accepts LL grammar. LL grammar is a subset of context-free grammar but with some restrictions to get the simplified version, in order to achieve easy implementation. LL grammar can be implemented by means of both algorithms namely, recursive-descent or table-driven.</p> <p>LL parser is denoted as LL(k). The first L in LL(k) is parsing the input from left to right, the second L in LL(k) stands for left-most derivation and k itself represents the number of look aheads. Generally k = 1, so LL(k) may also be written as LL(1).</p> <p></p>"},{"location":"lectures/04_Top-down/#first-and-follow-sets-for-a-cfg","title":"First and Follow sets for a CFG","text":""},{"location":"lectures/04_Top-down/#first-set","title":"First set","text":"<p>First set for string of grammar symbols \u03b1. FIRST(\u03b1), where \u03b1 is any string of grammar symbols, to be the set of terminals, \u2032a, that begin strings derived from \u03b1. Computing First set - FIRST(\u03b1) ={\ufe00a | \u03b1 \u21d2^* a\u03b2}\ufe00\u222a{\ufe00\u03b5 | \u03b1 \u21d2^* \u03b5}\ufe00 - To compute FIRST(X) for all grammar symbols X, apply the following rules until no more terminals or \u03f8 can be added to any FIRST set: 1. If X is a terminal, then FIRST(X) ={\ufe00}\ufe00. 2. If X is a nonterminal and X \u2192 Y\u2081Y\u2082\u2026Y\u2096 is a production for some k \u2265 1 then place a in FIRST(X) if for some i, a is in FIRST(Y<sub>i</sub>), and \u03b5 is in all of FIRST(Y\u2081), ..., FIRST(Y<sub>i-1</sub>). 3. If \u03b5 is in FIRST(Yj) for all j = 1, 2, ..., k, then add \u03b5 to FIRST(X). 4. If Y\u2081 does not derive \u03b5, then we add nothing more to FIRST(X). 5. If X \u2192 \u03b5 is a production, then add \u03b5 to FIRST(X).</p>"},{"location":"lectures/04_Top-down/#follow-set","title":"Follow set","text":"<p>Follow set for nonterminal A in a CFG. FOLLOW(A), for nonterminal A, is the set of terminals a that can appear immediately to the right of A in some sentential form. Computing Follow set - FOLLOW(X) ={\ufe00a | S\u21d2^* \u03b2Xa\u03b3}\ufe00 - To compute FOLLOW(X) for all nonterminals X, apply the following rules until nothing can be added to any FOLLOW set: 1. Place $ in FOLLOW(X), if X is the start symbol, and $ is the input right end marker (S\u2032 \u2192 S$). 2. If there is a production A \u2192 \u03b1X\u03b2, then everything in FIRST(\u03b2) except \u03b5, is in FOLLOW(X). 3. If there is a production A \u2192 \u03b1X, or a production A \u2192 \u03b1X\u03b2 where FIRST(\u03b2) contains \u03b5, then everything in FOLLOW(A), is in FOLLOW(X). </p> <p>example: Compute the First and Follow sets for non-terminals in the following grammar: </p> <p>solution: FIRST Sets The FIRST set of a nonterminal contains the set of terminals that can begin the string derived from that nonterminal.</p> <p>$ FIRST(E) $:</p> <p>$ E \\rightarrow T E' $, so $ FIRST(E) = FIRST(T) $. $ T \\rightarrow F T' $, so $ FIRST(T) = FIRST(F) $. $ F \\rightarrow (E) \\mid num $, so $ FIRST(F) = { (, num } $. Thus, $ FIRST(T) = { (, num } $, and $ FIRST(E) = { (, num } $.  </p> <p>$ FIRST(E') $:</p> <p>$ E' \\rightarrow + T E' \\mid \\epsilon $. $ + T E' $ starts with $ + $, so $ + $ is in $ FIRST(E') $. $ \\epsilon $ is also a possibility, so $ FIRST(E') = { +, \\epsilon } $.  </p> <p>$ FIRST(T) $:</p> <p>As computed above, $ T \\rightarrow F T' $, so $ FIRST(T) = FIRST(F) = { (, num } $.</p> <p>$ FIRST(T') $:</p> <p>$ T' \\rightarrow * F T' \\mid \\epsilon $. $ * F T' $ starts with $ * $, so $ * $ is in $ FIRST(T') $. $ \\epsilon $ is also a possibility, so $ FIRST(T') = { *, \\epsilon } $.</p> <p>$ FIRST(F) $:</p> <p>$ F \\rightarrow (E) \\mid num $, so $ FIRST(F) = { (, num } $.</p> <p></p> <p>FOLLOW Sets The FOLLOW set of a nonterminal contains the set of terminals that can appear immediately after that nonterminal in a derivation.</p> <p>$ FOLLOW(E) $:</p> <p>$ E $ is the start symbol, so $ FOLLOW(E) $ includes $ $ $ (end of input). $ E' \\rightarrow + T E' $ can follow $ E $ in $ (E) $, so $ FOLLOW(E) = { ), $ } $.</p> <p>$ FOLLOW(E') $:</p> <p>$ E' $ appears in $ E \\rightarrow T E' $, so $ FOLLOW(E') = FOLLOW(E) = { ), $ } $. Also, $ E' \\rightarrow + T E' $ ends with $ E' $, reinforcing the same follow set.</p> <p>$ FOLLOW(T) $:</p> <p>$ T $ appears in $ E \\rightarrow T E' $, so $ FOLLOW(T) = FOLLOW(E') = { ), $ } $. Additionally, $ E' \\rightarrow + T E' $ places $ + $ before $ T $, so $ FOLLOW(T) = { ), +, $ } $.</p> <p>$ FOLLOW(T') $:</p> <p>$ T' $ appears in $ T \\rightarrow F T' $, so $ FOLLOW(T') = FOLLOW(T) = { ), +, $ } $.</p> <p>$ FOLLOW(F) $:</p> <p>$ F $ appears in $ T \\rightarrow F T' $, so $ FOLLOW(F) = FOLLOW(T') = { ), +, $ } $. Also, $ F \\rightarrow (E) $ places $ ) $ after $ E $, which aligns with the follow set.</p> <p></p>"},{"location":"lectures/04_Top-down/#ll1-grammar","title":"LL(1) grammar","text":"<p>Predictive parsers, that is, recursive-descent parsers needing no backtracking, can be constructed for a class of grammars called LL(1). The first \"L\" in LL(1) stands for scanning the input from left to right, the second \"L\" for producinga leftmost derivation, and the \"1\" for using one input symbol of lookahead at each step to make parsing action decisions  </p> <p>A grammar G is LL(1) if A \u2192 \u03b1 | \u03b2 are two distinct productions of G:</p> <p>for no terminal, both \u03b1 and \u03b2 derive strings beginning with a.</p> <p>at most one of \u03b1 and \u03b2 can derive empty string.</p> <p>if \u03b2 \u2192 t, then \u03b1 does not derive any string beginning with a terminal in FOLLOW(A). definition A grammar G is LL(1) if and only if whenever A \u2192 \u03b1 | \u03b2 are two distinct productions of G, the following conditions hold: 1. FIRST(\u03b1) \u2229 FIRST(\u03b2) = \u03a6 2. if \u03b1\u21d2^* \u03b5 or \u03b2\u21d2^* \u03b5 then FIRST(A) \u2229 FOLLOW(A) = \u03a6</p> <p></p>"},{"location":"lectures/04_Top-down/#predictive-parsers-implementations","title":"Predictive parsers implementations","text":"<ul> <li>Recursive: Recursive descent parser \u2219 Each non-terminal parsed by a procedure, \u2219 Call other procedures to parse sub-nonterminals recursively, \u2219 Automated by tools such as ANTLR.</li> <li>Non-recursive: Table-driven parser \u2219 Use the LL(1) parsing table, \u2219 Parsing table determines for each non-terminal and each look-ahead symbol what one production to use \u2219 Push-down automata: essentially a table driven FSA + stack to do recursive calls,</li> </ul>"},{"location":"lectures/04_Top-down/#ll1-parsing-table","title":"LL(1) parsing table","text":"<p>If, after performing the above, there is no production at all in M[A; a], then set M[A; a] to error (which we normally represent by an empty entry in the table).</p> <p>example</p> <p> </p> <p>Consider production E \u2192 TE'. Since FIRST(T E') = FIRST(T ) = {(, id} this production is added to M[E , (] and M[E , id]. Production E' \u2192 +TE' is added to M[E' , +] since FIRST(+TE') = {+}. Since FOLLOW(E') = {), $}, production E' \u2192 \u03b5 is added to M[E' , )] and M[E' , $].</p> <p>example </p>"},{"location":"lectures/04_Top-down/#table-driven-predictive-parsing","title":"Table-driven predictive parsing","text":"<p>A nonrecursive predictive parser can be built by maintaining a stack explicitly, rather than implicitly via recursive calls. The parser mimics a leftmost derivation. If w is the input that has been matched so far, then the stack holds asequence of grammar symbols \u03b1 such that S \u21d2^* w\u03b1</p> <p></p> <p>This algorithm implements a predictive parsing using a parsing table $ M $ for a grammar $ G $ to determine if a string $ w $ is in the language $ L(G) $ and, if so, produces its leftmost derivation. Here's how it works: 1. Initialization:</p> <ul> <li>Start with the input string $ w $ and the parsing table $ M $.</li> <li>Let $ a $ be the first symbol of $ w $.</li> <li>Initialize the stack with the start symbol $ X $ (typically $ S $) and a marker $ $ $.</li> <li> <p>Main Loop:</p> </li> <li> <p>Continue while the stack is not empty and the top symbol $ X $ is not $ $ $.</p> </li> <li> <p>For each iteration:</p> <ul> <li>Match Terminal: If $ X = a $ (the current input symbol), pop $ X $ from the stack and advance $ a $ to the next symbol of $ w $.</li> <li>Terminal Mismatch: If $ X $ is a terminal but $ X \\neq a $, report an error.</li> <li>Nonterminal Processing: If $ X $ is a nonterminal, look up $ M[X, a] $ in the parsing table:<ul> <li>If $ M[X, a] $ is an error entry, report an error.</li> <li>If $ M[X, a] = X \\rightarrow Y_1 Y_2 \\cdots Y_k $ (a production), perform the following:<ul> <li>Pop $ X $ from the stack.</li> <li>Push the symbols $ Y_k, Y_{k-1}, \\ldots, Y_1 $ onto the stack, with $ Y_1 $ on top.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Update $ X $ to the new top stack symbol.</p> </li> <li> <p>Termination:</p> </li> <li> <p>The loop ends when the stack is empty or $ X = $ $.</p> </li> <li>If $ w $ is fully parsed and the stack contains only $ $ $, the algorithm outputs the leftmost derivation. Otherwise, it indicates an error.</li> </ul> <p>example: grammer:</p> <p> input:  </p> <p>id + id *id</p> <p>table:   parsing: </p> <p>Note that the sentential forms in this derivation correspond to the input that has already been matched (in column MATCHED) followed by the stack contents. The matched input is shown only to highlight the correspondence. For the same reason, the top of the stack is to the left; when we consider bottom-up parsing, it will be more natural to show the top of the stack to the right. The input pointer points to the leftmost symbol of the string in the INPUT column.</p>"},{"location":"lectures/04_Top-down/#transforming-grammars-into-ll1","title":"Transforming Grammars into LL(1)","text":""},{"location":"lectures/04_Top-down/#ll1-grammars-space","title":"LL(1) grammars space","text":"<ul> <li>Obviously, not all grammars can be converted into LL(1) form.</li> <li>Few transformation techniques can be applied to convert some grammar into LL(1) form.</li> </ul>"},{"location":"lectures/04_Top-down/#left-factoring","title":"Left factoring","text":"<ul> <li>Left factoring is a grammar transformation that is useful for producing a grammar suitable for predictive, or top-down, parsing.</li> <li>if A \u2192a\u03b1 | a\u03b2 | \u03b3 , FIRST(a\u03b1) \u2229 FIRST(a\u03b2) ={\ufe00a}\ufe00</li> <li>then     A \u2192 aB | \u03b3     B \u2192 \u03b1 | \u03b2</li> <li>or (extended BNF form)     A \u2192 a(\u03b1|\u03b2) | \u03b3</li> <li>Repeatedly apply this transformation until no two alternatives for a nonterminal have a common prefix.</li> </ul>"},{"location":"lectures/04_Top-down/#elimination-of-left-recursion","title":"Elimination of left recursion","text":"<ul> <li>A grammar is left recursive if it has a nonterminal A such that there is a derivation A \u21d2^+ A\u03b1 for some string \u03b1.</li> <li>Immediate left recursion: if A \u2192 A\u03b1 | \u03b2 , FIRST(A\u03b1) \u2229 FIRST(\u03b2) ={\ufe00\u03b2}\ufe00</li> <li>then A \u2192 \u03b2B B \u2192 \u03b1B | \u03b5</li> <li>or (extended BNF form) A \u2192 \u03b2{\u03b1}</li> <li>It does not eliminate left recursion involving derivations of two or more steps.</li> </ul> <p>This algorithm transforms a grammar $ G $ with no cycles or \\epsilon-productions into an equivalent grammar without left recursion. It proceeds as follows:</p> <ol> <li>Arrange Nonterminals: Order the nonterminals $ A_1, A_2, \\ldots, A_n $ in a sequence, which determines the processing order.</li> <li> <p>Iterate Over Nonterminals: For each nonterminal $ A_i $ (from 1 to $ n $): Substitute Earlier Productions: For each $ A_j $ (where $ j $ ranges from 1 to $ i-1 $):  </p> <ul> <li>Identify all productions of the form $ A_i \\rightarrow A_j \\gamma $, where \\gamma is a string of symbols.  </li> <li>Replace each such production with new productions $ A_i \\rightarrow \\delta_1 \\gamma | \\delta_2 \\gamma | \\cdots | \\delta_k \\gamma $, where $ \\delta_1, \\delta_2, \\ldots, \\delta_k $ are all existing productions of $ A_j $ (e.g., $ A_j \\rightarrow \\delta_1 | \\delta_2 | \\cdots | \\delta_k $). This substitution eliminates indirect left recursion by expanding $ A_j $ into its alternatives.</li> </ul> </li> <li> <p>Eliminate Immediate Left Recursion: After substitutions, check $ A_i $-productions for immediate left recursion (e.g., $ A_i \\rightarrow A_i \\alpha | \\beta $). Transform them into: New nonterminal $ A_i' $. Productions $ A_i \\rightarrow \\beta A_i' $ for all non-recursive productions $ A_i \\rightarrow \\beta $, and $ A_i' \\rightarrow \\alpha A_i' | \\epsilon $ for all recursive productions $ A_i \\rightarrow \\alpha A_i' $.</p> </li> </ol> <p>It is possible to eliminate left recursion by transformation to right recursion.</p> <p></p> <p>example  Convert the following expression grammar to LL(1) form: <pre><code>E \u2192 E + T | E \u2212 T | T\nT \u2192 T * F | T/F | F\nF \u2192 id | num | (E)\n</code></pre> left factoring: <pre><code>E \u2192 E(+T | \u2212 T) | T\nT \u2192 T(*F | /F) | F\nF \u2192 id | num | (E)\n</code></pre>  left-recursion deletion:</p> <p>```  E \u2192 TT\u2032 T\u2032 \u2192 +TT\u2032| \u2212 TT\u2032| \u03b5 T \u2192 FF\u2032 F\u2032 \u2192 *FF\u2032| /FF\u2032| \u03b5 F \u2192 id | num | (E) <pre><code> Grammar in EBNF form:\n\n ```\n E \u2192 T{+T | \u2212 T}\nT \u2192 F{*F | /F}\nF \u2192 id | num | (E)\n</code></pre></p>"},{"location":"lectures/04_Top-down/#error-recovery-in-predictive-parsing","title":"Error Recovery in Predictive Parsing","text":""},{"location":"lectures/04_Top-down/#predictive-parsing-errors","title":"Predictive parsing errors","text":"<ul> <li>An error is detected during predictive parsing when<ol> <li>The terminal on top of the stack does not match the next input symbol,</li> <li>Or nonterminal A is on top of the stack, a is the next input symbol, and M[A, a] is error (i.e., the parsing-table entry is empty).</li> </ol> </li> <li>Panic mode error recovery strategy: Skipping over symbols on the input until a token in a selected set of synchronizing tokens appears.</li> <li>Panic mode effectiveness depends on the choice of synchronizing set.</li> </ul>"},{"location":"lectures/04_Top-down/#synchronizing-set","title":"Synchronizing set","text":"<ol> <li>Place all symbols in FOLLOW(A) into the synchronizing set for nonterminal A.<ul> <li>If we skip tokens until an element ofFOLLOW(A) is seen and pop A from the stack, it is likely that parsing can continue.</li> </ul> </li> <li>Add keywords that begin statements to the synchronizing sets for the nonterminals generating expressions.</li> <li>If we add symbols in FIRST(A) to the synchronizing set for nonterminal A, then it may be possible to resume parsing according to A if a symbol in FIRST(A) appears in the input.</li> </ol>"},{"location":"lectures/04_Top-down/#panic-mode-error-recovery","title":"Panic mode error recovery","text":"<p>grammer:</p> <p> input:  </p> <p>+id * +id</p> <p>table:  </p> <ol> <li>If the parser looks up entry M[A, a] and finds that it is blank, then the input symbol a is skipped.</li> <li>If the entry is \"synch,\" then the nonterminal on top of the stack is popped in an attempt to resume parsing.</li> <li>If a token on top of the stack does not match the input symbol, then we pop the token from the stack.</li> </ol> <p>  We need to modify the \"match\" procedure.   The updated match(terminal t, Set synch) function now includes a synch set for error recovery. If the lookahead does not match the expected terminal $ t $, it calls syntaxError(synch) instead of immediately failing. The syntaxError(Set stop) function reports a \"Syntax Error\" and skips input symbols by advancing lookahead with nextTerminal() until it encounters a symbol in the stop set, improving resilience against syntax errors.</p>"},{"location":"lectures/04_Top-down/#phrase-level-error-recovery","title":"Phrase-level error recovery","text":"<ul> <li>Phrase-level error recovery is implemented by filling in the blank entries in the predictive parsing table with pointers to error routines.</li> <li>These routines may change, insert, ordelete symbols on the input and issue appropriate error messages.</li> </ul>"},{"location":"lectures/05_Bottom-up/","title":"Bottom-up Parsing","text":""},{"location":"lectures/05_Bottom-up/#bottom-up-parsing-process","title":"Bottom-Up Parsing Process","text":""},{"location":"lectures/05_Bottom-up/#_1","title":"<p>Bottom-up parsing is the process of reducing an input string to the start symbol S of the grammar. At each reduction step, a specific substring matching the body of a production (RHS) is replaced by the nonterminal at the head of that production (LHS). The key decisions during bottom-up parsing are about when to reduce and about what production to apply, as the parse proceeds.</p>","text":""},{"location":"lectures/05_Bottom-up/#handle-pruning","title":"Handle Pruning","text":"<ul> <li>Bottom-up parsing during a left-to-right scan of the input constructs a right-most derivation in reverse. </li> <li>A \"handle\" is a substring that matches the body of a production, and whose reduction represents one step along the reverse of a rightmost derivation.</li> <li>A handle A \u2192 \u03b2 in the parse tree for w: </li> </ul>"},{"location":"lectures/05_Bottom-up/#shift-reduce-parsing","title":"Shift-Reduce Parsing","text":"<ul> <li>Shift-reduce parsing is a form of bottom-up parsing in which a stack holds grammar symbols and an input buffer holds the rest of the string to be parsed. </li> <li>Primary operations are shift and reduce. </li> <li>During a left-to-right scan of the input string, the parser shifts zero or more input symbols onto the stack, until it is ready to reduce a string of grammar symbols on top of the stack. </li> <li>It then reduces to the head of the appropriate production with number R<sub>x</sub>. </li> <li>The parser repeats this cycle until it has detected an error or until the stack contains the start symbol and the input is empty.</li> <li>There are actually four possible actions a shift-reduce parser can make:</li> <li>Shift. Shift the next input symbol onto the top of the stack.</li> <li>Reduce. The right end of the string to be reduced must be at the top of the stack. Locate the left end of the string within the stack and decide with what nonterminal to replace the string.</li> <li>Accept. Announce successful completion of parsing.</li> <li>Error. Discover a syntax error and call an error recovery routine</li> </ul> <p>Primary operations </p>"},{"location":"lectures/05_Bottom-up/#shift-reduce-parsing-notations","title":"Shift-Reduce Parsing Notations","text":"<ul> <li>We use $ to mark the bottom of the stack and also the right end of the input. </li> <li>Split string into two substrings, right and left, the dividing point is marked by a |. </li> <li>Right substring is still not examined by parser (a string of terminals). </li> <li>Left substring has both terminals and non-terminals.</li> <li>Note: The | is not part of the string</li> <li>Initially, all input is unexamined \" | a1a2...an$\".</li> </ul>"},{"location":"lectures/05_Bottom-up/#shift-reduce-parsing-example","title":"Shift-Reduce Parsing Example","text":"<p>Let's consider the following productions: <pre><code>r1  S-&gt;E \nr4  E-&gt;T\nr2 E -&gt;E+T \nr3  T-&gt;(E)\nr5  T -&gt;num\n</code></pre> And the input string <code>|num+(num+num)$</code>. The parsing process would go as follows:</p> <p><pre><code>|num+(num+num)     shift\nnum|+(num+num)   reduce(r5)   \nT|+(num+num)    reduce(r4)\nE|+(num+num)  shift\nE+|(num+num)   shift\nE+(|num+num)   shift\nE+(num|+num)  reduce(r5)\nE+(T|+num)   reduce(r4)\nE+(E|+num)   shift\nE+(E+|num)   shift\nE+((E+num|)   reduce(r5)\nE+(E+T|)    reduce(r2)\nE+(E|)   shift\nE+(E)    reduce(r3)\nE+T    reduce(r2)\nE|    reduce(r1)\nS|\n</code></pre> Here, <code>shift</code> means pushing the next input symbol onto the top of the stack, and <code>reduce</code> means popping zero or more symbols off the stack (production RHS) and pushing a non-terminal on the stack (production LHS)</p> <p></p>"},{"location":"lectures/05_Bottom-up/#shift-reduce-parsing-stack","title":"Shift-Reduce Parsing Stack","text":"<ul> <li>Left substring can be implemented by a stack. </li> <li>Top of the stack is the | symbol. </li> <li>Shift pushes a terminal on the stack. </li> <li>Reduce pops Zero or more symbols of the stack (production RHS) and pushes a non-terminal on the stack (production LHS).</li> </ul> <p>example </p>"},{"location":"lectures/05_Bottom-up/#conflicts-during-shift-reduce-parsing","title":"Conflicts During Shift-Reduce Parsing","text":"<ul> <li>There are context-free grammars for which shift-reduce parsing cannot be used. </li> <li> <p>Every shift-reduce parser for such a grammar can reach a configuration in which the parser, knowing the entire stack and also the next k input symbols, cannot decide:</p> </li> <li> <p>Whether to shift or to reduce (a shift/reduce conflict)</p> </li> <li>Or cannot decide which of several reductions to make (a reduce/reduce conflict)</li> <li> <p>Consider the following ambiguous grammar for if-then-else statements:  </p> </li> <li> <p>If we have a shift-reduce parser in configuration: </p> </li> <li>There is a shift/reduce conflict,</li> <li>We cannot tell whether if expr then stmt is the handle.</li> </ul> <p>example  - Consider the following productions involving procedure calls and array references:  - If we have a shift-reduce parser in configuration:  - There is a reduce/reduce conflict, - It is evident that the id on top of the stack must be reduced, but by which production? - The correct choice is production (5) if id is a procedure, but production (7) if id is an array. - We need a precise mechanism to decide which action to take: shift or reduce, if reduce should be taken then to which production?</p>"},{"location":"lectures/05_Bottom-up/#final-thoughts","title":"Final Thoughts","text":"<p>Remember, mastering these concepts takes time and practice. Don't hesitate to ask questions or seek clarification if something is unclear. Happy studying!</p>"},{"location":"lectures/05_Bottom-up/#lr-parsing-and-lr-grammars","title":"LR Parsing and LR Grammars","text":"<ul> <li>LR parsing is the most prevalent type of bottom-up parser today. The term \"LR\" stands for left-to-right scanning of the input and constructing a rightmost derivation in reverse. </li> <li>A grammar for which we can construct a parsing table using an LR parsing algorithm is said to be an LR grammar. The class of grammars that can be parsed using LR methods is a proper superset of the class of grammars that can be parsed with predictive or LL methods.</li> </ul>"},{"location":"lectures/05_Bottom-up/#lrk-parsers-and-grammars","title":"LR(k) Parsers and Grammars","text":"<p>The \"k\" in LR(k) refers to the number of input symbols of lookahead that are used in making parsing decisions. The cases where k = 0 or k = 1 are of practical interest. When (k) is omitted, k is assumed to be 1. LR parsers are table-driven, similar to nonrecursive LL parsers. For a grammar to be LL(k), we must be able to recognize the use of a production by seeing only the first k symbols of what its right side derives.</p> <p>For a grammar to be LR(k), we must be able to recognize the occurrence of the right side of a production in a right-sentential form, with k input symbols of lookahead. This requirement is far less stringent than the one for LL(k) grammars. The principal drawback of the LR method is that it is too much work to construct an LR parser by hand for a typical programming-language grammar. A specialized tool, an LR parser generator, is needed.</p> <ul> <li>The most prevalent type of bottom-up parser today is based</li> <li>the \"R\"is for constructing a rightmost derivation in reverse,</li> <li>the \"k\"for the number of input symbols of lookahead that are used in making parsing decisions.</li> <li>The cases k = 0 or k = 1 are of practical interest.</li> <li>When (k) is omitted, k is assumed to be 1.</li> <li>LR parsers are table-driven, much like the non-recursive LL parsers.</li> </ul>"},{"location":"lectures/05_Bottom-up/#variants-of-lr-parsers","title":"Variants of LR Parsers","text":"<p>There are several variants of LR parsers: SLR parsers, LALR parsers, canonical LR(1) parsers, minimal LR(1) parsers, and generalized LR parsers (GLR parsers). LR parsers can be generated by a parser generator from a formal grammar defining the syntax of the language to be parsed. They are widely used for the processing of computer languages. An LR parser reads input text from left to right without backing up and produces a rightmost derivation in reverse. The name \"LR\" is often followed by a numeric qualifier, as in \"LR(1)\" or sometimes \"LR(k)\". To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned.</p>"},{"location":"lectures/05_Bottom-up/#lr0-parsing","title":"LR(0) parsing","text":""},{"location":"lectures/05_Bottom-up/#representing-item-sets","title":"Representing Item Sets","text":"<p>In the context of LR(0) parsing, an item set represents the current state of the parser. Each item in the set corresponds to a production in the grammar, with a dot indicating the current position in the production. For example, if we have a production <code>A -&gt; BC</code>, there will be four items corresponding to this production:</p> <ul> <li><code>A -&gt; .BC</code></li> <li><code>A -&gt; B.C</code></li> <li><code>A -&gt; BC.</code></li> <li><code>A -&gt; B.C.</code></li> </ul> <p>These items represent the different stages of recognizing the production <code>A -&gt; BC</code>.</p>"},{"location":"lectures/05_Bottom-up/#closure-and-goto-of-item-sets","title":"Closure and Goto of Item Sets","text":"<p>The closure operation takes a set of items and produces a new set containing all items that can be derived from the original set. This is done through a series of steps:</p> <ol> <li>Add every item in the original set to the closure set.</li> <li>For each item in the closure set that is of the form <code>A -&gt; \u03b1.B\u03b2</code>, check if there is a production <code>B -&gt; \u03b3</code> in the grammar. If so, add the item <code>B -&gt; .\u03b3\u03b2</code> to the closure set.</li> <li>Repeat step 2 until no more new items can be added to the closure set.</li> </ol> <p>The closure ensures that all nonterminals following the dot are expanded into their possible productions, allowing the parser to anticipate all valid next steps. This is a key step in constructing the LR(0) automaton.</p> <p>The GOTO operation, on the other hand, is used to determine the next state based on the current state and the next input symbol. It is defined as the closure of the set of all items of the form <code>A -&gt; \u03b1B.\u03b2</code>, where <code>A -&gt; \u03b1.B\u03b2</code> is in the current state.</p> <p>Intuitively, the GOTO functionis used to define the transitions in the LR(0) automaton for a grammar. The states of the automaton correspond to sets of items, and GOTO(I;B) specifies the transition from the state for I under input B. </p> <p>Note: Note: By convention, S \u2032 \u2192 S$ is the kernel of the first item, I0. example:</p> <p>Let's consider a simple grammar:</p> <pre><code>E -&gt; E + T | T\nT -&gt; F | (E)\nF -&gt; id\n</code></pre> <p>We start with the initial state <code>I0 = {E -&gt; .E + T}</code>. We apply the closure operation to get the closure of <code>I0</code>:</p> <pre><code>I0 = {E -&gt; .E + T, E -&gt; E + .T}\n</code></pre> <p>Then, we apply the GOTO operation with the input symbol <code>E</code> to get the next state:</p> <pre><code>GOTO(I0, E) = {E -&gt; E + .T, E -&gt; .T}\n</code></pre> <p>We continue this process until we reach the final state, which indicates that the input string is valid according to the grammar.</p> <p>example: <pre><code>(0) E\n\u2032 \u2192 E$\n(1) E \u2192 E + T\n(2) E \u2192 T\n(3) T \u2192 T * F\n(4) T \u2192 F\n(5) F \u2192 (E)\n(6) F \u2192 id\n</code></pre> CLOSURE(I0 ={\ufe00[E\u2032 \u2192 .E$]}\ufe00):  </p> <ul> <li>Start with [E' \\rightarrow \\cdot E\\$].</li> <li>For E after the dot, add all E-productions: [E \\rightarrow \\cdot E + T], [E \\rightarrow \\cdot T].</li> <li>For T in [E \\rightarrow \\cdot T], add all T-productions: [T \\rightarrow \\cdot T * F], [T \\rightarrow \\cdot F].</li> <li>For F in [T \\rightarrow \\cdot F], add all F-productions: [F \\rightarrow \\cdot (E)], [F \\rightarrow \\cdot id].</li> <li>Repeat until no new items are added, resulting in the full set.  </li> </ul> <p>GOTO(I0, E):  </p> <ul> <li>Take all items in I_0 and move the dot past E where E is after the dot.</li> <li>From [E' \\rightarrow \\cdot E\\$], move to [E' \\rightarrow E \\cdot \\$].</li> <li>From [E \\rightarrow \\cdot E + T], move to [E \\rightarrow E \\cdot + T].</li> <li>Collect these into I_1, excluding items where the dot cannot move past E.</li> </ul>"},{"location":"lectures/05_Bottom-up/#canonical-collection-of-sets-of-lr0-items","title":"Canonical Collection of Sets of LR(0) Items","text":"<p>To construct the parsing table, we need to compute the canonical collection of sets of LR(0) items for an augmented grammar. This involves creating a set of states, where each state represents a set of items. The transitions between states are determined by the GOTO operations.</p>"},{"location":"lectures/05_Bottom-up/#algorithm","title":"Algorithm","text":"<p>The items(G') algorithm is like building a map of all the possible steps a parser can take to understand a grammar $ G' $ with an added start rule. Here's how it works in a more natural way:</p> <ol> <li>Kick Things Off: It starts by creating a small group called $ C $ with just one set, which is the \"closure\" of the rule [S' \\rightarrow \\cdot S], where $ S' $ is a special starting point we add to the grammar.</li> <li> <p>Explore Step by Step: It keeps going through each group of rules (or \"sets\") in $ C $. For every group and every symbol (like a letter or word) in the grammar:</p> </li> <li> <p>If moving the parser's focus past that symbol (using $ \\text{GOTO} $) creates a new, useful set of rules that isn\u2019t already in $ C $, it adds that new set to the group.</p> </li> <li> <p>Keep Going Until Done: It repeats this exploration until no new groups pop up after a full round, meaning we\u2019ve mapped out all the possible parsing states.</p> </li> </ol> <p>example:</p> <p>Let's consider a slightly more complex grammar:</p> <pre><code>S -&gt; CC\nC -&gt; aC | d\n</code></pre> <p>We start by augmenting the grammar to handle left recursion:</p> <pre><code>(0) S\u2032 \u2192 S$\n(1) S \u2192 CC\n(2) C \u2192 aC\n(3) C \u2192 d\n</code></pre> <p>The canonical collection in this LR(0) parsing table is computed step by step as follows, based on the given grammar with productions $ S' \\rightarrow SS $, $ S \\rightarrow CC $, $ C \\rightarrow aC $, and $ C \\rightarrow d $. The process involves computing closures and GOTO transitions to build the set of states (I0 to I6):</p> <ol> <li> <p>Initialize with Closure of Start Symbol:</p> </li> <li> <p>Start with $ I_0 = \\text{CLOSURE}([S' \\rightarrow \\cdot SS]) $.</p> </li> <li>Add [S' \\rightarrow \\cdot SS].</li> <li>Since $ S $ follows the dot, add all $ S $-productions: [S \\rightarrow \\cdot CC].</li> <li>Since $ C $ follows the dot in [S \\rightarrow \\cdot CC], add all $ C $-productions: [C \\rightarrow \\cdot aC], [C \\rightarrow \\cdot d].</li> <li> <p>Result: $ I_0 = { [S' \\rightarrow \\cdot SS], [S \\rightarrow \\cdot CC], [C \\rightarrow \\cdot aC], [C \\rightarrow \\cdot d] } $.</p> </li> <li> <p>Compute GOTO for $ I_0 $:</p> </li> <li> <p>GOTO($ I_0, S $): Move dot past $ S $ in [S' \\rightarrow \\cdot SS] to get [S' \\rightarrow S \\cdot S]. No further closure needed. Result: $ I_4 $.</p> </li> <li>GOTO($ I_0, C $): Move dot past $ C $ in [S \\rightarrow \\cdot CC] to get [S \\rightarrow C \\cdot C]. Add $ C $-productions for the second $ C $: [C \\rightarrow \\cdot aC], [C \\rightarrow \\cdot d]. Result: $ I_3 $.</li> <li>GOTO($ I_0, a $): Move dot past $ a $ in [C \\rightarrow \\cdot aC] to get [C \\rightarrow a \\cdot C]. Add $ C $-productions: [C \\rightarrow \\cdot aC], [C \\rightarrow \\cdot d]. Result: $ I_2 $.</li> <li> <p>GOTO($ I_0, d $): Move dot past $ d $ in [C \\rightarrow \\cdot d] to get [C \\rightarrow d \\cdot]. This is a reduce item. Result: $ I_1 $.</p> </li> <li> <p>Expand from New States:</p> </li> <li> <p>GOTO($ I_3, C $): Move dot past second $ C $ in [S \\rightarrow C \\cdot C] to get [S \\rightarrow CC \\cdot]. This is a reduce item. Result: $ I_6 $.</p> </li> <li>GOTO($ I_2, C $): Move dot past $ C $ in [C \\rightarrow a \\cdot C] to get [C \\rightarrow aC \\cdot]. This is a reduce item. Result: $ I_5 $.</li> <li> <p>GOTO($ I_4, $ $): Move dot past $ S $ in [S' \\rightarrow S \\cdot S] with end marker $ $ $ to get [S' \\rightarrow SS \\cdot]. This is an accept state. Result: Accept (no new state needed).</p> </li> <li> <p>Iterate Until No New States:</p> </li> <li> <p>Check all new states ($ I_1, I_2, I_3, I_4, I_5, I_6 $) for further GOTO transitions.</p> </li> <li>$ I_1, I_5, I_6 $ are reduce states with no further moves.</li> <li>$ I_2 $ and $ I_3 $ were already expanded.</li> <li>No new states are generated, so the collection is complete: $ { I_0, I_1, I_2, I_3, I_4, I_5, I_6 } $.</li> </ol> <p>The resulting states and transitions form the canonical collection, with actions (shift, reduce, accept) assigned based on the dot positions and grammar rules. </p> <p></p>"},{"location":"lectures/05_Bottom-up/#structure-of-the-lr-parsing-table","title":"Structure of the LR Parsing Table","text":"<p>The LR parsing table consists of two parts: a parsing-action function <code>ACTION</code> and a goto function <code>GOTO</code>. The <code>ACTION</code> function determines what action the parser should take based on the current state and the next input symbol. The <code>GOTO</code> function determines the next state based on the current state and the next input symbol. </p> <p>example Assume that we have these: <pre><code>(0) S\u2032 \u2192 S$\n(1) S \u2192 CC\n(2) C \u2192 aC\n(3) C \u2192 d\n</code></pre> </p> <p>Now let's construct the LR(0) parsing table: 1. Prepare the Framework:    - Create a grid with rows labeled by states 0 through 6 (matching the canonical collection states I0 to I6).    - Divide the columns into two sections: \"Action\" for terminals ($ a $, $ d $, $ $ ) and \"Goto\" for nonterminals ( S $, $ C $).    - Initialize all cells as empty, ready to be filled based on the state items and transitions. 2. Analyze Each State for Actions:</p> <ul> <li> <p>State 0 ($ I_0 = { [S' \\rightarrow \\cdot SS], [S \\rightarrow \\cdot CC], [C \\rightarrow \\cdot aC], [C \\rightarrow \\cdot d] } $):</p> <ul> <li>$ [C \\rightarrow \\cdot aC] $: $ \\text{GOTO}(I_0, a) = I_2 , so set Action[ a $] = \"S2\" (shift to 2).</li> <li>$ [C \\rightarrow \\cdot d] $: $ \\text{GOTO}(I_0, d) = I_1 , so set Action[ d $] = \"S1\" (shift to 1).</li> <li>No complete items or $ $ -related moves, so Action[ $ $] = \"err\" (error).</li> <li>State 1 ($ I_1 = { [C \\rightarrow d \\cdot] } $):</li> <li>$ [C \\rightarrow d \\cdot] $ is a complete item, reduce by $ C \\rightarrow d $ (assume $ R_3 ), so set Action[ a ] = \"R3\", Action[ d ] = \"R3\", Action[ $ $] = \"R3\".</li> <li> <p>State 2 ($ I_2 = { [C \\rightarrow a \\cdot C], [C \\rightarrow \\cdot aC], [C \\rightarrow \\cdot d] } $):</p> </li> <li> <p>$ [C \\rightarrow \\cdot aC] $: $ \\text{GOTO}(I_2, a) = I_2 , so set Action[ a $] = \"S2\" (shift to 2).</p> </li> <li>$ [C \\rightarrow \\cdot d] $: $ \\text{GOTO}(I_2, d) = I_1 , so set Action[ d $] = \"S1\" (shift to 1).</li> <li>No complete items or $ $ -moves, so Action[ $ $] = \"err\".</li> <li> <p>State 3 ($ I_3 = { [S \\rightarrow C \\cdot C], [C \\rightarrow \\cdot aC], [C \\rightarrow \\cdot d] } $):</p> </li> <li> <p>$ [C \\rightarrow \\cdot aC] $: $ \\text{GOTO}(I_3, a) = I_2 , so set Action[ a $] = \"S2\" (shift to 2).</p> </li> <li>$ [C \\rightarrow \\cdot d] $: $ \\text{GOTO}(I_3, d) = I_1 , so set Action[ d $] = \"S1\" (shift to 1).</li> <li>No complete items or $ $ -moves, so Action[ $ $] = \"err\".</li> <li> <p>State 4 ($ I_4 = { [S' \\rightarrow S \\cdot S] } $):</p> </li> <li> <p>No terminals to shift on, and [S' \\rightarrow S \\cdot S] with $ $ $ completes the parse, so set Action[$ $ $] = \"accept\".</p> </li> <li>No valid moves for $ a $ or $ d , so Action[ a ] = \"err\", Action[ d $] = \"err\".</li> <li> <p>State 5 ($ I_5 = { [C \\rightarrow aC \\cdot] } $):</p> </li> <li> <p>$ [C \\rightarrow aC \\cdot] $ is a complete item, reduce by $ C \\rightarrow aC $ (assume $ R_2 ), so set Action[ a ] = \"R2\", Action[ d ] = \"R2\", Action[ $ $] = \"R2\".</p> </li> <li> <p>State 6 ($ I_6 = { [S \\rightarrow CC \\cdot] } $):</p> </li> <li> <p>$ [S \\rightarrow CC \\cdot] $ is a complete item, reduce by $ S \\rightarrow CC $ (assume $ R_1 ), so set Action[ a ] = \"R1\", Action[ d ] = \"R1\", Action[ $ $] = \"R1\".</p> </li> </ul> </li> <li> <p>Analyze Each State for Gotos:</p> </li> <li>State 0:<ul> <li>$ \\text{GOTO}(I_0, S) = I_4 , so set Goto[ S $] = 4.</li> <li>$ \\text{GOTO}(I_0, C) = I_3 , so set Goto[ C $] = 3.</li> </ul> </li> <li>State 1: No nonterminal transitions, so Goto[$ S ] and Goto[ C $] remain empty.</li> <li>State 2:<ul> <li>$ \\text{GOTO}(I_2, C) = I_5 , so set Goto[ C $] = 5.</li> </ul> </li> <li>State 3:<ul> <li>$ \\text{GOTO}(I_3, C) = I_6 , so set Goto[ C $] = 6.</li> </ul> </li> <li>States 4, 5, 6: No further nonterminal transitions, so Goto columns remain empty.</li> <li>Finalize the Table:</li> <li>Fill in the grid with the determined actions and gotos.</li> <li>Ensure all cells are populated: shifts (S), reduces (R), accept, or errors (err) for Action; state numbers for Goto; leave empty where no transition applies.</li> <li>The resulting table matches the image, with state 0 shifting on $ a $ to 2 (S2) and $ d $ to 1 (S1), going to 4 with $ S $ and 3 with $ C $, and so on, reflecting the parser\u2019s behavior based on the canonical collection.</li> </ul> <p></p> <p>LR(0) Parsing: Example</p> <p>Consider the following parsing table generated from the grammar <code>E -&gt; E + T | T</code> :</p> <pre><code>ACTION GOTO\nState int+;()ET\n0     s9    s8,13\n1     Accept\n2     Reduce E -&gt; T + E\n3     s5,s4\n4     Reduce E -&gt; T;\n5     s9,s8,23\n6     Reduce T -&gt; (E)\n7     s6\n8     s9,s8,73\n9     Reduce T -&gt; int\n</code></pre> <p>This table shows that if the parser is in state 0 and reads an integer, it shifts the integer and moves to state 9. If it reads a plus sign, it reduces <code>E -&gt; T</code> and stays in state 1. If it reads a left parenthesis, it reduces <code>T -&gt; (E)</code> and moves to state 6. And so on.</p> <p>LR(0) DFA Construction: Example</p> <p>Let's consider a slightly more complex grammar:</p> <pre><code>S -&gt; E\nE -&gt; T;\nE -&gt; T + E\nT -&gt; int\nT -&gt; (E)\n</code></pre> <p>We start by adding the initial item <code>S -&gt; .E</code> to the initial state:</p> <pre><code>State 0:\nS -&gt; .E\n</code></pre> <p>We then</p> <p>Structure of the LR Parsing Table: Example</p> <p>Let's consider the grammar <code>G'</code> from the previous example:</p> <p>(0) <code>S'</code> \u2192 <code>S$</code> (1) <code>S \u2192 CC</code> (2) <code>C \u2192 aC</code> (3) <code>C \u2192 d</code></p> <p>The parsing table for this grammar would be constructed based on the item sets and the GOTO and ACTION functions. The exact contents of the table would depend on the specific implementation of the LR parser, but it might look something like this:</p> State Input Action Next State 0 S' Shift 1 1 C Shift 2 2 a Shift 2 2 d Reduce ... ... ... ... <p>The 'Action' column indicates whether the parser should shift (read the next input symbol and push it onto the stack), reduce (apply a grammar rule to the top of the stack), or go to (move to a new state without consuming any input symbols).</p>"},{"location":"lectures/05_Bottom-up/#lr-parsing-algorithm","title":"LR-parsing algorithm","text":"<p>The LR-parsing algorithm works as follows:</p> <ol> <li>Initialize the stack with the start symbol of the grammar and the special end marker <code>$</code>.</li> <li>Read the first input symbol.</li> <li>Look up the current state and input symbol in the ACTION field of the parsing table to get the action to perform.</li> <li>Perform the action: if it's a shift, push the input symbol onto the stack and move to the next state. If it's a reduce, replace the top of the stack with the left-hand side of the grammar rule. If it's a go to, move to the next state without changing the stack.</li> <li>Repeat steps 2-4 until the entire input has been read and the stack contains only the start symbol and the end marker.</li> </ol> <ul> <li>All LR parsers (LR(0), LR(1), LALR(1), and SLR(1)) behave in this fashion,</li> <li>The only difference between one LR parser and another is the information in the ACTION and GOTO fields of the parsing table.</li> </ul> <p>example: Assume that we have these: <pre><code>(0) S\u2032 \u2192 S$\n(1) S \u2192 CC\n(2) C \u2192 aC\n(3) C \u2192 d\n</code></pre>  Now let's parse the following input: <pre><code>add$\n</code></pre> </p>"},{"location":"lectures/05_Bottom-up/#lr0-parser-scope","title":"LR(0) Parser Scope","text":"<p>The scope of an LR(0) parser refers to the set of languages it can recognize. An LR(0) parser can recognize a context-free language if and only if the language is unambiguous and does not contain left recursion.</p>"},{"location":"lectures/05_Bottom-up/#lr0-limitations","title":"LR(0) Limitations","text":"<p>One limitation of LR(0) parsing is that it cannot handle certain types of ambiguous grammars, such as those with left recursion or grammars that require more than one token of lookahead to resolve ambiguities. </p>"},{"location":"lectures/05_Bottom-up/#lr0-parsing-table-with-conflicts","title":"LR(0) Parsing Table with Conflicts","text":"<p>Conflicts in the LR(0) parsing table occur when there are multiple possible actions for a given state and input symbol. These conflicts must be resolved before the parsing table can be used effectively. One common way to resolve conflicts is by using operator precedence and associativity rules, or by transforming the grammar to remove the ambiguity.</p> <p></p> <p>The conflict in state 1 with input $ $ $ is a shift-reduce issue: the parser can either reduce $ S \\rightarrow E $ (r1) or shift to state 6 (s6). This arises because LR(0) lacks lookahead, allowing both actions from items like [S \\rightarrow E \\cdot] and $ [S' \\rightarrow S \\cdot $] $, making the parse ambiguous.</p> <p>LR(0) Grammar: Exercises</p> <p>To construct the LR(0) parsing table for a given grammar, you would typically follow these steps:</p> <ol> <li>Augment the grammar by adding a new start symbol and a new production that starts with the old start symbol followed by a special end marker <code>$</code>.</li> <li>Compute the closure of the set of items derived from the new start symbol.</li> <li>Repeat step 2 for each new state until no more new states can be added.</li> <li>For each state and input symbol, determine the action to take (shift, reduce, or go to) based on the transitions in the state machine.</li> <li>Write the resulting actions into the ACTION and GOTO fields of the parsing table.</li> </ol> <p>The exercise asks you to construct the LR(0) parsing table for several different grammars. You would need to follow the steps above for each grammar.</p> <p>Which one is LR(0)?</p> <p>To determine whether a grammar is suitable for LR(0) parsing, you would need to check whether the grammar is unambiguous and does not contain left recursion. If both conditions are met, then the grammar is suitable for LR(0) parsing.</p> <p>From the given grammars, the ones that are suitable for LR(0) parsing are:</p> <ul> <li><code>S \u2192 AB</code></li> <li><code>S \u2192 Ab</code></li> <li><code>A \u2192 \u03b5</code></li> <li><code>B \u2192 b</code></li> <li><code>S \u2192 A</code></li> <li><code>S \u2192 aa</code></li> <li><code>A \u2192 a</code></li> </ul> <p>The remaining grammars either contain left recursion (<code>S \u2192 AA</code> and <code>A \u2192 aA</code>) or are ambiguous (<code>S \u2192 A</code>), making them unsuitable for LR(0) parsing.</p>"},{"location":"lectures/05_Bottom-up/#simple-lr-parsing-slr-or-slr1","title":"Simple LR Parsing (SLR or SLR(1))","text":"<p>Simple LR parsing, also known as SLR or SLR(1), is an extension of LR(0) parsing. It introduces a single token of lookahead to help resolve conflicts between shift and reduce operations. </p> <p>For each reduction item <code>A \u2192 \u03b3\u00b7</code>, the parser looks at the lookahead symbol <code>c</code>. It applies the reduction only if <code>c</code> is in the <code>FOLLOW(A)</code> set, which contains all the terminal symbols that can appear immediately after <code>A</code> in a sentential form derived from the start symbol.</p>"},{"location":"lectures/05_Bottom-up/#slr-parsing-table","title":"SLR Parsing Table","text":"<p>The SLR parsing table eliminates some conflicts compared to the LR(0) table. It is essentially the same as the LR(0) table, but with reduced rows. Reductions do not fill entire rows. Instead, reductions <code>A \u2192 \u03b3\u00b7</code> are added only in the columns of symbols in <code>FOLLOW(A)</code>.</p>"},{"location":"lectures/05_Bottom-up/#slr-grammar","title":"SLR Grammar","text":"<ul> <li>SLR grammar: A grammar for which the SLR parsing table does not have any conflicts. </li> </ul> <p>An SLR grammar is a context-free grammar for which the SLR parsing table does not have any conflicts. In other words, an SLR grammar is one that can be parsed by an SLR parser without encountering any shift/reduce or reduce/reduce conflicts.</p> <p>example: </p> <p>Now we compute the SLR parsing tabble for the previous conficting LR(0) table.  Here instead of putting the r1 in all the rows we compute the FOLLOW(S) in production <code>S \u2192 .E</code>. In this case only $ is in the follow set so other cells in the column get empty, so the conflict gets resolved.</p> <p>SLR Parsing: Example</p> <p>The SLR parsing algorithm is similar to the LR(0) parsing algorithm, but with the addition of considering the lookahead symbol when deciding whether to shift or reduce. Here is a simplified version of the algorithm:</p> <ol> <li>Initialize the stack with the start symbol of the grammar and the special end marker <code>$</code>.</li> <li>Read the first input symbol.</li> <li>Look up the current state and input symbol in the ACTION field of the parsing table to get the action to perform.</li> <li>If the action is a shift, push the input symbol onto the stack and move to the next state. If the action is a reduce, replace the top of the stack with the left-hand side of the grammar rule. If the action is a go to, move to the next state without changing the stack.</li> <li>Repeat steps 2-4 until the entire input has been read and the stack contains only the start symbol and the end marker.</li> </ol> <p>SLR Parsing: Example</p> <p>Let's consider a simple grammar:</p> <pre><code>E -&gt; E + T | T\nT -&gt; F | (E)\nF -&gt; id\n</code></pre> <p>And the example input <code>id + id</code>.</p> <p>We start by initializing the stack with the start symbol <code>E</code> and the end marker <code>$</code>. Then we read the first input symbol <code>id</code>. According to the parsing table, we shift <code>id</code> onto the stack and move to state 2. We continue this process until we have consumed all the input symbols.</p> <p>At the end of the parsing process, if the stack contains only the start symbol and the end marker, the input string is accepted. Otherwise, it is rejected.</p> <p>example SLR collection:  SLR table: </p>"},{"location":"lectures/05_Bottom-up/#slr-parsing","title":"SLR Parsing","text":"<p>SLR (Simple LR) parsing is an extension of LR(0) parsing. It introduces a single token of lookahead to help resolve conflicts between shift and reduce operations. For each reduction item <code>A \u2192 \u03b3\u00b7</code>, the parser looks at the lookahead symbol <code>c</code>. It applies the reduction only if <code>c</code> is in the <code>FOLLOW(A)</code> set, which contains all the terminal symbols that can appear immediately after <code>A</code> in a sentential form derived from the start symbol.</p>"},{"location":"lectures/05_Bottom-up/#slr-parsing-scope","title":"SLR Parsing Scope","text":"<p>The scope of an SLR parser refers to the set of languages it can recognize. An SLR parser can recognize a context-free language if and only if the language is unambiguous and does not contain left recursion. However, unlike LR(0) parsers, SLR parsers can handle certain types of ambiguous grammars by considering the lookahead symbol when deciding whether to shift or reduce.</p>"},{"location":"lectures/05_Bottom-up/#slr-parsing-limitations","title":"SLR Parsing Limitations","text":"<p>Despite its advantages, SLR parsing still has limitations. One of the main limitations is that it cannot handle certain types of ambiguous grammars, such as those with left recursion or grammars that require more than one token of lookahead to resolve ambiguities. Additionally, the SLR parsing algorithm requires additional computational resources compared to LR(0) parsing due to the need to calculate the <code>FOLLOW(A)</code> set for each non-terminal. example <pre><code>(0) S\u2032 \u2192 S$\n(1) S \u2192 AaAb\n(2) S \u2192 BbBa\n(3) A \u2192 \u03b5\n(4) B \u2192 \u03b5\n</code></pre></p> <p></p> <p>To verify the type of a given grammer you should construct the parsing table for each type and if in had no conflicts, the grammar is of that type.</p> <p>Show That the Following Grammar Is Not SLR</p> <p>Let's consider the following grammar:</p> <pre><code>(0) S' \u2192 S$\n(1) S \u2192 AaAb\n(2) S \u2192 BbBa\n(3) A \u2192 \u03b5\n(4) B \u2192 \u03b5\n</code></pre> <p>If we try to construct the SLR parsing table for this grammar, we will encounter two reduce-reduce conflicts in the action cells for <code>state 0, a</code> and <code>state 0, b</code>. This is because both <code>A \u2192 \u03b5</code> and <code>B \u2192 \u03b5</code> are applicable in these cases, and neither can be decided upon solely based on the lookahead symbol. Therefore, this grammar is not SLR.</p> <p>In general, to show that a grammar is not SLR, we need to construct the SLR parsing table and check for any reduce-reduce conflicts. If we find any such conflicts, then the grammar is not SLR.</p> <p>SLR Parsing: Example</p> <p>Let's consider a simple grammar:</p> <pre><code>E -&gt; E + T | T\nT -&gt; F | (E)\nF -&gt; id\n</code></pre> <p>And the example input <code>id + id</code>.</p> <p>We start by initializing the stack with the start symbol <code>E</code> and the end marker <code>$</code>. Then we read the first input symbol <code>id</code>. According to the parsing table, we shift <code>id</code> onto the stack and move to state 2. We continue this process until we have consumed all the input symbols.</p> <p>At the end of the parsing process, if the stack contains only the start symbol and the end marker, the input string is accepted. Otherwise, it is rejected.</p>"},{"location":"lectures/05_Bottom-up/#lr1-parsing-and-lr1-grammars","title":"LR(1) Parsing and LR(1) Grammars","text":"<p>LR(1) parsing, also known as canonical LR(1) parsing, is an extension of LR(0) parsing. It uses similar concepts as SLR, but it uses one lookahead symbol instead of none. The idea is to get as much as possible out of one lookahead symbol. The LR(1) item is an LR(0) item combined with lookahead symbols possibly following the production locally within the same item set. For instance, an LR(0) item could be <code>S \u2192 \u00b7S + E</code>, and an LR(1) item could be <code>S \u2192 \u00b7S + E , +</code>. Similar to SLR parsing, lookahead only impacts reduce operations in LR(1). If the LR(1) parsing action function has no multiply defined entries, then the given grammar is called an LR(1) grammar.</p>"},{"location":"lectures/05_Bottom-up/#lr1-closure","title":"LR(1) Closure","text":"<p>Similar to LR(0) closure, but also keeps track of the lookahead symbol. If <code>I</code> is a set of items, <code>CLOSURE(I)</code> is the set of items such that:</p> <ol> <li>Initially, every item in <code>I</code> is in <code>CLOSURE(I)</code>,</li> <li>If <code>A \u2192 \u03b1 \u00b7 B</code> and <code>B \u2192 \u03b3</code> is a production whose closures are not in <code>I</code> then add the item <code>B \u2192 \u03b3</code> , <code>FIRST(\u03b2)</code> to <code>CLOSURE(I)</code>.</li> <li>In step (2) if <code>\u03b2 \u2192 \u03b5</code> then add the item <code>B \u2192 \u03b3 , \u03b4</code> to <code>CLOSURE(I)</code>.</li> <li>For recursive items with form <code>A \u2192 \u00b7A\u03b1 , \u03b4</code> and <code>`A \u2192 \u00b7\u03b2 , \u03b4</code> replace the items with <code>A \u2192 \u00b7A\u03b1 , \u03b4, FIRST(\u03b1)</code> and <code>A \u2192 \u00b7\u03b2 , \u03b4, FIRST(\u03b1)</code>.</li> <li>Apply these steps (2), (3), and (4) until no more new items can be added to <code>CLOSURE(I)</code>.</li> </ol>"},{"location":"lectures/05_Bottom-up/#lr1-goto-and-states","title":"LR(1) GOTO and States","text":"<p>Initial state: start with <code>[S' \u2192 S$ , $]</code> as the kernel of <code>I0</code>, then apply the <code>CLOSURE(I)</code> operation. The GOTO function is analogous to GOTO in LR(0) parsing.</p>"},{"location":"lectures/05_Bottom-up/#lr1-items","title":"LR(1) Items","text":"<p>An LR(1) item is a pair <code>[\u03b1; \u03b2]</code>, where <code>\u03b1</code> is a production from the grammar with a dot at some position in the RHS and <code>\u03b2</code> is a lookahead string containing one symbol (terminal or EOF). What about LR(1) items? Several LR(1) items may have the same core. For instance, <code>[A ::= X \u00b7 Y Z; a]</code> and <code>[A ::= X \u00b7 Y Z; b]</code> would be represented as <code>[A ::= X \u00b7 Y Z; {a, b}]</code>.</p> <p>LR(1) Parsing Table: Example</p> <p>Let's consider the following grammar:</p> <pre><code>(0) S' \u2192 S$\n(1) S \u2192 CC\n(2) C \u2192 aC\n(3) C \u2192 d\n</code></pre> <p>First, we augment the grammar by adding a new start symbol and a new production that starts with the old start symbol followed by a special end marker <code>$</code>. Then we compute the closure of the set of items derived from the new start symbol. We repeat this process for each new state until no more new states can be added. For each state and input symbol, we determine the action to take (shift, reduce, or go to) based on the transitions in the state machine. Finally, we write the resulting actions into the ACTION and GOTO fields of the parsing table.  In an LR(1) parser, lookaheads are computed as part of constructing the LR(1) items within each state of the canonical collection. For the given grammar $ (0) S' \\rightarrow S$ $, $ (1) S \\rightarrow CC $, $ (2) C \\rightarrow aC $, $ (3) C \\rightarrow d $, the lookaheads are determined during the closure and GOTO operations. Here's how they are computed step by step:</p> <ol> <li> <p>Initialize with the Augmented Production:</p> </li> <li> <p>Start with the initial item $ [S' \\rightarrow \\cdot S, $] $, where the lookahead $ $ $ is taken from the right side of the augmented production $ S' \\rightarrow S$ $, as $ $ $ is the end marker.</p> </li> <li> <p>Compute Closure:</p> </li> <li> <p>For each item $ [A \\rightarrow \\alpha \\cdot B \\beta, a] $ in a state (where $ B $ is a nonterminal), add $ [B \\rightarrow \\cdot \\gamma, b] $ for every production $ B \\rightarrow \\gamma $, where $ b $ is any terminal that can follow $ A \\rightarrow \\alpha B \\beta $.</p> </li> <li>Example: In state 0 with $ [S' \\rightarrow \\cdot S, $] $, add $ [S \\rightarrow \\cdot CC, $] $ because $ $ $ follows $ S $ in $ S' \\rightarrow S$ $.</li> <li> <p>For $ [S \\rightarrow \\cdot CC, $] $, add $ [C \\rightarrow \\cdot aC, $] $ and $ [C \\rightarrow \\cdot d, $] $, as $ $ $ can follow $ C $ in the context of $ S \\rightarrow CC $ followed by $ S' \\rightarrow S$ $.</p> </li> <li> <p>Compute GOTO and Propagate Lookaheads:</p> </li> <li> <p>When moving the dot past a symbol $ X $ to form $ \\text{GOTO}(I, X) $, the lookahead $ a $ from $ [A \\rightarrow \\alpha \\cdot X \\beta, a] $ is carried over to the new item $ [A \\rightarrow \\alpha X \\cdot \\beta, a] $.</p> </li> <li>Example: From $ [S' \\rightarrow \\cdot S, $] $ in state 0, $ \\text{GOTO}(I_0, S) = I_{14} $ with $ [S' \\rightarrow S \\cdot, $] $, keeping $ $ $ as the lookahead.</li> <li> <p>For $ [S \\rightarrow C \\cdot C, $] $ in state 3, $ \\text{GOTO}(I_3, C) = I_6 $ with $ [S \\rightarrow CC \\cdot, $] $, retaining $ $ $.</p> </li> <li> <p>Handle Epsilon and Follow Sets:</p> </li> <li> <p>If $ \\beta $ is empty (e.g., a reduction item), the lookahead $ a $ is determined by the FOLLOW set of $ A $, computed from the context where $ A $ appears. Since $ S' \\rightarrow S$ $ is the only context, $ FOLLOW(S) = { $ } $, and this propagates to all reductions.</p> </li> <li> <p>Example: $ [C \\rightarrow d \\cdot, $] $ in state 10 gets $ $ $ from $ FOLLOW(C) $ via $ S \\rightarrow CC $.</p> </li> <li> <p>Iterate Until Complete:</p> </li> <li> <p>Repeat closure and GOTO for new states, ensuring all lookaheads are propagated or computed based on the grammar\u2019s structure and the initial $ $ $ from $ S' \\rightarrow S$ $.</p> </li> </ol> <p></p> <p>LR(1) Parsing Table: Exercise</p> <p>Let's consider the following grammar:</p> <pre><code>S \u2192 E + S | E\nE \u2192 num\n</code></pre> <p>We would follow the same steps as in the example to construct the LR(1) parsing table for this grammar. However, since the grammar is quite simple, the resulting table should also be relatively straightforward.</p> <p>Remember, the goal is to identify any conflicts in the parsing table. A conflict occurs when there are multiple possible actions for a given state and input symbol. If there are no conflicts, then the grammar is suitable for LR(1) parsing.</p>"},{"location":"lectures/05_Bottom-up/#lalr1","title":"LALR(1)","text":"<ul> <li>LALR (Look-Ahead LR) parser: Simple technique to eliminate and minimize LR(1) states.</li> <li>Technique: If two LR(1) states are identical except for the look ahead symbol of their items, merge them.</li> <li>It is more memory efficient, typically merges several LR(1) states.</li> <li>May also have more reduce/reduce conflicts.</li> <li>Power of LALR parsing is enough for many mainstream computer languages</li> <li>Several automatic parser generators such as YACC or GNU Bison. </li> </ul>"},{"location":"lectures/05_Bottom-up/#lr1-construction","title":"LR(1) Construction","text":"<p>The LR(1) construction extends the LR(0) automaton construction to an LR(1) automaton. Just as in the LR(0) automaton, the states are a set of items that is closed under prediction. However, the items now contain a set of lookahead tokens. Thus, an LR(1) item has the form <code>X\u2192\u03b1.,\u03b2~~~~\u03bb</code>, where the symbols <code>\u03b1</code> represent the top of the automaton stack, the dot represents the current input position, the symbols <code>\u03b2</code> derive possible future input, and the set of tokens <code>\u03bb</code> describes tokens that could appear in the input stream after the derivation of <code>\u03b2</code> .</p>"},{"location":"lectures/05_Bottom-up/#lalr-grammars","title":"LALR Grammars","text":"<p>The number of LR(1) states is often unnecessarily large, because the LR(1) automaton ends up with many states that are identical other than lookahead tokens. This insight leads to LALR automata. An LALR automaton is exactly the same as an LR(1) automaton except that it merges together all states that are identical other than lookaheads. In the merge, the lookahead sets are combined for each item. Many parser generators are based on LALR, including commonly used software like yacc, bison, CUP, mlyacc, ocamlyacc, and Menhir.</p> <p>While LALR is in practice just about as good as full LR, it does occasionally lose some expressive power. To see why, consider what happens when the two LR(1) states in the following diagram are merged to form the state marked M: The two states on the top are unambiguous LR(1) states in which the lookahead character indicates which of the two productions to reduce. But when merged, the resulting state has reduce\u2013reduce conflicts on both <code>+</code> and <code>$</code>. When merging LR(1) states creates a new conflict, the grammar must be LR(1) but not LALR(1) .</p>"},{"location":"lectures/05_Bottom-up/#lalr1-parser-behavior","title":"LALR(1) Parser Behavior","text":"<p>The LALR(1) parser behaves similarly to the LR(1) parser for correct inputs, producing the same sequence of reduce and shift actions. On an incorrect input, the LALR parser produces the same sequence of actions up to the last shift action, although it might then do a few more reduce actions before reporting the error. So although the LALR parser has fewer states, its behavior is identical for correct inputs, and extremely similar for incorrect inputs .</p> <p>Let's illustrate LALR(1) parsing with an example. Consider the following context-free grammar:</p> <pre><code>A -&gt; CxA | \u03b5\nB -&gt; xC y | xC\nC -&gt; xB x | z\n</code></pre> <p>And suppose we want to parse the input string <code>xxzxx</code>.</p> <p>First, we need to construct the LALR(1) parsing table. This table guides the parsing process by mapping pairs of the form (state, input symbol) to actions (shift, reduce, accept, or error). The construction of the LALR(1) parsing table involves several steps, including the calculation of the FIRST and FOLLOW sets, the creation of the canonical collection of LR(1) items, and the merging of states to eliminate reduce-reduce conflicts.</p> <p>Once the LALR(1) parsing table is constructed, we can start parsing the input string. At each step, we examine the top symbol of the stack and the next input symbol, and follow the corresponding entry in the parsing table to decide whether to shift, reduce, or accept.</p> <p>Here's a simplified version of the parsing process for the input string <code>xxzxx</code>:</p> <pre><code>State Stack Input Action\n0    A    xxzxx Shift\n1    Ax   zxx  Reduce A -&gt; CxA\n2    CxA  zxx  Shift\n3    zxA  xx   Shift\n4    xxzxA xx   Reduce B -&gt; xC y\n5    xy   xx   Reduce C -&gt; xB x\n6    xx   zx   Reduce C -&gt; z\n7    z    xx   Reduce A -&gt; \u03b5\n8    xx   xx   Accept\n</code></pre> <p>In this example, the LALR(1) parser successfully parses the input string <code>xxzxx</code> according to the given grammar. Note that the actual parsing process would involve more steps and more complex entries in the parsing table.</p> <p>Keep in mind that this is a simplified example. In practice, the construction of the LALR(1) parsing table can be complex and time-consuming, especially for larger grammars. Also, the LALR(1) parser may produce different results for incorrect inputs, depending on how the parsing table is constructed and how conflicts are resolved.</p> <p>Let's illustrate LALR(1) parsing with an example. Consider the following context-free grammar:</p> <pre><code>S -&gt; xAy | xBy | xAz\nA -&gt; aS | q\nB -&gt; q\n</code></pre> <p>We want to parse the input string <code>xqy</code>.</p> <p>First, let's construct the LALR(1) parsing table. The construction of the LALR(1) parsing table involves several steps, including the calculation of the FIRST and FOLLOW sets, the creation of the canonical collection of LR(1) items, and the merging of states to eliminate reduce-reduce conflicts.</p> <p>Once the LALR(1) parsing table is constructed, we can start parsing the input string. At each step, we examine the top symbol of the stack and the next input symbol, and follow the corresponding entry in the parsing table to decide whether to shift, reduce, or accept.</p> <p>Here's a simplified version of the parsing process for the input string <code>xqy</code>:</p> <pre><code>State Stack Input Action\n0   S   xqy Shift\n1   Sx  qy  Reduce A -&gt; q\n2   Sxq y   Shift\n3   Sxy q   Reduce S -&gt; xBy\n4   By  q   Reduce B -&gt; q\n5   S   q   Accept\n</code></pre> <p>In this example, the LALR(1) parser successfully parses the input string <code>xqy</code> according to the given grammar. Note that the actual parsing process would involve more steps and more complex entries in the parsing table.</p> <p>Keep in mind that this is a simplified example. In practice, the construction of the LALR(1) parsing table can be complex and time-consuming, especially for larger grammars. Also, the LALR(1) parser may produce different results for incorrect inputs, depending on how the parsing table is constructed and how conflicts are resolved. example Assume that we have these: <pre><code>(0) S' \u2192 S$\n(1) S \u2192 CC\n(2) C \u2192 aC\n(3) C \u2192 d\n</code></pre>  Now let's convert it to LALR: Here I1 and I6 share the same productions but with different lookaheads so they can be merged. it is the same  i2 and I7, and I5 and I9.  After merging, combine the itmes' names (I1 and I6 -&gt; I1-6). For GOTO actions replace them with the new names.   After that constuct the parsing table. </p>"},{"location":"lectures/05_Bottom-up/#lalr1-parsing-scope","title":"LALR(1) Parsing scope","text":"<p>As mentioned before merging can result in conflicts. you can se an example below:</p> <p>Merging I1 and I7 results in reduce-reduce conflicts (R3/ R4) in action[1-7,a] and action[1-7, b]. </p>"},{"location":"lectures/05_Bottom-up/#embrace-the-power-of-ambiguous-grammars","title":"Embrace the Power of Ambiguous Grammars","text":"<p>Every ambiguous grammar fails to be LR and thus is not part of any of the classes of the LR grammars discussed. Yet, certain types of ambiguous grammars prove to be quite useful in the specification and implementation of languages. For language constructs like expressions, an ambiguous grammar provides a shorter, more natural specification than any equivalent unambiguous grammar. Furthermore, ambiguous grammars result in fewer productions, leading to parsing tables with a smaller size. Disambiguating rules that allow only one parse tree for each sentence add to the appeal of ambiguous grammars.</p>"},{"location":"lectures/05_Bottom-up/#the-impact-of-unambiguous-vs-ambiguous-grammars","title":"The Impact of Unambiguous vs. Ambiguous Grammars","text":"<p>Consider the parsing tables for an unambiguous expression grammar. The SLR parsing table contains 12 rows, while the LR(1) parsing table contains 22 rows. This difference in size underscores the benefits of ambiguous grammars. They simplify the specification of language constructs and reduce the size of the parsing tables, making them more manageable and efficient.</p>"},{"location":"lectures/05_Bottom-up/#resolving-conflicts-with-precedence-and-associativity","title":"Resolving Conflicts with Precedence and Associativity","text":"<p>When dealing with ambiguous grammars, conflicts can arise due to operator precedence and associativity. For instance, in an augmented expression grammar, the sets of LR(0) items can reveal potential conflicts. However, by carefully applying precedence and associativity rules, these conflicts can be effectively resolved.</p>"},{"location":"lectures/05_Bottom-up/#error-recovery-in-lr-parsing","title":"Error recovery in LR parsing","text":"<p>LR parsing is a powerful technique used in computer programming language compilers and other associated tools. One of the challenges it faces is handling errors. An LR parser will detect an error when it consults the parsing action table and finds an error entry. Errors are never detected by consulting the goto table. </p>"},{"location":"lectures/05_Bottom-up/#panic-mode-error-recovery","title":"Panic-Mode Error Recovery","text":"<ul> <li>An LR parser will detect an error when it consults the parsing action table and finds an error entry.</li> <li>Errors are never detected by consulting the goto table.</li> <li> <p>In LR parsing, we can implement panic-mode error recovery as follows:</p> </li> <li> <p>Scan down the stack until a state <code>s</code> with a goto on a particular nonterminal <code>A</code> is found.</p> </li> <li>Zero or more input symbols are then discarded until a symbol <code>a</code> is found that can legitimately follow <code>A</code>.</li> <li>The parser then stacks the state <code>GOTO(s, A)</code> and resumes normal parsing.</li> </ul> <p>This approach allows the parser to recover from errors by discarding erroneous input and resuming normal parsing.</p>"},{"location":"lectures/05_Bottom-up/#phrase-level-error-recovery","title":"Phrase-Level Error Recovery","text":"<ul> <li>Phrase-level recovery is another strategy for handling errors in LR parsing. This approach involves examining each error entry in the LR parsing table and deciding, based on language usage, the most likely programmer error that would give rise to that error. </li> <li> <p>An appropriate recovery procedure can then be constructed, presumably modifying the top of the stack and/or first input symbols in a way deemed appropriate for each error entry.</p> </li> <li> <p>In designing specific error-handling routines for an LR parser, we can fill in each blank entry in the action field with a pointer to an error routine that will take the appropriate action selected by the compiler designer. </p> </li> <li>The actions may include insertion or deletion of symbols from the stack or the input or both, or alteration and transposition of input symbols. </li> <li>We must make our choices so that the LR parser will not get into an infinite loop.</li> </ul> <p>example: LR parsing table with error routines for the expression grammar: <pre><code>E \u2192 E + E \nE \u2192 E * E \nE \u2192 ( E ) \nE \u2192 id\n</code></pre> </p> <ul> <li>e1:</li> <li>push state 3 (the goto of states 0, 2, 4 and 5 on id);</li> <li>issue diagnostic \"missing operand.\"</li> <li>e2:</li> <li>remove the right parenthesis from the input;</li> <li>issue diagnostic \"unbalanced right parenthesis.\"</li> <li>e3:</li> <li>push state 4 (corresponding to symbol +) onto the stack;</li> <li>issue diagnostic \"missing operator.</li> <li>e4:</li> <li>push state 9 (for a right parenthesis) onto the stack;</li> <li>issue diagnostic \"missing right parenthesis.\"</li> </ul> <p>Example of Error Recovery Using LR Parser</p> <p>Consider the following grammar:</p> <pre><code>E \u2192 E + E \nE \u2192 E * E \nE \u2192 ( E ) \nE \u2192 id\n</code></pre> <p>Suppose we want to parse the input string <code>id+)$</code>. Here's how the LR parser would work:</p> <pre><code>STACK INPUT\n0    id+)$\n0id3 +)$\n0E1  +)$\n0E1 + 4)$\n0E1 + 4id3\n0E1 + 4E7\n0E1\n$\n</code></pre> <p>In this example, the LR parser successfully parses the input string <code>id+)$</code> according to the given grammar. Note that the actual parsing process would involve more steps and more complex entries in the parsing table.</p> <p>Understanding and mastering these error recovery strategies can significantly improve the robustness and reliability of LR parsers.</p>"},{"location":"lectures/05_Bottom-up/#the-yacc-parser-generator","title":"The YACC parser generator","text":"<p>YACC (Yet Another Compiler-Compiler) is a powerful tool used in the field of computer science to generate parsers for compilers. It was designed to produce a parser from a given grammar specification, which is a set of rules defining the syntax of a particular language. The parser generated by YACC is an LALR (Look-Ahead, Left-to-Right, Rightmost derivation with 1 lookahead token) parser, operating from left to right and trying to derive the rightmost element of a sentence in a sentence structure according to the grammar.</p> <p>YACC works in three main parts: declarations, translation rules, and supporting C routines. Each part plays a crucial role in the process of generating a parser for a given language.</p>"},{"location":"lectures/05_Bottom-up/#declarations","title":"Declarations","text":"<p>The declarations section of a YACC specification includes information about the tokens used in the syntax definition. Tokens are the smallest units of meaningful data in a programming language, and they could be keywords, identifiers, operators, literals, etc. YACC automatically assigns numbers for tokens, but this can be overridden by specifying a number after the token name. For example, <code>%token NUMBER 621</code>. YACC also recognizes single characters as tokens, so the assigned token numbers should not overlap with ASCII codes.</p> <pre><code>%token NUMBER \n%token ID\n</code></pre>"},{"location":"lectures/05_Bottom-up/#translation-rules","title":"Translation Rules","text":"<p>The translation rules section contains grammar definitions in a modified BNF (Backus-Naur Form) form. These rules define how the parser should interpret sequences of tokens. Each rule in YACC has a string specification that resembles a production of a grammar. It has a nonterminal on the left-hand side (LHS) and a few alternatives on the right-hand side (RHS). YACC generates an LALR(1) parser for the language from the productions, which is a bottom-up parser.</p> <pre><code>%%\n/* rules */ \n....\n%% \n</code></pre>"},{"location":"lectures/05_Bottom-up/#supporting-c-routines","title":"Supporting C Routines","text":"<p>The supporting C routines section includes C code external to the definition of the parser and variable declarations. It can also include the specification of the starting symbol in the grammar: <code>%start nonterminal</code>. If the <code>yylex()</code> function is not defined in the auxiliary routines sections, then it should be included with <code>#include \"lex.yy.c\"</code>. If the YACC file contains the <code>main()</code> definition, it must be compiled to be executable.</p> <pre><code>/* auxiliary routines */\n....\n</code></pre>"},{"location":"lectures/05_Bottom-up/#building-a-c-compiler-using-lex-and-yacc","title":"Building a C Compiler using Lex and Yacc","text":"<p>Building a C compiler involves several steps, including writing the lexical analyzer (Lex), writing the syntax analyzer (Yacc), and integrating the two. The Lex program reads the source code and breaks it down into tokens, while the Yacc program takes these tokens and checks if they conform to the grammar of the language.</p> <p>To build a C compiler, you start by writing the Lex file, which defines the tokens for the C language. After that, you write the Yacc file, which defines the grammar of the C language and specifies how the tokens should be parsed. Once you have both Lex and Yacc files ready, you can integrate them by adding the <code>#include \"lex.yy.c\"</code> statement in the Yacc file. Then, you can compile the Yacc file using the <code>yacc -v -d parser1.y</code> command and link it with the Lex library using the <code>gcc -ll y.tab.c</code> command.</p> <p>example</p> <p></p>"},{"location":"lectures/05_Bottom-up/#using-yacc-with-ambiguous-grammars","title":"Using YACC with ambiguous grammars","text":"<ul> <li>By default YACC will resolve all parsing action conflict using the following two rules:</li> <li>A reduce/reduce conflict is resolved by choosing the    conflicting production listed first in the YACC specification.</li> <li>A shift/reduce conflict is resolved in favor of shift.</li> <li>Since these default rules may not always be what the compiler writer wants, YACC provides a general mechanism for resolving shift/reduce conflicts.</li> <li>That is precedences and associativities to terminals:</li> <li>%left \u2019+\u2019 \u2019-\u2019,</li> <li>%right \u2019=\u2019,</li> <li>%nonassoc \u2019&lt;\u2019</li> </ul>"},{"location":"lectures/05_Bottom-up/#yacc-exercises","title":"YACC Exercises","text":"<ol> <li>Implement both versions of simple and advanced calculator compilers with YACC or GNU BISON.</li> <li>Add the exponent operator, \u2227, to your calculator.</li> <li>Add error recovery methods discussed in the previous section to your calculator compiler.</li> <li>Discuss other bottom-up parser generators (e.g., GNU BISON) within your groups.</li> </ol>"},{"location":"lectures/05_Bottom-up/#cyk-parsing-algorithm","title":"CYK Parsing Algorithm","text":""},{"location":"lectures/05_Bottom-up/#designing-more-powerful-parsers","title":"Designing More Powerful Parsers","text":"<p>Designing more powerful parsers often involves considering different parsing methods. Two of the most prevalent types of parsers are LL and LR parsers. These parsers are deterministic, directional, and operate in linear time, making them capable of recognizing restricted forms of context-free grammars.</p> <p>However, parsing more grammars non-directionally poses a challenge, especially when we consider allowing more time-consuming algorithms. To address this, we can employ various strategies:</p> <ul> <li>Brute Forcing: This approach involves enumerating everything possible. While straightforward, it can be computationally expensive due to its exhaustive nature.   </li> <li>Backtracking: This strategy involves trying different subtrees and discarding partial solutions if they prove unsuccessful. This allows the parser to explore different branches of the parse tree until a valid parse is found.</li> <li>Dynamic Programming: This method involves saving partial solutions in a table for later use. This technique is efficient as it avoids recomputation by storing the result of a subproblem and reusing it when needed. However, dynamic programming requires a non-directional parsing method, which is not inherent in LL and LR parsers.</li> </ul>"},{"location":"lectures/05_Bottom-up/#directionality-of-parsing-methods","title":"Directionality of Parsing Methods","text":"<p>Parsing methods can be categorized into two types: directional and non-directional methods.</p> <p>Directional methods process the input symbol by symbol from left to right. LL and LR parsers fall under this category. The advantage of directional methods is that parsing starts and makes progress before the last symbol of the input is seen.</p> <p>Non-directional methods, on the other hand, allow access to input in an arbitrary order. They require the entire input to be in memory before parsing can start. This flexibility allows non-directional parsers to handle more flexible grammars than directional parsers. An example of a non-directional parser is the CYK parser.</p>"},{"location":"lectures/05_Bottom-up/#cyk-cocke-younger-kasami","title":"CYK (Cocke-Younger-Kasami)","text":"<p>The CYK algorithm is one of the earliest recognition and parsing algorithms, developed independently by three Russian scientists: J. Cocke, D.H. Younger, and T. Kasami. This algorithm uses a bottom-up parsing approach, reducing already recognized right-hand sides of a production rule to its left-hand side non-terminal. As it is non-directional, it accesses input in an arbitrary order, necessitating the entire input to be in memory before parsing can begin. - Bottom-up parsing (starts with terminals): reduces already recognized right-hand side of a production rule to its left-hand side non-terminal - Non-directional: accesses input in arbitrary order so requires the entire input to be in memory before parsing can start</p>"},{"location":"lectures/05_Bottom-up/#cyk-parsing","title":"CYK Parsing","text":"<p>The CYK algorithm operates on a dynamic programming or table-filling approach. It constructs solutions compositionally from sub-solutions. Notably, the CYK algorithm recognizes any context-free grammar in Chomsky Normal Form. It operates on a binary parse tree.</p>"},{"location":"lectures/05_Bottom-up/#chomsky-normal-form","title":"Chomsky Normal Form","text":"<p>A Context-Free Grammar (CFG) is said to be in Chomsky Normal Form (CNF) if each rule is of the form <code>A \u2192 BC</code> or <code>A \u2192 a</code>, where <code>a</code> is any terminal, and <code>A</code>, <code>B</code>, <code>C</code> are non-terminals. <code>B</code> and <code>C</code> cannot be the start variable. We allow the rule <code>S \u2192 \u03b5</code> if <code>\u03b5</code> is in <code>L</code>.</p>"},{"location":"lectures/05_Bottom-up/#cyk-algorithm-basic-idea","title":"CYK Algorithm: Basic Idea","text":"<p>The CYK algorithm works as follows:</p> <ol> <li>For a grammar <code>G</code> (in CNF) and a word (or string) <code>w</code>:</li> <li>For every substring <code>v1</code> of length 1, find all non-terminals <code>A</code> such that <code>Av1 \u21d2 *</code>.</li> <li>For every substring <code>v2</code> of length 2, find all non-terminals <code>A</code> such that <code>Av2 \u21d2 *</code>.</li> <li>...</li> <li>For the unique substring <code>w</code> of length <code>|w|</code>, find all non-terminals <code>A</code> such that <code>Aw \u21d2 *</code>.</li> <li>Check whether the start symbol <code>S</code> belongs to the last set.</li> </ol> <p>For more details about converting a CFG grammar to the CNF form, please refer to the appendix slides.</p>"},{"location":"lectures/05_Bottom-up/#solution-representation-for-substring-recognition","title":"Solution Representation for Substring Recognition","text":"<p>In the context of the CYK (Cocke-Younger-Kasami) algorithm, solution representation for substring recognition is essential. The CYK algorithm is a dynamic programming algorithm that is used for parsing context-free grammars. It constructs parse trees by recognizing and combining smaller parse trees. Therefore, the ability to recognize specific subsequences within a larger string is crucial. </p>"},{"location":"lectures/05_Bottom-up/#substring-recognition","title":"Substring Recognition","text":"<p>Substring recognition is a key component of the CYK algorithm. It involves identifying and recognizing specific subsequences within a larger string. This process is fundamental to the operation of the CYK algorithm, which constructs parse trees by recognizing and combining smaller parse trees. The recognition of these smaller parse trees is done through the recognition table. </p>"},{"location":"lectures/05_Bottom-up/#recognition-table","title":"Recognition Table","text":"<p>The recognition table is a two-dimensional array that is used to store the potential non-terminal symbols that can generate a particular substring of the input string. Each cell in the table represents a substring of the input string. The value of a cell indicates the set of non-terminal symbols that can generate the corresponding substring. This table is filled iteratively, with each cell being updated based on the values of its neighboring cells. </p>"},{"location":"lectures/05_Bottom-up/#cyk-algorithm-example-trace","title":"CYK Algorithm: Example Trace","text":"<ol> <li>Initialization (Diagonal - Length 1 substrings):</li> <li>For each character in \"babab\":<ul> <li>Position 0: 'b' \u2192 B (from B \u2192 b)</li> <li>Position 1: 'a' \u2192 A or C (from A \u2192 a, C \u2192 a)</li> <li>Position 2: 'b' \u2192 B</li> <li>Position 3: 'a' \u2192 A or C</li> <li>Position 4: 'b' \u2192 B</li> </ul> </li> <li>Table (first row):<ul> <li>(0,0): {B}</li> <li>(1,1): {A, C}</li> <li>(2,2): {B}</li> <li>(3,3): {A, C}</li> <li>(4,4): {B}</li> </ul> </li> <li>Length 2 Substrings:</li> <li>(0,1): 'ba' \u2192 Check A \u2192 BA: B (from 0) and A (from 1) \u2192 {A}</li> <li>(1,2): 'ab' \u2192 No direct match, but check B \u2192 BC: B (from 1) and C (from 2) \u2192 {B} if C is possible later</li> <li>(2,3): 'ba' \u2192 {A}</li> <li>(3,4): 'ab' \u2192 No direct match</li> <li>Table update:<ul> <li>(0,1): {A}</li> <li>(1,2): {}</li> <li>(2,3): {A}</li> <li>(3,4): {}</li> </ul> </li> <li>Length 3 Substrings:</li> <li>(0,2): 'bab' \u2192 Check S \u2192 AB: A (from 0,1) and B (from 2) \u2192 {S}</li> <li>(1,3): 'aba' \u2192 Check S \u2192 CB: C (from 1) and B (from 3) \u2192 {S} if C is confirmed</li> <li>(2,4): 'bab' \u2192 {S}</li> <li>Table update:<ul> <li>(0,2): {S}</li> <li>(1,3): {S}</li> <li>(2,4): {S}</li> </ul> </li> <li>Length 4 Substrings:</li> <li>(0,3): 'baba' \u2192 Check S \u2192 AB: A (from 0,2) and B (from 3) \u2192 {S}</li> <li>(1,4): 'abab' \u2192 Check S \u2192 CB: C (from 1,3) and B (from 4) \u2192 {S}</li> <li>Table update:<ul> <li>(0,3): {S}</li> <li>(1,4): {S}</li> </ul> </li> <li>Length 5 Substring (Whole String):</li> <li>(0,4): 'babab' \u2192 Check S \u2192 AB: A (from 0,3) and B (from 4) \u2192 {S}, or S \u2192 CB: C (from 0,3) and B (from 4) \u2192 {S}</li> <li>Table update:<ul> <li>(0,4): {S}</li> </ul> </li> </ol> <p>The top cell (0,4) contains {S}, indicating that the string \"babab\" can be derived from the start symbol S, confirming it is accepted by the grammar.</p>"},{"location":"lectures/05_Bottom-up/#cyk-pseudocode","title":"CYK Pseudocode","text":""},{"location":"lectures/05_Bottom-up/#cyk-complexity-analysis","title":"CYK Complexity Analysis","text":"<p>The time and space complexity of the CYK algorithm can be analyzed as follows:</p> <p>The space complexity of the CYK algorithm is O(n\u00b2), where n is the size of the input word. This is because the algorithm uses a n x n table to store the recognition table.</p> <p>The time complexity of the CYK algorithm is O(|G|n\u00b3), where |G| is the size of the grammar and n is the size of the input word. This is because the algorithm needs to iterate over the input string and the grammar rules, which results in a cubic time complexity.</p> <p>The complexity of the grammar |G| is defined as the sum of the lengths of the right-hand sides of all production rules in the grammar, denoted as:</p> <p>|G| = \u2211_{(A\u2192v)\u2208P} (1 + |v|)</p>"},{"location":"lectures/05_Bottom-up/#cyk-parsing-problems","title":"CYK Parsing: Problems","text":"<p>While the CYK algorithm is powerful, it does come with certain challenges. The high time and space complexity can make it less suitable for large inputs or complex grammars. Additionally, converting the grammar to Chomsky Normal Form (CNF) can sometimes make it difficult to retain the intended structure of the grammar.</p>"},{"location":"lectures/05_Bottom-up/#cyk-parsing-exercises","title":"CYK Parsing: Exercises","text":"<p>There are several exercises associated with the CYK algorithm. These exercises provide practical applications of the algorithm and help to deepen understanding of its workings.</p> <p>For instance, one exercise asks to show the CYK algorithm with a given grammar and input word. Another exercise asks to use the CYK method to determine if a specific string is in the language generated by a given grammar. There are also exercises on modifying the CYK algorithm to count the number of parse trees for a given input string and discussing the probabilistic version of the CYK method.</p>"},{"location":"lectures/05_Bottom-up/#cyk-parsing-exercise-solutions","title":"CYK Parsing: Exercise Solutions","text":"<ol> <li>For the given grammar and input word, the CYK algorithm can be shown by manually applying the algorithm's steps to the given input and grammar.</li> <li>To determine if the string <code>w = aaabbbb</code> is in the language generated by the grammar <code>S \u2192 aSb | b</code>, we can apply the CYK algorithm to this string and grammar.</li> <li>To modify the CYK algorithm to count the number of parse trees of a given input string, we can add a counter to the algorithm that increments whenever a new parse tree is formed.</li> <li>For the probabilistic version of the CYK method (P-CYK), weights (probabilities) are stored in the table instead of booleans, so P[i,j,A] will contain the minimum weight (maximum probability) that the substring from i to j can be derived from A. Further extensions of the algorithm allow all parses of a string to be enumerated from lowest to highest weight (highest to lowest probability).</li> </ol>"},{"location":"lectures/06_Translation-Methods/","title":"Lecture 6: Translation Methods","text":""},{"location":"lectures/06_Translation-Methods/#outline","title":"Outline","text":"<ol> <li>Syntax-directed translation and model-driven translation</li> <li>Syntax-Directed Definitions<ul> <li>Inherited and synthesized attributes</li> </ul> </li> <li>Evaluation Orders for SDD's<ul> <li>S-Attributed and L-Attributed SDDs</li> </ul> </li> <li>Applications of SDD</li> <li>Syntax-Directed Translation Schemes<ul> <li>Postfix SDT's</li> <li>Non-postfix SDT's</li> </ul> </li> <li>Model-Driven Translation<ul> <li>Visitor mechanism</li> <li>Listener mechanism</li> <li>Exercises</li> </ul> </li> <li>Summary of Lecture 6</li> </ol>"},{"location":"lectures/06_Translation-Methods/#toward-transforming-everything","title":"Toward transforming everything","text":""},{"location":"lectures/06_Translation-Methods/#1-syntax-directed-translation-and-model-drived-translation","title":"1. Syntax-directed translation and model-drived translation","text":""},{"location":"lectures/06_Translation-Methods/#overview","title":"Overview","text":"<p>We will explore two primary translation methods used in computer languages and compilers:</p> <ol> <li>Syntax-directed translation</li> <li> <p>Model-driven translation</p> <ul> <li> <p>A syntax-directed translator reads input and immediately emits output as it goes.</p> </li> <li> <p>A model-driven translator creates a parse tree or syntax tree input model and then walks it to generate output. Everything centers around an input model created by the parser.</p> </li> </ul> </li> </ol> <p></p>"},{"location":"lectures/06_Translation-Methods/#syntax-directed-translation","title":"Syntax-directed translation","text":"<ul> <li>Syntax-directed translation is more efficient than model-driven translation.</li> <li>It augments the parser code of the grammar with application domain codes in a messy way.</li> <li>Limitations:<ul> <li>It has no opportunity to analyze an input model, which is typically required for complex translations.</li> <li>Syntax-directed translators also break down when the order of input and output constructs differs significantly.</li> </ul> </li> </ul>"},{"location":"lectures/06_Translation-Methods/#model-driven-translation","title":"Model-driven translation","text":"<ul> <li>The only difference between this and a syntax-directed translator is that we generate text while walking a tree instead of while parsing a token stream.</li> <li>Model-driven translation is less efficient than syntax-directed translation.</li> <li>Advantages:<ul> <li>Sequestering specific actions from parser code in a separate tree walker lets us reuse the parser for other tasks.</li> <li>We can do more advanced analysis before generating output.</li> </ul> </li> </ul>"},{"location":"lectures/06_Translation-Methods/#translation-methods-taxonomy","title":"Translation Methods Taxonomy","text":""},{"location":"lectures/06_Translation-Methods/#2-syntax-directed-definitions-sdd","title":"2. Syntax-Directed Definitions (SDD)","text":"<p>A Syntax-Directed Definition (SDD) is a context-free grammar augmented with attributes and rules. Attributes are linked to grammar symbols, while rules are associated with productions. An SDD defines the values of attributes by linking semantic rules to grammar productions.</p> <p>Example: Infix-to-Postfix Translator</p> PRODUCTION SEMANTIC RULE E -&gt; E1 + T E.code = E1.code || T.code || '+' <p>In this example:</p> <ul> <li>Both E and T possess a string-valued attribute named code.</li> <li>The semantic rule dictates that the string E.code is created by concatenating E1.code, T.code, and the character +.</li> <li>Directly manipulating strings this way can be inefficient.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#syntax-directed-translation-schemes-sdt","title":"Syntax-Directed Translation Schemes (SDT)","text":"<p>An SDT embeds program fragments, known as semantic actions, within the bodies of productions.</p> <p>E -&gt; E1 + T {print('+')}</p> <p>Key points:</p> <ul> <li>Semantic actions are conventionally enclosed in curly braces.</li> <li>The placement of a semantic action within a production body dictates its execution order. In the example above, the action is executed at the end.</li> <li>Semantic actions can be positioned anywhere within a production body.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#comparing-sdd-and-sdt","title":"Comparing SDD and SDT","text":"<ul> <li>An SDT is similar to an SDD, but it explicitly defines the evaluation order of semantic rules.</li> <li>SDDs are often more readable and better suited for specifications.</li> <li>SDTs can be more efficient, making them preferable for implementations.</li> <li>In practice, SDDs may have limited side effects, such as printing results or symbol table interactions. An SDD with no side effects is also known as an attribute grammar.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#implementing-sdd-and-sdt","title":"Implementing SDD and SDT","text":"<ul> <li>Attributes of a symbol X can be implemented as data fields in the records representing the nodes for X.</li> <li>A general approach to syntax-directed translation involves:<ol> <li>Building a parse tree or a syntax tree.</li> <li>Computing attribute values by visiting the tree nodes in a left-to-right, depth-first order (pre-order traversal).</li> </ol> </li> <li>Translation can sometimes be performed during parsing without constructing an explicit tree, especially for L-attributed translations.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#inherited-and-synthesized-attributes","title":"Inherited and Synthesized Attributes","text":"<p>Nonterminals can have two kinds of attributes:</p> <ul> <li> <p>Synthesized Attribute: A synthesized attribute for a nonterminal A at a parse-tree node N is determined by a semantic rule linked to the production at N (which must have A as its head). It is defined solely based on the attribute values of its children and N itself.</p> </li> <li> <p>Inherited Attribute: An inherited attribute for a nonterminal B at a node N is defined by a semantic rule associated with the production at the parent of N (where B is a symbol in the production's body). It is defined using attribute values from N's parent, N itself, and N's siblings.</p> </li> </ul>"},{"location":"lectures/06_Translation-Methods/#sdd-with-synthesized-attributes-example","title":"SDD with Synthesized Attributes: Example","text":"<p>This example illustrates an SDD for a simple desk calculator where each nonterminal has a single synthesized attribute, val.</p> PRODUCTION SEMANTIC RULES 1) L -&gt; E n L.val = E.val 2) E -&gt; E1 + T E.val = E1.val + T.val 3) E -&gt; T E.val = T.val 4) T -&gt; T1 * F T.val = T1.val \u00d7 F.val 5) T -&gt; F T.val = F.val 6) F -&gt; (E) F.val = E.val 7) F -&gt; digit F.val = digit.lexval <p>In this example:</p> <ul> <li>Each nonterminal has a single synthesized attribute, val.</li> <li>The semantic rules define the values of the val attributes.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#annotated-parse-tree-with-synthesized-attribute","title":"Annotated parse tree with synthesized attribute","text":"<ul> <li>A parse tree, showing the value(s) of its attribe(s) is called an annotated parse tree.</li> <li>Annotated parse tree for 3*5+4n </li> </ul>"},{"location":"lectures/06_Translation-Methods/#sdd-with-inherited-attributes-example","title":"SDD with Inherited Attributes: Example","text":"<p>In this example, the nonterminal T' has both an inherited attribute (inh) and a synthesized attribute (syn). In the production T -&gt; * F T'1, the head T' inherits the left operand of *.</p> PRODUCTION SEMANTIC RULES 1) T -&gt; F T' T'.inh = F.val, T.val = T'.syn 2) T' -&gt; * F T'1 T'1.inh = T'.inh \u00d7 F.val, T'.syn = T'1.syn 3) T' -&gt; \u03f5 T'.syn = T'.inh 4) F -&gt; digit F.val = digit.lexval <p>In this example:</p> <ul> <li>The nonterminal T' has both an inherited attribute (inh) and a synthesized attribute (syn).</li> <li>The semantic rules define the values of the inh and syn attributes.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#annotated-parse-tree-with-inherited-attribute","title":"Annotated parse tree with inherited attribute","text":"<ul> <li>annotated parse tree for 3*5 in top-down parsing:</li> </ul>"},{"location":"lectures/06_Translation-Methods/#3-evaluation-orders-for-sdds","title":"3. Evaluation Orders for SDDs","text":""},{"location":"lectures/06_Translation-Methods/#sdd-dependency-graphs","title":"SDD Dependency Graphs","text":"<p>A dependency graph helps determine the computation order for attribute values. It illustrates the flow of information among attribute instances in a parse tree. An edge from one attribute instance to another signifies that the first attribute's value is required to compute the second.</p> <p>Algorithm for Constructing a Dependency Graph:</p> <ol> <li>For each parse-tree node labeled with a grammar symbol X, the dependency graph includes a node for each attribute of X.</li> <li>If a semantic rule for production p defines a synthesized attribute A.b based on X.c, an edge is drawn from X.c to A.b.</li> <li>If a semantic rule for production p defines an inherited attribute B.c based on X.a, an edge is drawn from X.a to B.c. The node M for X can be the parent or a sibling of the node N for B.</li> </ol>"},{"location":"lectures/06_Translation-Methods/#sdd-dependency-graphs-example","title":"SDD dependency graphs: Example","text":""},{"location":"lectures/06_Translation-Methods/#ordering-the-evaluation-of-attributes","title":"Ordering the Evaluation of Attributes","text":"<p>If a dependency graph contains an edge from node M to node N, the attribute for M must be evaluated before the attribute for N. The permissible evaluation orders are sequences of nodes <code>&lt;N1, N2, ..., Nk&gt;</code> where if an edge exists from Ni to Nj, then i &lt; j. Such an ordering is known as a topological sort of the graph.</p>"},{"location":"lectures/06_Translation-Methods/#ordering-the-evaluation-of-attributes-example","title":"Ordering the evaluation of attributes: Example","text":""},{"location":"lectures/06_Translation-Methods/#s-attributed-and-l-attributed-sdds","title":"S-Attributed and L-Attributed SDDs","text":""},{"location":"lectures/06_Translation-Methods/#s-attributed-definitions","title":"S-Attributed Definitions","text":"<ul> <li>An SDD that exclusively uses synthesized attributes is called S-attributed.</li> <li>The attributes in an S-attributed SDD can be evaluated in any bottom-up order of the parse tree nodes. A postorder traversal is a common method for this.</li> <li>S-attributed definitions can be implemented during bottom-up parsing, as this process corresponds to a postorder traversal.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#l-attributed-definitions","title":"L-Attributed Definitions","text":"<ul> <li>In L-attributed definitions, dependency-graph edges can only flow from left to right.</li> <li>This structure allows attributes to be evaluated in a single left-to-right pass, making them ideal for single-pass compilers.</li> <li>Each attribute must be either:<ul> <li>Synthesized.</li> <li>Inherited, but with a restriction: for a production A -&gt; X1X2...Xn, an inherited attribute Xi.a can only depend on:<ul> <li>Inherited attributes of the head A.</li> <li>Attributes of the symbols to its left (X1, ..., Xi-1).</li> <li>Attributes of Xi itself, without creating cycles in the dependency graph.</li> </ul> </li> </ul> </li> </ul>"},{"location":"lectures/06_Translation-Methods/#l-attributed-definitions-example","title":"L-attributed definitions: Example","text":""},{"location":"lectures/06_Translation-Methods/#4-applications-of-sdd","title":"4. Applications of SDD","text":""},{"location":"lectures/06_Translation-Methods/#construction-of-syntax-trees-for-bottom-up-parsing","title":"Construction of Syntax Trees for Bottom-Up Parsing","text":"<p>An S-attributed definition can be used to construct syntax trees for a simple expression grammar.</p> PRODUCTION SEMANTIC RULES 1) E -&gt; E1 + T E.node = new Node('+', E1.node, T.node) 2) E -&gt; E1 - T E.node = new Node('-', E1.node, T.node) 3) E -&gt; T E.node = T.node 4) T -&gt; (E) T.node = E.node 5) T -&gt; id T.node = new Leaf(id, id.entry) 6) T -&gt; num T.node = new Leaf(num, num.val) <p>For the input a - 4 + c, the syntax tree construction steps during a bottom-up parse would be:</p> <ol> <li>p1 = new Leaf(id, 'a');</li> <li>p2 = new Leaf(num, 4);</li> <li>p3 = new Node('-', p1, p2);</li> <li>p4 = new Leaf(id, 'c');</li> <li>p5 = new Node('+', p3, p4);</li> </ol> <p></p>"},{"location":"lectures/06_Translation-Methods/#construction-of-syntax-trees-for-top-down-parsing","title":"Construction of Syntax Trees for Top-Down Parsing","text":"<p>An L-attributed definition can achieve the same syntax tree translation as the S-attributed one.</p> PRODUCTION SEMANTIC RULES 1) E -&gt; T E' E.node = E'.syn, E'.inh = T.node 2) E' -&gt; + T E'1 E'1.inh = new Node('+', E'.inh, T.node), E'.syn = E'1.syn 3) E' -&gt; - T E'1 E'1.inh = new Node('-', E'.inh, T.node), E'.syn = E'1.syn 4) E' -&gt; \u03f5 E'.syn = E'.inh 5) T -&gt; (E) T.node = E.node 6) T -&gt; id T.node = new Leaf(id, id.entry) 7) T -&gt; num T.node = new Leaf(num, num.val)"},{"location":"lectures/06_Translation-Methods/#5-syntax-directed-translation-schemes-sdt","title":"5. Syntax-Directed Translation Schemes (SDT)","text":"<ul> <li>Syntax-directed translation schemes (SDTs) are a  complementary notation to syntax-directed definitions (SDDs).</li> <li> <p>Weuse SDT\u2019s to implement two important classes of SDD\u2019s  efficiently (during parsing):</p> <ol> <li>The underlying grammar is LR-parsable, and the SDD is S-attributed.</li> <li>The underlying grammar is LL-parsable, and the SDD is L-attributed.</li> </ol> </li> <li> <p>In SDT, an action may be placed at any position within the  body of a production.</p> </li> </ul> <p>Definition:  Postfix translation schemes: SDT\u2019s with all actions at the right  ends of the production bodies are called postfix SDT\u2019s. </p>"},{"location":"lectures/06_Translation-Methods/#implementing-sdts","title":"Implementing SDTs","text":"<p>Any SDT can be implemented as follows:</p> <ol> <li>Ignoring the actions, parse the input and produce a parse  tree as a result.</li> <li>Examine each interior node N, say one for production A \u2192 \u03b1.  Add additional children to N for the actions in \u03b1, so the  children of N from left to right have exactly the symbols and  actions of \u03b1.</li> <li>Perform a preorder traversal of the tree, and as soon as a  node labeled by an action is visited, perform or execute that  action.</li> </ol>"},{"location":"lectures/06_Translation-Methods/#postfix-sdts","title":"Postfix SDT's","text":"<ul> <li>Postfix SDT\u2019s can be implemented during LR parsing by  executing the actions when reductions occur.</li> <li>To do this, the attribute(s) of each grammar symbol must  be put on the stack in a place where they can be found during  the reduction.</li> <li>The best plan is to place the attributes along with the  grammar symbols (or the LR states that represent these  symbols) in records on the stack itself.</li> </ul> <p>Example:</p> <p></p> <p></p>"},{"location":"lectures/06_Translation-Methods/#non-postfix-sdts","title":"Non-postfix SDT's","text":"<p>SDT's with actions inside productions</p> <ul> <li>We converted S-attributed SDD\u2019s into postfix SDT\u2019s, with  actions at the right ends of productions.</li> <li>As long as the underlying grammar is LR, postfix SDT\u2019s can be  parsed and translated bottom-up.</li> <li>We now consider SDT\u2019s for the more general case of an  L-attributed SDD assuming that the underlying grammar can  be parsed top-down.</li> <li>With any grammar, the technique below can be implemented  by attaching actions to a parse tree and executing them during  preorder traversal of the tree.</li> </ul> <p>SDT's for L-attributed definitions</p> <ul> <li> <p>The rules for turning an L-attributed SDD into an SDT are as  follows:</p> <ol> <li>Embed the action that computes the inherited attributes for  a nonterminal A immediately before that occurrence of A in  the body of the production. If several inherited attributes for A  depend on one another in an acyclic fashion, order the  evaluation of attributes so that those needed first are  computed first.</li> <li>Place the actions that compute a synthesized attribute for  the head of a production at the end of the body of that  production.</li> <li>Treat actions as dummy nonterminals, and then variables can  be treated as the synthesized attributes of dummy  nonterminals.</li> </ol> </li> </ul> <p>Implementationof SDT\u2019s for L-attributed SDD\u2019s</p> <ul> <li>During top-down parsing, semantic actions for inherited  attributes can be executed while descending into the parse  tree.</li> <li>Synthesized attributes are calculated while returning up the  tree, ensuring that the semantic analysis does not require  revisiting nodes.</li> <li> <p>We discuss the following methods for translation L-attributed  SDD\u2019s during (recursive-descent) parsing:</p> <ol> <li>Use a recursive-descent parser with one function for each  nonterminal. The function for nonterminal A receives the  inherited attributes of A as arguments and returns the  synthesized attributes of A.</li> <li>Generate code-on-the-fly, using a recursive-descent parser.</li> </ol> </li> </ul> <p>L-Attributed SDD's and LL Parsing</p> <ul> <li>Suppose that an L-attributed SDD is based on an LL-grammar  and that we have converted it to an SDT with actions  embedded in the productions.</li> <li>Wecan perform the translation during LL parsing by extending  the parser stack to hold actions and certain data items needed  for attribute evaluation.</li> <li> <p>To implement an SDT in conjunction with an LL-parser  the attributes are kept on the parsing stack, and the rules  fetch the needed attributes from known locations on the stack.</p> </li> <li> <p>We use the following two principles to manage attributes on  the stack:</p> <ol> <li>The inherited attributes of a nonterminal A are placed in the  stack record that represents that nonterminal.</li> <li>The code to evaluate these attributes will usually be represented  by an action-record immediately above the stack record for A.</li> <li>in fact, the conversion of L-attributed SDD\u2019s to SDT\u2019s ensures  that the action-record will be immediately above A.</li> <li>The synthesized attributes for a nonterminal A are placed in a  separate synthesize-record that is immediately below the  record for A on the stack.</li> </ol> </li> <li> <p>Action-records contain pointers to code to be executed.  Actions may also appear in synthesize-records.</p> </li> <li>(Refer to Aho et al., 2006, Section 5.5.3)</li> </ul> <p>L-Attributed SDD's and LR Parsing</p> <ul> <li>So far, we saw that every S-attributed SDD on an LR  grammar can be converted to a postfix SDT and then  implemented during a bottom-up parse.</li> <li>Also, L-attributed SDD on an LL grammar can be parsed  top-down.</li> <li>We also know that LL grammars are a proper subset of the  LR grammars, and the S-attributed SDD\u2019s are a proper subset  of the L-attributed SDD\u2019s.</li> </ul> <p>So, the question is: Can we handle L-attributed SDD\u2019s on LR grammars?</p> <ul> <li> <p>The answer is: No! We cannot (See Aho et al., 2006, P. 348).</p> <ul> <li>Remember: We still can build the parse tree first and then perform the translation, i.e., do model-driven translation.</li> </ul> </li> <li> <p>We can do bottom-up every translation that we can do  top-down.</p> </li> <li>More precisely, given an L-attributed SDD on an LL grammar,  we can adapt the grammar to compute the same SDD on  the new grammar during an LR parse.</li> <li>Introduce into the grammar a marker nonterminal in place of  each embedded action (Refer to Aho et al., 2006, Section  5.5.4).</li> </ul> <p>Remarks</p> <ul> <li>Markers are nonterminals that derive only \u03f8 and that appear  only once among all the bodies of all productions.</li> <li>L-attributed grammars are well-suited for top-down parsing as  they allow semantic information to be propagated and  evaluated in a manner consistent with the left-to-right order of  parsing.</li> <li>This makes them a natural fit for implementing  syntax-directed translation in top-down parsers, particularly in  the context of single-pass compilers.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#6-model-driven-translation","title":"6. Model-Driven Translation","text":"<p>Model-driven translation utilizes parse-tree visitors and other tree walkers for language applications like type checking and code generation. We will look at the visitor and listener mechanisms provided by ANTLR.</p> <p>The primary difference between these mechanisms is that listener methods are called automatically by an ANTLR-provided walker, while visitor methods require explicit calls to traverse their children. If you forget to invoke visitor methods on a node's children, those subtrees will not be visited.</p>"},{"location":"lectures/06_Translation-Methods/#parse-tree-listeners","title":"Parse-Tree Listeners","text":"<ul> <li>ANTLR's runtime provides the ParseTreeWalker class to walk a tree and trigger calls to a listener. It performs a depth-first walk.</li> <li>ANTLR generates a ParseTreeListener subclass for each grammar, with enter and exit methods for each rule.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#parse-tree-visitors","title":"Parse-Tree Visitors","text":"<ul> <li>Visitors are useful when you need to control the tree walk yourself by explicitly calling methods to visit children.</li> <li>To start a walk, you create a visitor implementation and call its visit() method.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#building-a-translator-with-a-listener","title":"Building a Translator with a Listener","text":"<p>As an example, we can build a tool to generate a Java interface file from a Java class definition. This involves listening for \"events\" from a Java parse-tree walker. The core interface is JavaListener, which ANTLR generates automatically.</p>"},{"location":"lectures/06_Translation-Methods/#summary","title":"Summary","text":"<ul> <li>Syntax-Directed Translation: Computes values or performs actions on-the-fly during parsing without creating an explicit parse tree.</li> <li>Model-Driven Translation: Computes values or performs actions after building a parse tree by walking it.</li> <li>Inherited and Synthesized Attributes: SDDs use two attribute types. Synthesized attributes are computed from children's attributes, while inherited attributes are computed from the parent's and/or siblings' attributes.</li> <li>S-Attributed Definitions: Contain only synthesized attributes.</li> <li>L-Attributed Definitions: Can have both inherited and synthesized attributes, with restrictions on inherited attributes to ensure a left-to-right evaluation order.</li> <li>Dependency Graphs: Show the flow of information between attribute instances and determine the evaluation order. Cyclic dependency graphs indicate problematic SDDs where attribute evaluation is not possible.</li> <li>Implementing S-Attributed SDD's: Can be done with a postfix SDT on an LR parser stack.</li> <li>Implementing L-Attributed SDD's: Can be handled by a recursive-descent parser for top-down parsable grammars.</li> <li>Listener and Visitor Mechanisms: ANTLR offers two model-driven translation approaches. Listeners are event-driven, while visitors give you explicit control over the tree traversal.</li> </ul>"},{"location":"lectures/06_Translation-Methods/#reading-assignments","title":"Reading Assignments","text":"<ol> <li>Read and practice Chapter 5 of the Dragon book (Aho et al., 2006).</li> <li>Read and practice Chapter 5 of the Engineering a Compiler book (Cooper and Torczon, 2022).</li> <li>Read and practice Chapter 4 of The Definitive ANTLR 4 Reference book (Parr, 2013).</li> <li>Read and practice Appendix 1 of Lecture 6.</li> </ol>"},{"location":"lectures/06_Translation-Methods/#references","title":"References","text":"<ul> <li>Aho,Alfred V., Monica S. Lam, Ravi Sethi and Jeffrey D. Ullman  (2006). Compilers: Principles, Techniques, and Tools (2nd  Edition). USA: Addison-Wesley Longman Publishing Co., Inc. ISBN:</li> <li> <p>URL: http://infolab.stanford.edu/  %5C~ullman/dragon/errata.html.</p> </li> <li> <p>Cooper, K.D. and L. Torczon (2022). Engineering a Compiler.  Elsevier Science, Morgan Kaufmann Publishers Inc. ISBN:</p> </li> <li> <p>URL:  https://books.google.com/books?id=WxTozgEACAAJ.</p> </li> <li> <p>Cormen, T. H., C. E. Leiserson, R. L. Rivest and C. Stein (2022).  Introduction to algorithms. 4th edition. MIT Press. ISBN:</p> </li> <li> <p>URL:  https://mitpress.mit.edu/books/introduction algorithms-fourth-edition.</p> </li> <li> <p>Parr, T. (2009). Language Implementation Patterns: Create  Your Own Domain-Specific and General Programming  Languages. Pragmatic Bookshelf. ISBN: 9781680503746. URL:  https://books.google.com/books?id=Ag9QDwAAQBAJ.</p> </li> <li> <p>\u2014(2013). The Definitive ANTLR 4 Reference. Pragmatic  Bookshelf. ISBN: 9781680505009. URL:  https://books.google.com/books?id=gA9QDwAAQBAJ.</p> </li> </ul>"},{"location":"lectures/07_Semantic-Analysis/","title":"Lecture 7: Semantic Analysis (Types and Symbol Tables)","text":""},{"location":"lectures/07_Semantic-Analysis/#outline","title":"Outline","text":"<ol> <li>Type Checking Basics<ul> <li>Type systems</li> <li>Type expressions</li> <li>Type system implementation</li> <li>Scope in type checking</li> </ul> </li> <li>Symbol Tables<ul> <li>Implementing symbol tables</li> <li>Use of symbol tables</li> </ul> </li> <li>Types and Declarations</li> <li>Type Checking, Conversion, and Inference SDT's<ul> <li>Type checking</li> <li>Type conversion</li> <li>Type inference</li> </ul> </li> <li>Symbol Tables for Program Analysis and Transformation</li> <li>Summary of Lecture 7</li> </ol>"},{"location":"lectures/07_Semantic-Analysis/#type-checking-in-compilers","title":"Type checking in compilers","text":""},{"location":"lectures/07_Semantic-Analysis/#non-context-free-syntax","title":"Non-context-free syntax","text":"<ul> <li>Programs that are correct with respect to the language's lexical and context-free syntactic rules may still contain other syntactic errors.</li> <li>Lexical analysis and context-free syntax analysis are not powerful enough to ensure the correct usage of variables, objects, functions, statements, etc.</li> <li>Non-context-free syntactic analysis is known as semantic analysis.</li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#semantic-analysis","title":"Semantic analysis","text":"<ul> <li>The semantic analyzer uses the syntax tree and the information in the symbol table to check the source program for semantic consistency with the language definition.</li> <li>Symbol tables are data structures that are used by compilers to hold information about source-program constructs.</li> <li>The information is collected incrementally by the analysis phases of a compiler and used by the synthesis phases to generate the target code.</li> <li>An important part of semantic analysis is type checking, where the compiler checks that each operator has matching operands.</li> <li>We shall study various types of symbol tables and SDT's for type checking in this lecture.</li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#incorrect-programs","title":"Incorrect programs","text":"<ul> <li>Example 1: lexical analysis does not distinguish between different variable or function identifiers (it returns the same token for all identifiers):</li> </ul> Correct Incorrect <code>int a; a=1;</code> <code>int a; b=1;</code> <ul> <li>Example 2: syntax analysis does not correlate the declarations with the uses of variables in the program:</li> </ul> Correct Incorrect <code>int a; a=1;</code> <code>a=1;</code> <ul> <li>Example 3: syntax analysis does not correlate the types from the declarations with the uses of variables:</li> </ul> Correct Incorrect <code>int a; a=1;</code> <code>int a; a=1.0;</code>"},{"location":"lectures/07_Semantic-Analysis/#goals-of-semantic-analysis","title":"Goals of semantic analysis","text":"<ul> <li>Semantic analysis ensures that the program satisfies a set of additional rules regarding the usage of programming constructs (variables, objects, expressions, statements).</li> <li>Examples of semantic rules:<ul> <li>Variables must be declared before being used.</li> <li>A variable should not be declared multiple times in the same scope.</li> <li>In an assignment statement, the variable and the assigned expression must have the same type.</li> <li>The condition of an <code>if-statement</code> must have type Boolean.</li> </ul> </li> <li>Categories of rules:<ul> <li>Semantic rules regarding types.</li> <li>Semantic rules regarding scopes.</li> </ul> </li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#1-type-checking-basics","title":"1. Type Checking Basics","text":""},{"location":"lectures/07_Semantic-Analysis/#type-systems","title":"Type systems","text":"<p>What are types?</p> <ul> <li>Types describe the values possibly computed during execution of the program.</li> <li>Types are predicates on values.<ul> <li>e.g., \"int x\" in Java means x \u2208 [-2^31, 2^31-1].</li> </ul> </li> <li>Think \"type = set of possible values\".</li> <li>Type errors: improper, type-inconsistent operations during program execution.</li> <li>Type-safety: absence of type errors at run time.</li> </ul> <p>Type information</p> <p>Type information classifies a program's constructs (e.g., variables, statements, expressions, functions) into categories, and imposes rules on their use (in terms of those categories) with the goal of avoiding runtime errors.</p> Construct Example Type variables <code>int a;</code> integer location expressions <code>(a+1)==2</code> Boolean statements <code>a=1.0;</code> void functions <code>int pow(int n, int m)</code> int \u00d7 int \u2192 int <p>How to ensure type-safety?</p> <ul> <li>Bind (assign) types, then check types:<ul> <li>Type binding: defines types for constructs in the program (e.g., variables, functions).<ul> <li>Can be either explicit (<code>int x</code>) or implicit (<code>x=1</code>).</li> <li>Type consistency (safety) = correctness with respect to the type bindings.</li> </ul> </li> <li>Type checking: static semantic checks to enforce the type safety of the program.<ul> <li>Enforce a set of type-checking rules.</li> </ul> </li> </ul> </li> </ul> <p>Static vs. dynamic typing</p> <ul> <li>Static and dynamic typing refer to type definitions (i.e., bindings of types to variables, expressions, etc.).</li> <li>Statically typed language: types are defined and checked at compile-time, and do not change during the execution of the program.<ul> <li>E.g., Pascal, C, Java.</li> </ul> </li> <li>Dynamically typed language: types defined and checked at run-time, during program execution.<ul> <li>E.g., Lisp, Scheme, Smalltalk, Python.</li> </ul> </li> </ul> <p>Strong vs. weak typing</p> <ul> <li>Strong and weak typing refer to how much type consistency is enforced.<ul> <li>Strongly typed languages: guarantees that accepted programs are type-safe: Java.</li> <li>Weakly typed languages: allow programs that contain type errors: C.</li> </ul> </li> <li>Can achieve strong typing using either static or dynamic typing.</li> </ul> <p>Why static checking?</p> <ul> <li>Efficient code: Dynamic checks slow down the program execution.</li> <li>Guarantees that all executions will be safe: With dynamic checking, you never know when the next execution of the program will fail due to a type error.</li> <li>But is conservative for sound systems; needs to be expressive: Reject few type-safe programs.</li> </ul> <p>Type systems concept summary</p> <ul> <li>Static vs. dynamic typing: when to define/check types?</li> <li>Strong vs. weak typing: how many type errors?</li> <li>Sound type systems: statically catch all type errors (and possibly reject some programs that have no type errors).</li> </ul> Strong Typing Weak Typing Static Typing ML, Pascal, Java, Modula-3 C, C++ Dynamic Typing Scheme, Python, Smalltalk, PostScript assembly code"},{"location":"lectures/07_Semantic-Analysis/#type-expressions","title":"Type expressions","text":"<p>Recap: Type system</p> <ul> <li>Type is a predicate on value.</li> <li>Type expressions: describe the possible types in the program: e.g., int, string, array[], class, etc.</li> <li>Type system: defines types for language constructs (e.g., expressions, statements, ...).</li> </ul> <p>Type expression</p> <ul> <li>Languages have basic types (a.k.a. primitive types or ground types).<ul> <li>E.g., int, char, boolean.</li> </ul> </li> <li>Build type expressions using basic types with:<ul> <li>Type constructors</li> <li>Type aliases</li> </ul> </li> </ul> <p>Array type</p> <ul> <li>Various kinds of array types in different programming languages.</li> <li><code>array(T)</code>: array with elements of type T and no bounds.<ul> <li>C, Java: <code>int[]</code>, Modula-3: <code>array of integer</code>.</li> </ul> </li> <li><code>array(T, S)</code>: array with size.<ul> <li>C: <code>int[10]</code>, Modula-3: <code>array[10] of integer</code>.</li> <li>May be indexed 0..size-1.</li> </ul> </li> <li><code>array(T, L, U)</code>: array with upper/lower bounds.<ul> <li>Pascal or Ada: <code>array[2..5] of integer</code>.</li> </ul> </li> <li><code>array(T, S1, ..., Sn)</code>: multi-dimensional arrays.<ul> <li>FORTRAN: <code>real(3,5)</code>.</li> </ul> </li> </ul> <p>Record type</p> <ul> <li>A record is <code>{id1: T1, ..., idn: Tn}</code> for some identifiers <code>idi</code> and types <code>Ti</code>.</li> <li>Supports access operations on each field, with corresponding type.</li> <li>C: <code>struct { int a; float b; }</code></li> <li>Pascal: <code>record a: integer; b: real; end</code></li> <li>Objects: generalize the notion of records.</li> </ul> <p>Pointers</p> <ul> <li>Pointer types characterize values that are addresses of variables of other types.</li> <li><code>Pointer(T)</code>: pointer to an object of type T.</li> <li>C pointers: <code>T*</code> (e.g., <code>int *x;</code>).</li> <li>Pascal pointers: <code>^T</code> (e.g., <code>x: ^integer;</code>).</li> <li>Java: Object references.</li> </ul> <p>Function Types</p> <ul> <li>A function value can be invoked with some argument expressions with types <code>Ti</code>, returns return type <code>Tr</code>.</li> <li>Type: <code>T1 \u00d7 T2 \u00d7 ... \u00d7 Tn \u2192 Tr</code>.</li> <li>C/C++ functions: <code>int pow(int x, int y)</code> has type <code>int \u00d7 int \u2192 int</code>.</li> <li>Java: methods have function types.</li> <li>Some languages have first-class functions (usually in functional languages, e.g., ML, LISP).</li> <li>C and C++ have function pointers.</li> <li>Java does not.</li> </ul> <p>Type alias</p> <ul> <li>Some languages allow type aliases (type definitions, equates):<ul> <li>C: <code>typedef int int_array[];</code></li> <li>Modula-3: <code>type int_array = array of int;</code></li> </ul> </li> <li>Java does not allow type aliases.</li> <li>Aliases are not type constructors! <code>int_array</code> is the same type as <code>int[]</code>.</li> <li>Different type expressions may denote the same type.</li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#type-system-implementation","title":"Type system implementation","text":"<p>Implementation approaches</p> <ol> <li>Use a separate class hierarchy for type ASTs:     <pre><code>class BaseType extends Type\n...\nclass IntType extends BaseType\n...\nclass BoolType extends BaseType\nclass ArrayType extends Type {\n  Type elemType;\n}\nclass FunctionType extends Type ...\n</code></pre></li> <li>Translate type expressions to type objects during parsing:     <pre><code>// non-terminal Type type\ntype ::= BOOLEAN { RESULT=new BoolType(); }\n       | ARRAY LBRACKET type:t RBRACKET { RESULT=new ArrayType(t); }\n       ...\n</code></pre></li> <li>Bind names to type objects in the symbol table during subsequent AST traversal.</li> </ol>"},{"location":"lectures/07_Semantic-Analysis/#scope-in-type-checking","title":"Scope in type checking","text":"<p>Processing type declaration</p> <ul> <li>Type declarations add new identifiers and their types in the symbol tables.</li> <li>Class definitions must be added to symbol table:<ul> <li><code>class_defn ::= CLASS ID:id { decls:d }</code></li> </ul> </li> <li>Forward references require multiple passes over AST to collect legal names.<ul> <li>Example: <code>class A { B b; } class B { ... }</code></li> </ul> </li> <li>Need to handle scope information.</li> </ul> <p>Scope information</p> <ul> <li>Scope information characterizes the declaration of identifiers and the portions of the program where use of each identifier is allowed.</li> <li>Example identifiers: variables, functions, objects, labels.</li> <li>Lexical scope is a textual region in the program:<ol> <li>Statement block</li> <li>Formal argument list</li> <li>Object body</li> <li>Function or method body</li> <li>Module body</li> <li>Whole program (multiple modules)</li> </ol> </li> <li>Scope of an identifier: the lexical scope in which it is valid.</li> </ul> <p>Scope examples in C:</p> <ul> <li>Scope of variables in statement blocks:</li> </ul> <p></p> <p>Other scope examples:</p> <ul> <li>Function arguments: <code>int factorial(int n) { /* scope of n is the function body */ }</code></li> <li>Labels: <code>void f() { goto I; ... I: a=1; /* scope of I is the function body */ }</code></li> <li>Object fields and methods:     <pre><code>class A {\n  private int x; // scope of x is class A\n  public void g() { x=1; } // scope of g is class A and its subclasses\n}\nclass B extends A {\n  public int h() { g(); }\n}\n</code></pre></li> </ul> <p>Semantic rules for scopes</p> <ul> <li> <p>Main rules regarding scopes:</p> <ul> <li>Rule 1: Use an identifier only if it is defined in an enclosing scope.</li> <li>Rule 2: Do not declare identifiers of the same kind with identical names more than once in the same scope.</li> </ul> </li> <li> <p>You can declare identifiers with the same name if they are of a different kind</p> </li> </ul> <p></p>"},{"location":"lectures/07_Semantic-Analysis/#2-symbol-tables","title":"2. Symbol Tables","text":"<ul> <li>Semantic checks refer to properties of identifiers in the program - their scope or type.</li> <li>Need an environment to store the information about identifiers - Symbol Table (or Symtab).</li> <li>Each entry in the symbol table contains:<ul> <li>The name of an identifier.</li> <li>Additional information: its kind, its type, its size, if it is constant, ...</li> </ul> </li> </ul> <p>Example Symbol Table:</p> NAME KIND TYPE OTHER foo fun int x int \u2192 bool extern m par int auto n par int const tmp var bool const <p>Handling scope information in symbol table</p> <ul> <li>How to represent scope information in the symbol table?<ol> <li>There is a hierarchy of scopes in the program.</li> <li>Use a similar hierarchy of symbol tables.</li> <li>One symbol table for each scope.</li> <li>Each symbol table contains the symbols declared in that lexical scope.</li> </ol> </li> </ul> <p>Example</p> <p></p> <p>Handling identifiers with the same name</p> <ul> <li>The hierarchical structure of symbol tables automatically solves the problem of resolving name collisions.</li> <li>To find the declaration of an identifier that is active at a program point:<ol> <li>Start from the current scope.</li> <li>Go up in the hierarchy until you find an identifier with the same name, or fail.</li> </ol> </li> </ul> <p>Example:</p> <p></p> <p>Catching semantic errors</p> <p></p> <p>Symbol table operations</p> <ul> <li>Three main operations:<ol> <li>Create a new empty symbol table with a given parent table.</li> <li>Insert a new identifier in a symbol table (or error on re-declaration).</li> <li>Look-up an identifier in a symbol table (or error if not found).</li> </ol> </li> <li>Cannot build symbol tables during lexical analysis because the hierarchy of scopes is encoded in the syntax.</li> <li>Build the symbol tables either:<ul> <li>While parsing, using semantic actions.</li> <li>After the AST is constructed.</li> </ul> </li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#implementing-symbol-tables","title":"Implementing symbol tables","text":"<p>Implementation methods</p> <ul> <li>Array implementation</li> <li>List implementation</li> <li>Hash-table implementation</li> </ul> <ol> <li> <p>Array implementation:</p> <ul> <li> <p>Simple implementation = array</p> <ul> <li>one entry per symbol.</li> <li>Scan the array for loopup, compare name at each entry.</li> </ul> </li> <li> <p>Disadvantage: </p> <ul> <li>Table has fixed size.</li> <li>Need to know in advance the number of entries</li> </ul> </li> </ul> </li> <li> <p>List implementation:</p> <ul> <li>Dynamic structure = list<ul> <li>One cell per entry in the table.</li> <li>Can grow dynamically during compilation.</li> </ul> </li> <li>Disadvantage:<ul> <li>Inefficient for large Symbol tables</li> <li>Need to scan half the list on average</li> </ul> </li> </ul> </li> <li> <p>Hash-table implementation:</p> <ul> <li>Efficient implementation = hash table<ul> <li>It is an array of lists (buckets)</li> <li>Uses a hashing function to map the symbol name to the corresponding bucket: <code>hashfunc : string \u2192 int</code>.</li> </ul> </li> <li>Good hash function = even distribution in the buckets.</li> <li>We shall implement scopes by setting up a separate symbol table for each scope.</li> </ul> </li> </ol> <p>Symbol table per scope (Chained symbol tables)</p> <p></p> <ul> <li>A chain of symbol tables can represent nested scopes. A table for an inner scope holds a reference to the table for its parent (outer) scope.</li> <li>Lookup proceeds from the current scope's table up through the chain of parent tables.</li> </ul> <p>Java implementation for Chained Symbol Tables:</p> <pre><code>// package symbols;\nimport java.util.*;\npublic class Env {\n    private Hashtable table;\n    protected Env prev;\n\n    public Env (Env p) {\n        table = new Hashtable();\n        prev = p;\n    }\n\n    public void put (String s, Symbol sym) {\n        table.put(s, sym);\n    }\n\n    public Symbol get(String s) {\n        for(Env e = this; e != null; e = e.prev ) {\n            Symbol found = (Symbol)(e.table.get(s));\n            if(found != null) return found;\n        }\n        return null;\n    }\n}\n</code></pre> <p>Forward references</p> <ul> <li>A forward reference is the use of an identifier within the scope of its declaration, but before the declaration itself appears in the text.</li> <li>Any compiler phase that uses information from the symbol table must be performed after the table is fully constructed.</li> <li>This means you cannot type-check and build the symbol table at the same time if forward references are allowed.</li> <li>Example requiring 2 passes:     <pre><code>class A {\n  int f1() { return f2(); } // f2 used before it's declared\n  int f2() { return 1; }\n}\n</code></pre></li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#use-of-symbol-tables","title":"Use of symbol tables","text":"<p>SDT for translating with symbol tables An SDT can manage the creation and destruction of symbol tables for nested blocks.</p> <ul> <li>When entering a block (<code>{</code>), create a new <code>Env</code> and link it to the previous one.</li> <li>When exiting a block (<code>}</code>), restore the previous <code>Env</code>.</li> <li>For declarations (<code>type id;</code>), add the symbol to the current <code>top</code> environment.</li> <li>For uses (<code>id</code>), look up the symbol starting from the <code>top</code> environment.</li> </ul> <p>Example Input:</p> <pre><code>{ int x; char y;\n  { bool y; x; y; }\n  x; y; }\n</code></pre> <p>Example Output (with types annotated):</p> <pre><code>{ { x:int; y:bool; } x:int; y:char; }\n</code></pre> <p>In a recursive-descent parser, the <code>saved</code> environment would be a local variable in the procedure for <code>block</code>.</p>"},{"location":"lectures/07_Semantic-Analysis/#3-types-and-declarations","title":"3. Types and Declarations","text":"<p>An SDT can process type declarations and compute type information (like type expressions and memory width) to store in the symbol table.</p> <p>Grammar:</p> <ul> <li>D \u2192 T id; D | \u03b5</li> <li>T \u2192 B C | record { D }</li> <li>B \u2192 int | float</li> <li>C \u2192 \u03b5 | [num] C</li> </ul> <p>SDT with Semantic Rules: Attributes <code>type</code> and <code>width</code> are synthesized up the parse tree.</p> <ul> <li>For <code>B</code>: Set base type and width (<code>int</code> is 4, <code>float</code> is 8).</li> <li>For <code>C</code>: An inherited attribute passes the base type down, and synthesized attributes build up array types. <code>C.type = array(num.value, C1.type)</code> and <code>C.width = num.value * C1.width</code>.</li> <li>For <code>T</code>: Combines the base type and array information.</li> <li>For <code>D</code>: Adds the final type and computed offset to the symbol table for each identifier.</li> <li>For <code>P</code>: A marker nonterminal <code>M</code> can be used to initialize a global <code>offset</code> variable, allowing the SDT to be written in postfix form. P \u2192 M D where M \u2192 \u03b5 {offset=0;}.</li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#4-type-checking-conversion-and-inference-sdts","title":"4. Type Checking, Conversion, and Inference SDT's","text":""},{"location":"lectures/07_Semantic-Analysis/#type-checking","title":"Type checking","text":"<p>Type synthesis and type inference</p> <ul> <li>To do type checking, a compiler needs to assign a type expression to each component of the source program.</li> <li>Type checking can take on two forms: synthesis and inference.<ul> <li>Type synthesis builds up the type of an expression from the types of its subexpressions.</li> <li>Type inference determines the type of a language construct from the way it is used. It is needed for languages like ML, which check types but do not require names to be declared.</li> </ul> </li> </ul> <p>Type checking rules</p> <ul> <li>Type checking is the validation of a set of type rules. Examples:<ul> <li>The type of a variable must match its declaration.</li> <li>Operands of <code>+</code>, <code>*</code>, <code>-</code>, <code>/</code> must be integers; the result is an integer.</li> <li>Operands of <code>==</code>, <code>!=</code> must be integer or string; the result is boolean.</li> <li>In an assignment, the variable and expression types must match.</li> <li>In a function call, actual argument types must match formal parameter types.</li> <li>The return value type must match the function's declared return type.</li> </ul> </li> </ul> <p>Semantic rules for type checking (Expressions):</p> <ul> <li><code>E \u2192 literal</code> \u21d2 <code>E.type := 'char'</code></li> <li><code>E \u2192 num</code> \u21d2 <code>E.type := 'int'</code></li> <li><code>E \u2192 id</code> \u21d2 <code>E.type := lookup(id.entry)</code></li> <li><code>E \u2192 E1 mod E2</code> \u21d2 <code>E.type := if E1.type == 'int' and E2.type == 'int' then 'int' else type_error</code></li> <li><code>E \u2192 E1[E2]</code> \u21d2 <code>E.type := if E2.type == 'int' and E1.type == 'array(s,t)' then t else type_error</code></li> </ul> <p>Semantic rules for type checking (Statements):</p> <ul> <li><code>S \u2192 id := E</code> \u21d2 <code>S.type := if id.type == E.type then void else type_error</code></li> <li><code>S \u2192 if E then S1</code> \u21d2 <code>S.type := if E.type == 'boolean' and S1.type == void then void else type_error</code></li> <li><code>S \u2192 while E do S1</code> \u21d2 <code>S.type := if E.type == 'boolean' and S1.type == void then void else type_error</code></li> </ul> <p>Type checking implementation</p> <ul> <li>Type checking can be implemented by an AST visitor.</li> </ul> <pre><code>class TypeCheck implements Visitor {\n    Object visit(Add e, Object symbolTable) {\n        Type t1 = (Type) e.e1.accept(this, symbolTable);\n        Type t2 = (Type) e.e2.accept(this, symbolTable);\n        if (t1 == Int &amp;&amp; t2 == Int) return Int;\n        else throw new TypeCheckError(\"+\");\n    }\n    Object visit(Num e, Object symbolTable) {\n        return Int;\n    }\n    Object visit(Id e, Object symbolTable) {\n        return ((SymbolTable)symbolTable).lookupType(e);\n    }\n}\n</code></pre> <p>Types comparison</p> <ul> <li>Option 1: Implement <code>T1.Equals(T2)</code>. For OO languages, also need <code>T1.SubtypeOf(T2)</code>.</li> <li>Option 2: Use unique objects for each distinct type. This allows for faster comparison using <code>==</code>.</li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#type-conversion","title":"Type conversion","text":"<ul> <li>Widening conversions: intended to preserve information (e.g., <code>int</code> to <code>float</code>).</li> <li>Narrowing conversions: can lose information (e.g., <code>double</code> to <code>int</code>).</li> <li>Explicit conversion (cast): programmer must write something to cause the conversion.</li> <li>Implicit conversion (coercion): done automatically by the compiler. Usually limited to widening conversions.</li> </ul> <p>Type conversion implementation</p> <ul> <li>For an expression like <code>E \u2192 E1 + E2</code>:<ol> <li>Find the maximum type: <code>E.type = max(E1.type, E2.type)</code>.</li> <li>Widen both operands to the max type: <code>a1 = widen(E1.addr, E1.type, E.type)</code>.</li> <li>Generate code for the operation: <code>gen(E.addr '=' a1 '+' a2)</code>.</li> </ol> </li> <li>The <code>widen</code> function generates conversion instructions if necessary.     <pre><code>Addr widen(Addr a, Type t, Type w) {\n    if (t == w) return a;\n    else if (t == integer &amp;&amp; w == float) {\n        temp = new Temp();\n        gen(temp '=' '(float)' a);\n        return temp;\n    }\n    else error;\n}\n</code></pre></li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#type-inference","title":"Type inference","text":"<ul> <li>Unification is the problem of determining whether two expressions <code>s</code> and <code>t</code> can be made identical by substituting expressions for the variables in <code>s</code> and <code>t</code>.</li> <li>This process is used to infer types in languages that don't require explicit type declarations.</li> <li>Example: For <code>fun length(x) = if null(x) then 0 else length(tl(x)) + 1;</code>, the compiler unifies type variables to deduce that <code>x</code> must be a <code>list(\u03b1)</code> and the function's type is <code>list(\u03b1) \u2192 integer</code>.</li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#5-symbol-tables-for-program-analysis-and-transformation","title":"5. Symbol Tables for Program Analysis and Transformation","text":"<ul> <li>Symbol tables are not just for compilation; they are crucial for advanced program analysis and transformation tools, like those used for automated refactoring.</li> <li>Generic Symbol Table: A generic, reusable symbol table implementation for statically scoped languages is available at <code>github.com/antlr/symtab</code>.</li> <li>Object-oriented models: Some tools (like JDeodorant) use detailed object-oriented models to represent types, properties, and relationships for sophisticated analysis.</li> <li>SciTools Understand:<ul> <li>Provides a code-search database based on Entities (anything in code like a file, class, variable) and References (a specific place where an entity appears, defining a relationship).</li> <li>A Kind Filter is a string used to filter lists (e.g., <code>db-&gt;ents(\"public method\")</code>).</li> <li>The Python API allows querying this database to analyze code structure.</li> <li>Example Query: List the file and line where each function is defined.     <pre><code>import understand\ndb = understand.open(\"test.udb\")\nents = db.ents(\"function ~unknown ~unresolved\")\nfor ent in sorted(ents, key=lambda ent: ent.longname()):\n    ref = ent.ref(\"definein\");\n    if ref is not None:\n        print (ent.longname(), \"(\", ent.parameters(), \")\");\n        print (\" \", ref.file().relname(), \"(\", ref.line(), \")\");\n</code></pre></li> </ul> </li> <li>Open Understand: An open-source implementation of the SciTools Understand Python API.</li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#summary","title":"Summary","text":"<ul> <li>Semantic checks ensure the correct usage of variables, objects, expressions, statements, functions, and labels in the program.</li> <li>Scope semantic checks ensure that identifiers are correctly used within the scope of their declaration.</li> <li>Type semantic checks ensure the type consistency of various constructs in the program.</li> <li>Symbol tables: a data structure for storing information about symbols in the program.<ul> <li>Used in semantic analysis and subsequent compiler stages.</li> <li>Used in program analysis for code query.</li> </ul> </li> </ul>"},{"location":"lectures/07_Semantic-Analysis/#references","title":"References","text":"<ul> <li>Aho, Alfred V., Monica S. Lam, Ravi Sethi and Jeffrey D. Ullman (2006). Compilers: Principles, Techniques, and Tools (2nd Edition).</li> <li>Hagberg, Aric A., Daniel A. Schult and Pieter J. Swart (2008). \u201cExploring Network Structure, Dynamics, and Function using NetworkX\u201d.</li> <li>Leicht, E. A. and M. E. J. Newman (mar. de 2008). \u201cCommunity Structure in Directed Networks\u201d.</li> <li>Mitchell, Brian S. and Spiros Mancoridis (mar. de 2006). \u201cOn the Automatic Modularization of Software Systems Using the Bunch Tool\u201d.</li> <li>Parr, T. (2009). Language Implementation Patterns: Create Your Own Domain-Specific and General Programming Languages.</li> <li>SciTools (2020). Understand.</li> <li>Tsantalis, Nikolaos and Alexander Chatzigeorgiou (2009). \u201cIdentification of Move Method Refactoring Opportunities\u201d.</li> <li>Zafeiris, Vassilis E., Sotiris H. Poulias, N.A. Diamantidis and E.A. Giakoumakis (2017). \u201cAutomated refactoring of super-class method invocations to the Template Method design pattern\u201d.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/","title":"Lecture 8: Intermediate-Code Generation","text":""},{"location":"lectures/08_Intermediate-Code-Generation/#outline","title":"Outline","text":"<ol> <li>Intermediate representations in compilers</li> <li>Abstract Syntax Trees<ul> <li>Variants of abstract syntax trees</li> </ul> </li> <li>Three-Address Codes<ul> <li>Three address code implementation</li> <li>SDT's for three address code generation</li> </ul> </li> <li>Static Single-Assignment Form</li> <li>Basic blocks and control flow graphs</li> <li>Summary of Lecture 8</li> </ol>"},{"location":"lectures/08_Intermediate-Code-Generation/#intermediate-code-generation-in-compilers","title":"Intermediate-code generation in compilers","text":"<p>Diagram Description: Compiler Pipeline</p> <p>The diagram illustrates the flow from source code to assembly code within a compiler, divided into a front-end and a back-end.</p> <ol> <li>Source Code (e.g., <code>if (b==0)a=b;</code>) is processed by Lexical Analysis into a Token Stream.</li> <li>The Token Stream is processed by Syntax Analysis to build a Parse tree and syntax tree. This stage interacts with the Symbol table by inserting information.</li> <li> <p>Semantic Analysis takes the syntax tree and produces a Decorated AST. This stage uses the symbol table. An example symbol table is shown:</p> Name Type Size b int 4 byte a float 8 byte </li> <li> <p>Intermediate code generation transforms the decorated AST into an intermediate representation.     The components from Lexical Analysis to Intermediate Code Generation constitute the Front-end (machine independent).</p> </li> <li>Code optimization refines the intermediate code.</li> <li>Code generation produces the final target code (e.g., Assembly code like <code>cmp rcx, 0</code> and <code>cmovz [rbp-8],rcx</code>).     The components from Code Optimization to the final target code constitute the Back-end (machine dependent).</li> </ol>"},{"location":"lectures/08_Intermediate-Code-Generation/#1-intermediate-representations-in-compilers","title":"1. Intermediate representations in compilers","text":""},{"location":"lectures/08_Intermediate-Code-Generation/#overview","title":"Overview","text":"<ul> <li>This lecture covers different types of intermediate representations (IRs):<ol> <li>Abstract syntax trees</li> <li>Three-address codes</li> <li>Static single-assignment (SSA) form </li> </ol> </li> <li>We will use the syntax-directed formalisms from Lecture 6 to specify the translation from high-level to low-level code.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#front-end-vs-back-end-in-compilers","title":"Front-end vs. back-end in compilers","text":"<ul> <li>In the analysis-synthesis model of a compiler, the front-end analyzes a source program and creates an intermediate representation, from which the back-end generates target code.</li> <li>Details of the source language are confined to the front-end, and details of the target machine to the back-end.</li> <li>With a suitably defined IR, a compiler for language <code>i</code> and machine <code>j</code> can be built by combining the front-end for language <code>i</code> with the back-end for machine <code>j</code>.</li> <li>This approach can save considerable effort: <code>m x n</code> compilers can be built by writing just <code>m</code> front-ends and <code>n</code> back-ends (i.e., <code>m+n</code>).</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#high-level-vs-low-level-ir","title":"High-level vs. low-level IR","text":"<ul> <li>In the process of translating a program in a given source  language into code for a given target machine, a compiler may  construct a sequence of intermediate representations.</li> <li>High-level representations are close to the source language.    <ul> <li>Syntax-trees are high level.</li> </ul> </li> <li>Low-level representations are close to the target machine.<ul> <li>Three-address code can range from high- to low-level, depending on the choice of operators.</li> </ul> </li> <li>For looping statements, a syntax tree represents the components of a statement, whereas three-address code contains labels and jump instructions, similar to machine language.</li> </ul> <p>Diagram Description: IR Flow</p> <p>The diagram shows a flow from multiple source languages (C, Fortran, Pascal) to a High-Level IR (HIR), which is then translated to a Low-Level IR (LIR). The LIR is then used to generate code for multiple target machines (Pentium, Java bytecode, PowerPC).</p>"},{"location":"lectures/08_Intermediate-Code-Generation/#c-programming-language-as-an-intermediate-code","title":"C programming language as an intermediate code","text":"<ul> <li>The choice of an IR varies from compiler to compiler.</li> <li>An IR may be an actual language or internal data structures shared by the compiler phases.</li> <li>C is often used as an intermediate form because:<ol> <li>it is flexible,</li> <li>it compiles into efficient machine code,</li> <li>and its compilers are widely available.</li> </ol> </li> <li>The original C++ compiler used a front-end that generated C code, treating a C compiler as its back-end.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#2-abstract-syntax-trees","title":"2. Abstract Syntax Trees","text":""},{"location":"lectures/08_Intermediate-Code-Generation/#recap-asts-for-expressions","title":"Recap: ASTs for expressions","text":"<p>An Abstract Syntax Tree (AST) for the expression <code>a1 := (2 + 12 * 3) / (6 - 19)</code> visually represents the operations and their operands in a hierarchical structure. The root is the assignment <code>:=</code>, with <code>a1</code> as the left child and the division <code>/</code> as the right child. The structure continues downwards, representing each sub-expression.</p>"},{"location":"lectures/08_Intermediate-Code-Generation/#code-generation-based-on-ast","title":"Code generation based on AST","text":"<p>A post-order traversal of the AST can generate stack-based machine code. For the expression <code>a1 := (2 + 12 * 3) / (6 - 19)</code>, the steps would be:</p> <ol> <li><code>push 2</code></li> <li><code>push 12</code></li> <li><code>push 3</code></li> <li><code>mul</code></li> <li><code>add</code></li> <li><code>push 6</code></li> <li><code>push 19</code></li> <li><code>sub</code></li> <li><code>div</code></li> <li><code>pop a1</code></li> </ol> <p></p>"},{"location":"lectures/08_Intermediate-Code-Generation/#ast-for-if-statement","title":"AST for if statement","text":"<ul> <li> <p>An AST can be an n-ary tree instead of a binary tree. For example, an <code>if</code> statement can have three children: the condition, the <code>then</code> block, and the <code>else</code> block.     </p> </li> <li> <p>We are interested in binary ASTs.</p> </li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#left-child-right-sibling-representation-lcrs-of-asts","title":"Left-child, right-sibling representation (LCRS) of ASTs","text":"<ul> <li>LCRS is a method for encoding a multi-way tree (where nodes can have any number of children) as a binary tree.</li> <li>Instead of each node pointing to all its children, it holds only two pointers: one to its first child and one to its immediate next sibling.</li> <li>Advantages of this binary tree representation:<ol> <li>It removes the need to know the number of children a node has in advance.</li> <li>It restricts the number of pointers per node to a maximum of two, simplifying the code.</li> </ol> </li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#sdt-for-ast-construction","title":"SDT for AST construction","text":"<p>A syntax-directed definition can be used to produce syntax trees for arithmetic expressions. Each production has a semantic rule that creates a new node (either <code>Node</code> for operators or <code>Leaf</code> for operands).</p> <p></p>"},{"location":"lectures/08_Intermediate-Code-Generation/#ast-in-eclipse-jdt","title":"AST in Eclipse JDT","text":"<p>The Eclipse Java Development Tools (JDT) generate detailed ASTs for Java code.</p> <ul> <li>If Statement: An <code>IfStmt</code> node has children for the condition (e.g., <code>BinaryExpr:greater</code>), the <code>then</code> block, and the <code>else</code> block. Each block contains further statements.</li> <li>While Statement: A <code>WhileStmt</code> node has children for the loop condition (e.g., <code>UnaryExpr:not</code>) and the loop body, which is a <code>BlockStmt</code>.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#variants-of-abstract-syntax-trees","title":"Variants of abstract syntax trees","text":""},{"location":"lectures/08_Intermediate-Code-Generation/#directed-acyclic-graphs-for-expressions","title":"Directed acyclic graphs for expressions","text":"<ul> <li>In an AST, the tree for the common subexpression subtree  would be replicated as many times as the subexpression  appears in the original expression.</li> <li>Adirected acyclic graph (hereafter called a DAG ) for an  expression identifies the common subexpressions  (subexpressions that occur more than once) of the expression.</li> <li>Like the syntax tree for an expression, a DAG has leaves  corresponding to atomic operands and interior nodes  corresponding to operators.</li> <li>The difference is that a node N in a DAG has more than one  parent if N represents a common subexpression.</li> <li> <p>DAG\u2019s can be constructed by using the same techniques that  construct syntax trees.</p> </li> <li> <p>DAG representation for the expression  a +a(b\u2212c)+(b\u2212c)d.</p> </li> <li>Node \u2019\u2212\u2019 has two parents, representing its two uses in the  subexpressions, <code>a * (b \u2212 c) and a + a *(b \u2212c)</code>.</li> </ul> <p></p>"},{"location":"lectures/08_Intermediate-Code-Generation/#the-value-number-method-for-constructing-dags","title":"The Value-Number method for constructing DAGs","text":"<ul> <li>This method stores nodes of a DAG in an array of records.</li> <li>Each record contains an operation code, and pointers to its children (or a lexical value for leaves). The index of the record in the array is its \"value-number\".</li> <li>Algorithm: When creating a new node, the algorithm first searches the array to see if an identical node (same operator and children) already exists. If so, it returns the value-number of the existing node; otherwise, it creates a new node and returns its new value-number.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#3-three-address-codes","title":"3. Three-Address Codes","text":""},{"location":"lectures/08_Intermediate-Code-Generation/#three-address-codes-general-style","title":"Three-address codes: General style","text":"<ul> <li>In three-address code (TAC), there is at most one operator on the right side of an instruction; that is, no built-up arithmetic expressions are permitted.</li> <li>A source-language expression like <code>x+y*z</code> might be translated into the sequence of three-address instructions:     <pre><code>(1) T1 = x * y \n(2) T2 = x + T1\n</code></pre></li> <li>where T1 and T2 are compiler-generated temporary names. </li> <li>unraveling of multi-operator arithmetic expressions and of nested ow-of-control statements makes three-address code desirable for target-code generation and optimization.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#three-address-codes-vs-asts","title":"Three-address codes vs. ASTs","text":"<ul> <li>Three-address code is a linearized representation of a syntax tree or a DAG in which explicit names correspond to the interior nodes of the graph. </li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#addresses-and-instructions","title":"Addresses and Instructions","text":"<p>An address in three-address code can be one of the following: </p> <ol> <li>A name: Source-program names can be used as addresses in three-address code.</li> <li>A constant: A compiler must handle many different types of constants and variables.</li> <li>A compiler-generated temporary: It is useful, especially in optimizing compilers, to create a distinct name each time a temporary is needed.</li> </ol> <p>A symbolic label represents the index of a three-address instruction in the sequence of instructions.</p>"},{"location":"lectures/08_Intermediate-Code-Generation/#common-forms-of-the-three-address-instructions","title":"Common forms of the three-address instructions","text":"<ol> <li> <p>Assignment instructions:</p> <ul> <li><code>x = y op z</code>, where op is a binary arithmetic or logical operation.</li> </ul> </li> <li> <ul> <li><code>x = op y</code>, where op is a unary operation like unary minus, logical negation, or type conversion operators.</li> </ul> </li> <li>Copy instructions: <code>x = y</code>, where x is assigned the value of y.</li> <li>Unconditional jump: <code>goto L</code>. The instruction with label L is executed next.</li> <li>Conditional jumps: <code>if x goto L</code> and <code>ifFalse x goto L</code>. These instructions execute the instruction with label L next if x is true or false, respectively.</li> <li>Relational jumps: <code>if x relop y goto L</code>, which applies a relational operator to x and y and jumps to L if the relation is true.</li> <li>Procedure and function calls:<ul> <li><code>param x</code> for parameters.</li> <li><code>call p, n</code> for a procedure call.</li> <li><code>y = call p, n</code> for function calls.</li> <li><code>return y</code> where y is an optional returned value.</li> </ul> </li> <li>Indexed copy instructions:<ul> <li><code>x = y[i]</code> sets x to the value in the location i memory units beyond location y.</li> <li><code>x[i] = y</code> sets the contents of the location i units beyond x to the value of y.</li> </ul> </li> <li>Address and pointer assignments: <code>x = &amp;y</code>, <code>x = *y</code>, and <code>*x = y</code>.</li> </ol>"},{"location":"lectures/08_Intermediate-Code-Generation/#three-address-instructions-example","title":"Three-address instructions: Example","text":"<p>For the statement <code>do i = i + 1; while (a[i] &lt; v);</code>, the TAC can use symbolic labels or position numbers.</p> <p></p>"},{"location":"lectures/08_Intermediate-Code-Generation/#addressing-array-elements","title":"Addressing array elements","text":"<ul> <li>To access an element in a one-dimensional array <code>A[low..high]</code> with element type <code>t</code> of size <code>T</code>, the address is calculated as <code>base + (i - low) * T</code>.</li> <li>This expression can be rewritten as <code>i * T + (base - low * T)</code>.</li> <li>The subexpression <code>C = base - low * T</code> can be pre-calculated at compile time.</li> <li>Accessing an element then requires two TAC instructions:     <pre><code>T1 = i * T\nT2 = T1 + C \n</code></pre></li> <li>For a two-dimensional array <code>A[low1..high1][low2..high2]</code> stored in row-major order (used in C-family languages), the address of <code>A[i1, i2]</code> is: <code>base + ((i1 - low1) * n2 + i2 - low2) * T</code> where <code>n2</code> is the number of columns.</li> <li>This can be optimized to <code>((i1 * n2) + i2) * T + C</code>, where the constant <code>C</code> is pre-calculated at compile time. This requires four TAC instructions.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#three-address-code-implementation","title":"Three-address code implementation","text":"<ul> <li>The description of three-address instructions specifies the  components of each type of instruction, but it does not specify  the representation of these instructions in a data structure.</li> <li>In a compiler, these instructions can be implemented as objects  or as records with fields for the operator and the operands.</li> <li> <p>Three such representations are called \"quadruples,\"  \"triples,\"and \"indirect triples.\"</p> </li> <li> <p>Quadruples: A quadruple has four fields: <code>op</code>, <code>arg1</code>, <code>arg2</code>, and <code>result</code>. For the code <code>a = (b * (-c)) + (b * (-c))</code>:</p> </li> </ul> op arg1 arg2 result 0 minus c t1 1 * b t1 t2 2 minus c t3 3 * b t3 t4 4 + t2 t4 t5 5 = t5 a <ul> <li>Triples: A triple has only three fields: <code>op</code>, <code>arg1</code>, and <code>arg2</code>. The result of an operation is referred to by its position, rather than by an explicit temporary name.</li> </ul> op arg1 arg2 0 minus c 1 * b (0) 2 minus c 3 * b (2) 4 + (1) (3) 5 = a (4) <ul> <li>Indirect Triples: This representation consists of a list of pointers to triples. This allows an optimizing compiler to move an instruction by reordering the pointer list, without affecting the triples themselves.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#sdts-for-three-address-code-generation","title":"SDT's for three address code generation","text":"<p>Syntax-Directed Translation (SDT) can be used to generate three-address code during parsing by attaching semantic rules to grammar productions.</p> <ul> <li>Expressions: For an expression like <code>E1 + E2</code>, the semantic rule generates code for <code>E1</code> and <code>E2</code>, creates a new temporary variable <code>temp</code>, and then generates the instruction <code>temp := E1.place + E2.place</code>.</li> <li>Array References: For an array reference like <code>id[E]</code>, the rules compute the offset. For a multi-dimensional array like <code>L1[E]</code>, the rules recursively calculate the address.</li> <li>Control Flow:<ul> <li>For an <code>if-then</code> statement, the SDT generates a new label <code>end</code> and creates a conditional jump: <code>if E.place = 0 then goto end</code>.</li> <li>For <code>if-then-else</code>, it uses two new labels (<code>else</code> and <code>end</code>) to manage the control flow.</li> <li>For a <code>while</code> loop, it uses two new labels (<code>begin</code> and <code>after</code>) to create the loop structure with a conditional jump to exit.</li> </ul> </li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#4-static-single-assignment-form","title":"4. Static Single-Assignment Form","text":"<ul> <li>Static single-assignment form (SSA) is an intermediate representation that facilitates certain code optimizations.</li> <li>Key Property: In SSA, all assignments are to variables with distinct names. To achieve this, original variables are split into multiple versions, typically with a subscript (e.g., <code>p</code> becomes <code>p1</code>, <code>p2</code>, <code>p3</code>, etc.), so that each new variable name corresponds to only one definition.</li> </ul> Three-Address Code Static Single-Assignment Form <code>p = a + b</code> <code>p1 = a + b</code> <code>q = p - c</code> <code>q1 = p1 - c</code> <code>p = q * d</code> <code>p2 = q1 * d</code> <code>p = e - p</code> <code>p3 = e - p2</code> <code>q = p + q</code> <code>q2 = p3 + q1</code>"},{"location":"lectures/08_Intermediate-Code-Generation/#the-\u03c6-function","title":"The \u03c6-function","text":"<ul> <li>The same variable may be defined in two different control-flow paths in a program.</li> <li>SSA uses a special \u03c6 (phi) function to merge the different versions of a variable that come from different control-flow paths. <code>y3 = \u03c6(y1, y2)</code> means that the value of <code>y3</code> will be <code>y1</code> if control came from the path where <code>y1</code> was defined, and <code>y2</code> if it came from the path where <code>y2</code> was defined.</li> <li>\u03c6-functions are not implemented as machine operations. A compiler implements a \u03c6-function by inserting \"move\" operations at the end of every predecessor block.</li> <li>SSA makes data-flow analyses like determining use-define (UD) chains easier to perform. When looking at a use of a variable, there is only one place where that variable may have received its value.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#5-basic-blocks-and-control-flow-graphs","title":"5. Basic blocks and control flow graphs","text":""},{"location":"lectures/08_Intermediate-Code-Generation/#control-flow-graph-cfg","title":"Control Flow Graph (CFG)","text":"<ul> <li>A Control Flow Graph (CFG) is a graph representation of computation and control flow in a program.</li> <li>Nodes: The nodes of the graph are Basic Blocks.</li> <li>Edges: An edge represents a branch between basic blocks.</li> <li>Basic Block: A sequence of assignment and expression evaluations that ends with a branch. It is a sequence of operations that are executed as a unit. Once the first operation in a basic block is performed, the remaining operations will also be performed without any other intervening operations.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#control-flow-graph-for-three-addresses-codes","title":"Control flow graph for three addresses codes","text":"<ul> <li>Control-Flow graph (CFG), similar to AST, is kind of a  intermediate representation generated by the compilers.</li> <li>It is typically created from the three addresses codes.</li> <li>Example: Programs that turns a 10 \u00d7 10 matrix a into an  identity matrix.<ul> <li>Real-valued array elements take 8 bytes each, and that the  matrix a is stored in row-major form.</li> </ul> </li> </ul> <pre><code>for i from 1 to 10 do\n    for j from 1 to 10 do\n        a[i, j] = 0.0;\nfor i from 1 to 10 do\n    a[i, i] = 1.0;\n</code></pre>"},{"location":"lectures/08_Intermediate-Code-Generation/#generating-control-flow-graphs","title":"Generating Control-Flow Graphs","text":"<p>The process involves two steps:</p> <ol> <li> <p>Partitioning into Basic Blocks: This is done by identifying the leaders in the three-address code. A leader is the first instruction of a basic block. The rules for finding leaders are:</p> <ol> <li>The first three-address instruction is a leader.</li> <li>Any instruction that is the target of a conditional or unconditional jump is a leader.</li> <li>Any instruction that immediately follows a conditional or unconditional jump is a leader. A basic block consists of a leader and all instructions up to but not including the next leader.</li> </ol> </li> <li> <p>Constructing the Flow Graph:</p> <ul> <li>The basic blocks are the nodes of the flow graph.</li> <li>There is an edge from block <code>B</code> to block <code>C</code> if and only if it is possible for the first instruction in block C to immediately follow the last instruction in block B. This can happen in two ways:<ul> <li>There is a jump from the end of <code>B</code> to the beginning of <code>C</code>.</li> <li><code>C</code> immediately follows <code>B</code> in the original order, and <code>B</code> does not end in an unconditional jump.</li> </ul> </li> <li>Special <code>entry</code> and <code>exit</code> nodes that do not correspond to executable instructions are often added.</li> </ul> </li> </ol> <p>CFGs are essential for many code optimizations because they make the transfer of control explicit and are helpful for tasks like register allocation.</p>"},{"location":"lectures/08_Intermediate-Code-Generation/#6-summary-of-lecture-8","title":"6. Summary of Lecture 8","text":"<ul> <li>Intermediate Representation (IR): A compiler's front-end analyzes a source program and creates an IR, from which the back-end generates target code. An IR is typically a combination of a graphical notation and three-address code. </li> <li>Abstract Syntax Tree (AST): A node in an AST represents a construct, and the children of the node represent its subconstructs.</li> <li>Three-Address Code (TAC): Takes its name from instructions of the form \"x = y op z\", with at most one operator per instruction. It also includes additional instructions for control flow. </li> <li>Control Flow Graph (CFG): A graph representation of all paths that might be traversed through a program during its execution. The CFG is an essential tool for many compiler optimizations and static analysis.</li> </ul>"},{"location":"lectures/08_Intermediate-Code-Generation/#references","title":"References","text":"<ul> <li>Aho,Alfred V., Monica S. Lam, Ravi Sethi and Jeffrey D. Ullman  (2006). Compilers: Principles, Techniques, and Tools (2nd  Edition). USA: Addison-Wesley Longman Publishing Co., Inc. ISBN:</li> <li> <p>URL: http://infolab.stanford.edu/  %5C~ullman/dragon/errata.html.</p> </li> <li> <p>Ammann,P. and J. Offutt (2016). Introduction to software  testing. Cambridge University Press. ISBN: 9781316773123. URL:  https://cs.gmu.edu/%5C~offutt/softwaretest.</p> </li> </ul>"},{"location":"projects/","title":"Compiler projects agenda","text":""},{"location":"projects/#project-proposals-list","title":"Project proposals list","text":"<p>I have designed and planned several practical projects that explore the applications of compiler science in program analysis. These projects, detailed in Table 1, have been assigned to students enrolled in the IUST compiler course across various semesters. Each project aims to deepen students' understanding of compiler concepts through hands-on experience. For more information, please click on the links in the \"Project\" column to view the respective project proposals.</p> <p>Table 1: Compiler projects.</p> Project Description Semesters Courses OpenUnderstand  2 Low-level source   code metrics calculation Spring 2022 Compiler OpenUnderstand Symbols table   development Fall 2021,   Spring 2022, Compiler QualityMeter - Source code   quality attribute computation     - Refactoring   opportunity detection Fall 2021 Advanced compiler CodART 2 Source code   smell detection Spring 2021   (Cancelled) Compiler CodART Source code   refactoring Fall 2020,   Spring 2021, Compiler CodART Refactoring   to design pattern at the source code level Fall 2020 Advanced compiler CleanCode Source code   smell detection Fall 2019,   Spring 2020 Compiler CodA Source code instrumentation   and testbed analysis tool Fall 2018 Compiler /     Advanced compiler ANTLR MiniJava Parse-tree and intermediate code generation for the MiniJava and eMiniJava programming languages with   ANTLR Fall 2016,   Spring 2017, Fall 2024 Compiler"},{"location":"projects/#top-student-solutions","title":"Top student solutions","text":"<ul> <li>MiniJava to C Compiler (Aghajari) [www]</li> </ul>"},{"location":"projects/core_clean_code_development/","title":"Core clean code development","text":"<p>The input to our software tool, CleanCode, is a c# class.  cleanCode analyzes the source code and determines how clean the code is.  The output is a list of the line numbers of the given class, in which the clean code principles proposed by Robert Martin, in his book, Clean Code, are violated. The current version of cleanCode, checks 14 principles and we are going to add more principles in our next version. </p> <p>Click to visit clean code project.</p>"},{"location":"projects/core_code_smell_development/","title":"Core code smell development","text":"<p>The following proposal has been initially prepared for the IUST Compiler and Advanced Software Engineering courses in Winter and Spring 2021.</p> <p>Note: Before reading this proposal ensure that you have read and understood the CodART white-paper.</p> <p>Students may form groups of up to three persons. Each group must develop mechanisms for a subset of code smells listed in Table 2. The exact list of code smells will be assigned to each group subsequently. The refactoring operations in Table 1 and code smells in Table 2 may update during the semester. </p> <p>To facilitate and organized the development process, this proposal defines the project in various phases. The project is divided into three separate phases.</p> <p>In the first phase, students must read about refactoring and code smells and understand the current state of the CodART completely. As a practice, they are asked to fix the existing issues on the project repository about refactoring operations developed in the first proposal.</p> <p>In the second phase, each group is asked to develop algorithms to automatically detect one or more code smells in a given Java project using ANTLR tool and other compiler techniques. TA team frequently helps the students at this phase to develop their algorithms. </p> <p>In the third phase, each group is asked to connect the code smells detection scripts to the corresponding refactoring and automate the overall quality improvement process. </p>"},{"location":"projects/core_code_smell_development/#grading-policy-for-bsc-students","title":"Grading policy for BSc students","text":"<p>Table 6 shows the grading policy for the BSc students. It may change in the future. </p> <p>Table 6. grading policy for BSc students</p> Activity Score   (100) Understanding   the CodART project and Fix the existing issues 30 Implementing   smell detection approaches 40 Connecting   code smells to refactoring and harnessing the overall process 20 Documenting   the new source codes and pushing them to GitHub 10 Testing   project on all projects available in CodART benchmarks 20+   (extra bonus)"},{"location":"projects/core_code_smell_development/#grading-policy-for-msc-students","title":"Grading policy for MSc students","text":"<p>Table 7 shows the grading policy for the MSc students. It may change in the future. </p> <p>Table 7. grading policy for MSc students</p> Activity Score   (100) Understanding   the paper and presenting it 20 Implementing   the paper 30 Evaluating   the implementation 30 Documenting   the project 20 Testing   project on all projects available in CodART benchmarks 20+   (extra bonus) <p>To follow project's future phases, meet our next proposal: Core search-based development.</p>"},{"location":"projects/core_refactoring_to_design_patterns_development/","title":"Core refactoring to design patterns development","text":"<p>To be announced.</p>"},{"location":"projects/core_refactorings_development/","title":"Core refactoring development","text":"<p>The following proposal was initially prepared for the IUST Compiler and Advanced compiler courses in Fall 2020. </p> <p>Students must form groups of up to three persons, and each group must implement several refactoring operations. The exact list of refactoring will be assigned to each group subsequently. The refactoring operations in Table 1 may update during the semester. </p> <p>As an example of refactoring automation, we have implemented the EncapsulateField refactoring, illustrated in Figure 1. A na\u00efve implementation is available on the project official Github page at https://m-zakeri.github.io/CodART. In addition, 26 refactoring operations in Table 1 have been implemented by MultiRefactor [7] based on RECODER, three of them have been implemented by JDeodrant [8], and other operations have been automated in  [3], [6]. RECODER extracts a model of the code that can be used to analyze and modify the code before the changes are applied and written to file. The tool takes Java source code as input and will output the modified source code to a specified folder. The input must be fully compilable and must be accompanied by any necessary library files as compressed jar files.</p>"},{"location":"projects/core_refactorings_development/#grading-policy-for-bsc-students","title":"Grading policy for BSc students","text":"<p>Table 4 shows the grading policy for the BSc students. It may change in the future. </p> <p>Table 4. grading policy for BSc students</p> Activity Score   (100) Refactoring   operations implementation (moderate level) 50 Evaluation   of the tool on the benchmark projects 30 Documentations 20 Search-based   refactoring recommendation 30+   (extra bonus)"},{"location":"projects/core_refactorings_development/#grading-policy-for-msc-students","title":"Grading policy for MSc students","text":"<p>Table 5 shows the grading policy for the MSc students. It may change in the future. </p> <p>Table 5. grading policy for MSc students</p> Activity Score   (100) Refactoring   operations implementation (advanced level) 40 Search-based   refactoring recommendation 30 Evaluation   of the tool on the benchmark projects 20 Documentations 10 Improving   the state-of-the-arts papers 30+   (extra bonus) <p>To follow project's phases, refer to our next proposal: Core code smell development.</p>"},{"location":"projects/core_software_metrics_development/","title":"Core software metrics development","text":"<p>Visit QualityMeter project repository.</p>"},{"location":"projects/core_source_code_instrumentation_development/","title":"Core source code instrumentation development","text":"<p>Visit the CodA project documentation.</p>"},{"location":"projects/core_symbol_table_development/","title":"Core symbol table development","text":"<p>Visit OpenUnderstand project documentation.</p>"},{"location":"projects/mini_java_compiler_development/","title":"Mini-Java compiler front-end development","text":"<p>The project includes parse tree and intermediate code generation (in form of C three addresses (TAC) code) for the MiniJava or eMiniJava programming languages with ANTLR.</p>"},{"location":"projects/mini_java_compiler_development/#project-title","title":"Project Title","text":"<p>Three-Address Code Generation for MiniJava Language Using ANTLR</p>"},{"location":"projects/mini_java_compiler_development/#introduction","title":"Introduction","text":"<p>The goal of this project is to design and implement a compiler that translates a subset of the Java programming language, referred to as MiniJava or eMiniJava, into Three-Address Code (TAC) in C programming language. The project will involve constructing a parse tree, performing semantic analysis, and generating intermediate code that can be used for further compilation or execution.</p>"},{"location":"projects/mini_java_compiler_development/#objectives","title":"Objectives","text":"<ol> <li>Define the MiniJava or eMiniJava Language: Establish the syntax and semantics of the MiniJava language, including data types, control structures, and function definitions.</li> <li>Build a Parser Using ANTLR: Utilize ANTLR to create a parser that can read Mini-Java source code and produce a parse tree.</li> <li>Create a Semantic Analyzer: Implement a semantic analysis phase to check for type correctness and scope resolution.</li> <li>Generate Three-Address Code: Develop an intermediate code generation module that translates the parse tree into TAC in C language.</li> <li>Testing and Validation: Test the compiler with various MiniJava programs to validate the correctness and efficiency of the generated TAC.</li> </ol>"},{"location":"projects/mini_java_compiler_development/#proposals","title":"Proposals","text":"<ul> <li>Spring 2025, Team work policy</li> <li>Fall 2024, Team work policy</li> </ul>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/","title":"Mini-Java compiler front-end development","text":"<p>The project includes parse tree and intermediate code generation (in form of C three addresses (TAC) code) for the MiniJava or eMiniJava programming languages with ANTLR.</p>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#project-title","title":"Project Title","text":"<p>Three-Address Code Generation for MiniJava Language Using ANTLR</p>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#introduction","title":"Introduction","text":"<p>The goal of this project is to design and implement a compiler that translates a subset of the Java programming language, referred to as MiniJava or eMiniJava, into Three-Address Code (TAC) in C programming language. The project will involve constructing a parse tree, performing semantic analysis, and generating intermediate code that can be used for further compilation or execution.</p>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#objectives","title":"Objectives","text":"<ol> <li>Define the MiniJava or eMiniJava Language: Establish the syntax and semantics of the MiniJava language, including data types, control structures, and function definitions.</li> <li>Build a Parser Using ANTLR: Utilize ANTLR to create a parser that can read Mini-Java source code and produce a parse tree.</li> <li>Create a Semantic Analyzer: Implement a semantic analysis phase to check for type correctness and scope resolution.</li> <li>Generate Three-Address Code: Develop an intermediate code generation module that translates the parse tree into TAC in C language.</li> <li>Testing and Validation: Test the compiler with various MiniJava programs to validate the correctness and efficiency of the generated TAC.</li> </ol>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#project-steps","title":"Project Steps","text":""},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#step-1-define-the-mini-java-language","title":"Step 1: Define the Mini-Java Language","text":"<ul> <li>Syntax Specification: Create a formal grammar for MiniJava or eMiniJava, including:</li> <li>Basic data types (int, boolean, etc.)</li> <li>Control structures (if-else, while loops)</li> <li>Method definitions and calls</li> <li>Class declarations</li> <li>Semantics: Outline the semantic rules, including type checking and scoping.</li> </ul> <p>Note: The grammar can be found on MiniJava grammar or eMiniJava or Java8 or Java20.</p>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#step-2-build-a-parser-using-antlr","title":"Step 2: Build a Parser Using ANTLR","text":"<ul> <li>ANTLR Setup: Install ANTLR and set up the development environment.</li> <li>Grammar File Creation: Write the ANTLR grammar file (<code>.g4</code>) based on the defined syntax.</li> <li>Parser Generation: Use ANTLR to generate the parser and lexer from the grammar file.</li> <li>Parse Tree Construction: Implement code to construct the parse tree from MiniJava source code.</li> </ul>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#step-3-create-a-semantic-analyzer","title":"Step 3: Create a Semantic Analyzer","text":"<ul> <li>Symbol Table Management: Implement a symbol table to store variable and method information.</li> <li>Type Checking: Develop functions to perform type checking during the traversal of the parse tree.</li> <li>Scope Resolution: Ensure correct scoping rules for variables and methods.</li> </ul>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#step-4-generate-three-address-code","title":"Step 4: Generate Three-Address Code","text":"<ul> <li>Intermediate Representation Design: Define the structure of TAC, which typically consists of instructions in the form <code>OP dst, src1, src2</code> ( or <code>dst = scr1 OP scr2 ;</code> in the C language).</li> <li>Code Generation Algorithm:</li> <li>Traverse the parse tree.</li> <li>Generate TAC instructions based on the constructs encountered (e.g., assignments, arithmetic operations, method calls). Implement a visitor or listener to traverse the parse tree and generate TAC.</li> <li>Maintain temporary variables for intermediate results.</li> <li>Output Format: Implement functionality to output the generated TAC in C language format (e.g., <code>dst = scr1 OP scr2 ;</code>).</li> </ul>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#step-5-testing-and-validation","title":"Step 5: Testing and Validation","text":"<ul> <li>Test Cases Creation: Develop a suite of MiniJava programs that cover various features of the language (see also: MiniJava example programs.</li> <li>Validation Process: Execute the generated TAC using a C compiler to verify correctness.</li> <li>(Optional) Performance Evaluation: Assess the efficiency of the generated code and optimize if necessary.</li> </ul>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#deliverables","title":"Deliverables","text":"<ol> <li>ANTLR Grammar File: The <code>.g4</code> file defining the MiniJava syntax.</li> <li>Parser Implementation: Code for the parser and lexer generated by ANTLR.</li> <li>Semantic Analyzer Code: Implementation of type checking and scope resolution.</li> <li>TAC Generation Module: Code that translates the parse tree into Three-Address Code in C.</li> <li>Documentation: A comprehensive report detailing the design decisions, implementation steps, and testing results.</li> </ol>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#timeline","title":"Timeline","text":"Week Task 1 Define MiniJava language syntax and semantics 2 Set up ANTLR and create grammar file 3 Generate parser and lexer; construct parse tree 4 Implement semantic analysis 5 Develop TAC generation module 6 Testing and validation of generated code 7 Finalize documentation and prepare presentation"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#teamwork","title":"Teamwork","text":"<p>Read more at Teamwork Policy page.</p>"},{"location":"projects/fall2024/minijava-to-c-compiler-development-fall2024/#conclusion","title":"Conclusion","text":"<p>This project aims to provide a comprehensive understanding of compiler design principles by implementing a complete pipeline from parsing to intermediate code generation. By focusing on MiniJava and utilizing ANTLR, students will gain hands-on experience with modern tools and techniques in compiler construction. </p> <p>The project will provide hands-on experience in compiler design, specifically in parsing and intermediate code generation. By the end of the project, students will have a functional compiler that translates MiniJava code into three-address code in C, demonstrating their understanding of compiler construction principles.</p> <p>The successful completion of this project will not only enhance theoretical knowledge but also practical skills in programming languages and compilers.</p>"},{"location":"projects/fall2024/team_work_policy/","title":"Teamwork Policy for Compiler Design Course Project","text":""},{"location":"projects/fall2024/team_work_policy/#objective","title":"Objective","text":"<p>To ensure effective collaboration, communication, and accountability among team members working on the Compiler Design Course Project focused on the generation of Three-Address Code for Mini-Java Language.</p>"},{"location":"projects/fall2024/team_work_policy/#team-composition","title":"Team Composition","text":"<ul> <li>Each team must consist of at most three members.</li> <li>Teams should be formed based on mutual agreement among members / students.</li> <li>Each member is expected to contribute equally to the project.</li> </ul>"},{"location":"projects/fall2024/team_work_policy/#example-of-roles-and-responsibilities-assignments","title":"Example of Roles and Responsibilities Assignments","text":"<ul> <li>Team Leader: </li> <li>Coordinates meetings and manages project timelines.</li> <li>Serves as the primary point of contact with the instructor.</li> <li> <p>Ensures that tasks are assigned and deadlines are met.</p> </li> <li> <p>Developer 1: </p> </li> <li>Focuses on building the parser and lexer using ANTLR.</li> <li> <p>Responsible for writing the ANTLR grammar file and generating the parser.</p> </li> <li> <p>Developer 2: </p> </li> <li>Works on semantic analysis and symbol table management.</li> <li> <p>Implements type checking and scope resolution functionalities.</p> </li> <li> <p>Developer 3 (if applicable):</p> </li> <li>Focuses on Three-Address Code generation and output formatting.</li> <li>Assists in testing and validation of the generated code.</li> </ul> <p>(Note: Roles can be adjusted based on team preferences and strengths but must discuss in the project final report.)</p>"},{"location":"projects/fall2024/team_work_policy/#communication","title":"Communication","text":"<ul> <li>Regular Meetings: </li> <li>Teams are required to meet at least once a week to discuss progress, challenges, and next steps.</li> <li> <p>Meetings can be held in-person or virtually, depending on team preferences.</p> </li> <li> <p>Communication Channels: </p> </li> <li>Teams should establish a preferred communication platform (e.g., Slack, Discord, email) for ongoing discussions and updates.</li> <li>Document important decisions and discussions in a shared document accessible to all members.</li> </ul>"},{"location":"projects/fall2024/team_work_policy/#task-management","title":"Task Management","text":"<ul> <li>Task Breakdown: </li> <li>Divide the project into manageable tasks and assign them to team members based on their roles.</li> <li> <p>Use project management tools (e.g., Trello, Asana, GitHub Projects) to track progress.</p> </li> <li> <p>Accountability: </p> </li> <li>Each member is responsible for completing their assigned tasks on time.</li> <li>Team members should support each other in overcoming challenges and ensuring project milestones are met.</li> </ul>"},{"location":"projects/fall2024/team_work_policy/#conflict-resolution","title":"Conflict Resolution","text":"<ul> <li>Open Communication: </li> <li> <p>Encourage open discussion of any conflicts or disagreements among team members or even other teams.</p> </li> <li> <p>Mediation: </p> </li> <li>If conflicts cannot be resolved internally, seek assistance from the instructor or course facilitator for mediation.</li> </ul>"},{"location":"projects/fall2024/team_work_policy/#submission-and-documentation","title":"Submission and Documentation","text":"<ul> <li>Collaborative Work: </li> <li> <p>All code and documentation should reflect the collaborative effort of the team.</p> </li> <li> <p>Version Control: </p> </li> <li>Use version control systems (e.g., Git) to manage code changes and maintain a history of contributions by each member.</li> </ul>"},{"location":"projects/fall2024/team_work_policy/#evaluation-criteria","title":"Evaluation Criteria","text":"<ul> <li>Team performance will be evaluated based on:</li> <li>Quality of contributions from each member.</li> <li>Collaboration and communication effectiveness.</li> <li>Adherence to deadlines and project milestones.</li> </ul>"},{"location":"projects/fall2024/team_work_policy/#conclusion","title":"Conclusion","text":"<p>This teamwork policy aims to foster a positive and productive working environment for all team members. By following these guidelines, teams can effectively collaborate on the Compiler Design Course Project, ensuring a successful outcome while enhancing individual learning experiences.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/","title":"Mini-Java to C","text":"<p>Welcome to the Compiler Course Project! In this project, you will develop a compiler to translate Mini-Java code into C, including the generation of Three Address Code (TAC). This hands-on project will help you understand the key components involved in compiler construction.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Mini-Java to C</li> <li>Overview of Compiler Components</li> <li>Lexer (Lexical Analysis)</li> <li>Parser (Syntax Analysis)</li> <li>Semantic Analyzer</li> <li>Code Generator</li> <li>Using ANTLR</li> <li>Introduction to Mini-Java</li> <li>Supported Features of Mini-Java<ul> <li>Object-Oriented and Inheritance</li> <li>Control Structures</li> <li>Arrays</li> <li>Basics</li> </ul> </li> <li>Limitations of Mini-Java</li> <li>Understanding the Parser</li> <li>Parsing Mini-Java Code</li> <li>Steps in Parsing</li> <li>Example AST Representation</li> <li>Importance of the Parser</li> <li>Semantic Analyzer</li> <li>Scope and Variable Resolution<ul> <li>Example 1: Undefined Variable</li> <li>Example 2: Variable Shadowing</li> </ul> </li> <li>Type Checking<ul> <li>Example 3: Type Mismatch</li> </ul> </li> <li>Method Call Verification<ul> <li>Example 4: Incorrect Argument Type in Method Call</li> <li>Example 5: Incorrect Number of Arguments in Method Call</li> </ul> </li> <li>Resolving Instance Variables with <code>this</code><ul> <li>Example 6: Using <code>this</code> for Accurate Scope Resolution</li> </ul> </li> <li>Correct Usage of <code>break</code> and <code>continue</code><ul> <li>Example 7: Ensuring <code>break</code> and <code>continue</code> are Used Within Loop Scopes</li> </ul> </li> <li>Ensuring Effective Semantic Analysis</li> <li>Generating TAC</li> <li><code>if</code> Statement</li> <li><code>while</code> Loop</li> <li><code>do-while</code> Loop</li> <li><code>for</code> Loop</li> <li>Implementing Inheritance in C</li> <li>Understanding Inheritance in C<ul> <li>Example: Java Code with Inheritance</li> <li>Equivalent C Code Using Structures</li> </ul> </li> <li>Accessing Inherited Fields</li> <li>Casting Between Structures<ul> <li>Example: Casting in C</li> </ul> </li> <li>Method Overriding Example<ul> <li>Java Example with Method Overriding</li> <li>Translating to C with Function Pointers</li> <li>Implementing Functions</li> <li>Initializing Structs and Setting Up Function Pointers</li> <li>Invoking Methods</li> </ul> </li> <li>Handling int[] Array</li> <li>Suggestion: Predefined int_array Class</li> <li>Translating Mini-Java Array Declarations</li> <li>TAC with Functions and Stack</li> <li>Concept</li> <li>Example Translation</li> <li>Obvious Constraints and Examples</li> <li>Constraint 1: Single Entry Point</li> <li>Constraint 2: Unique Class Names</li> <li>Constraint 3: Method and Class Definition Order</li> <li>Constraint 4: Translating System.out.println()</li> <li>Constraint 5: Ignoring Comments</li> <li>Constraint 6: No Cyclic Dependencies</li> <li>Deliverables</li> <li>Timeline</li> <li>Teamwork</li> <li>Conclusion</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#overview-of-compiler-components","title":"Overview of Compiler Components","text":"<p>A compiler comprises four main parts:</p> <ol> <li>Lexer (Lexical Analysis)</li> <li> <p>The lexer, or lexical analyzer, is responsible for breaking down the source Mini-Java code into tokens. These tokens are the smallest units of meaning, such as keywords, operators, identifiers, and literals. The lexer simplifies the parsing process by transforming complex strings into manageable elements. Lexical analysis in Java typically involves recognizing language-specific syntax and removing unnecessary characters like whitespace and comments.</p> </li> <li> <p>Parser (Syntax Analysis)</p> </li> <li> <p>The parser takes the tokens produced by the lexer and arranges them according to the grammatical structure of the language to build an Abstract Syntax Tree (AST). This tree representation captures the hierarchical syntactic structure of the source code, outlining the relationships between different constructs. Parsing ensures that the code adheres to the syntax rules of Mini-Java and serves as a foundation for subsequent analysis and code generation.</p> </li> <li> <p>Semantic Analyzer</p> </li> <li> <p>Semantic analysis checks the source code for semantic errors, ensuring logical consistency and correctness. This phase involves type checking, scope resolution, and verifying that operations are performed on compatible data types. The semantic analyzer uses the AST to validate constraints not covered by syntax rules, such as variable declaration before use and type compatibility.</p> </li> <li> <p>Code Generator</p> </li> <li>The code generator is responsible for translating the AST into an intermediate representation, such as Three Address Code (TAC), which simplifies the process of generating target code. After producing TAC, the code generator further translates this representation into C code, maintaining the original program's semantics. This output can be compiled and executed using a C compiler.</li> </ol>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#using-antlr","title":"Using ANTLR","text":"<p>You can use ANTLR for this project, which simplifies the creation of the Lexer and Parser components for you. ANTLR allows you to define a grammar for Mini-Java, and it will generate the corresponding lexer and parser code.</p> <ul> <li>Advantages of ANTLR: </li> <li>Reduces the complexity of manual lexer and parser implementation.</li> <li>Accepts grammar as input and outputs the necessary code.</li> <li> <p>Supports multiple target languages, including Java and C++.</p> </li> <li> <p>Using ANTLR:</p> </li> <li>Define a well-structured grammar file for Mini-Java.</li> <li>Use ANTLR to generate lexer and parser code from the grammar.</li> <li>Integrate the generated code into your compiler project.</li> </ul> <p>Your compiler can be implemented in any programming language, but we suggest using either C++ or Java due to their compatibility with ANTLR and prevalent use in compiler projects.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#introduction-to-mini-java","title":"Introduction to Mini-Java","text":"<p>Mini-Java is a simplified version of the Java programming language designed for educational purposes, particularly to teach compiler construction. It maintains key features of Java's object-oriented nature but with several restrictions to simplify parsing and semantic analysis.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#supported-features-of-mini-java","title":"Supported Features of Mini-Java","text":"<p>Your compiler should support the following aspects of Mini-Java:</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#object-oriented-and-inheritance","title":"Object-Oriented and Inheritance","text":"<ul> <li>Classes and inheritance: Mini-Java supports creating classes and deriving subclasses from existing ones.</li> <li>Method overriding: Allows a subclass to provide a specific implementation of a method that is already defined in its superclass.</li> <li>Field access across inheritance chains: Enables access to fields defined in superclasses.</li> <li><code>this</code> reference: Refers to the current object whose method or constructor is being invoked.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#control-structures","title":"Control Structures","text":"<ul> <li>Conditional Statements:</li> <li> <p><code>if</code>, <code>else if</code>, and <code>else</code> for branching logic.</p> </li> <li> <p>Loops:</p> </li> <li><code>for</code>, <code>while</code>, and <code>do-while</code> loops for iterative operations.</li> <li>Supports <code>break</code> and <code>continue</code> statements for loop control.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#arrays","title":"Arrays","text":"<ul> <li>Integer array support (<code>int[]</code>): Only integer arrays are supported.</li> <li>Array length property: Retrieves the number of elements in an array.</li> <li>Array indexing: Access elements via zero-based index.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#basics","title":"Basics","text":"<ul> <li>Basic data types: Only <code>int</code> and <code>boolean</code> are supported as primitive types.</li> <li>Classes and objects: Object-oriented approach with no interfaces or inner classes.</li> <li>Method calls: Invoke methods defined in classes.</li> <li>Arithmetic operations: Supports <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code> for mathematical operations.</li> <li>Logical operations: Supports <code>&amp;&amp;</code>, <code>||</code>, <code>!</code> for boolean logic.</li> <li>Relational operators: Includes <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>==</code>, <code>!=</code> for comparisons.</li> <li>Variable assignments: Manage and mutate data held in variables. Note that defining variable values within a class itself is not allowed (only declaration).</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#limitations-of-mini-java","title":"Limitations of Mini-Java","text":"<ul> <li>No support for interfaces or inner classes.</li> <li>No support for strings or the <code>switch</code> statement.</li> </ul> <p>This specification defines the scope of Mini-Java programs that your compiler will need to handle. Ensure your grammar and code generation handle these features while adhering to their constraints.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#understanding-the-parser","title":"Understanding the Parser","text":"<p>The parser is a critical component in the compilation process, transforming the sequence of tokens generated by the lexer into a structured format, typically an Abstract Syntax Tree (AST). This representation captures the syntactic structure of the source code, facilitating semantic analysis and code generation in subsequent phases.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#parsing-mini-java-code","title":"Parsing Mini-Java Code","text":"<p>Let\u2019s explore how your parser might handle and parse a simple Mini-Java program such as: <pre><code>public class Main {  \n    public static void main(String[] args) {  \n        int a = 100;  \n        int b = 2;  \n        System.out.println(a * b);  \n    }  \n}\n</code></pre></p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#steps-in-parsing","title":"Steps in Parsing","text":"<ol> <li>Tokenization: </li> <li> <p>The lexer first breaks down the source code into tokens, identifying keywords like <code>public</code>, <code>class</code>, <code>int</code>, etc., identifiers like <code>Main</code>, <code>a</code>, <code>b</code>, and literals like <code>100</code> and <code>2</code>.</p> </li> <li> <p>Grammar Comprehension: </p> </li> <li> <p>The parser applies the grammar rules of Mini-Java to these tokens. It recognizes constructs such as class declarations, method definitions, variable declarations, and expressions.</p> </li> <li> <p>Constructing the AST:</p> </li> <li> <p>Class Declaration: The parser recognizes the class declaration <code>public class Main</code> and represents it as a node in the AST, with <code>Main</code> as a child node.</p> </li> <li> <p>Method Declaration: It then identifies the <code>public static void main(String[] args)</code> method header, creating another node under the <code>Main</code> class node.</p> </li> <li> <p>Variable Declarations: </p> <ul> <li><code>int a = 100;</code> and <code>int b = 2;</code> are decomposed into type <code>int</code> with identifiers <code>a</code> and <code>b</code>, each having respective children nodes representing the literal assignments.</li> </ul> </li> <li> <p>Expression Parsing: </p> <ul> <li>The expression <code>System.out.println(a * b);</code> is parsed by first identifying the method invocation <code>System.out.println</code> and then parsing the arithmetic operation <code>a * b</code>. This operation is further broken down into its respective components \u2014 identifiers <code>a</code> and <code>b</code> with the multiplication operator <code>*</code>.</li> </ul> </li> <li> <p>Validation and Error Handling:</p> </li> <li>During parsing, the parser checks for syntactic errors. If an error is detected, such as a missing semicolon or an unexpected token, the parser provides an error message with the location and description of the error.</li> </ol>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-ast-representation","title":"Example AST Representation","text":"<p>From the parsing process described, the AST might look something like this in a simplified representation:</p> <ul> <li>Class Node: <code>Main</code></li> <li>Method Node: <code>main</code><ul> <li>Declaration Node: <code>int a = 100</code></li> <li>Declaration Node: <code>int b = 2</code></li> <li>Expression Node: <code>System.out.println</code></li> <li>Operator Node: <code>*</code><ul> <li>Identifier Node: <code>a</code></li> <li>Identifier Node: <code>b</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#importance-of-the-parser","title":"Importance of the Parser","text":"<p>The parser not only structures the code but also flags syntactic errors that need to be resolved for successful compilation. By creating an accurate AST, subsequent phases, like semantic analysis and code generation, receive the structured data required to enforce language semantics and produce intermediate and target code representations.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#semantic-analyzer","title":"Semantic Analyzer","text":"<p>Semantic analysis is a crucial phase in the compilation process that ensures a program is semantically correct according to language rules. Building upon the parser's syntax tree, semantic analysis focuses on scope resolution, type checking, and enforcing semantic constraints.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#scope-and-variable-resolution","title":"Scope and Variable Resolution","text":"<p>Semantic analysis ensures that each variable is declared before use and that variable access is correct within the given scope. Here are some examples:</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-1-undefined-variable","title":"Example 1: Undefined Variable","text":"<pre><code>int i = 100;\nSystem.out.println(x); // Error: 'x' is not defined!\n</code></pre> <ul> <li>Analysis: The parser may construct a correct syntax tree for this statement, but the semantic analysis will flag an error because x is used without being declared in the current or any enclosing scope.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-2-variable-shadowing","title":"Example 2: Variable Shadowing","text":"<pre><code>int i = 100;\n{\n    boolean i = true;\n    System.out.println(i); // Prints true, i is boolean\n}\nSystem.out.println(i); // Prints 100, i is integer\n</code></pre> <ul> <li>Analysis: This example demonstrates variable shadowing, where the inner block's <code>boolean i = true;</code> shadows the outer <code>int i = 100;</code>. The semantic analyzer ensures that the correct variable is accessed based on the current scope.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#type-checking","title":"Type Checking","text":"<p>Semantic analysis must enforce type constraints to ensure expressions and assignments are type-safe.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-3-type-mismatch","title":"Example 3: Type Mismatch","text":"<pre><code>int i = true; // Error: Cannot assign boolean to int\n</code></pre> <ul> <li>Analysis: The semantic analysis phase detects a type mismatch error, as a boolean value <code>true</code> is being incorrectly assigned to an integer variable.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#method-call-verification","title":"Method Call Verification","text":"<p>Semantic analysis checks method calls for correct argument types and counts:</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-4-incorrect-argument-type-in-method-call","title":"Example 4: Incorrect Argument Type in Method Call","text":"<p><pre><code>class A {\n    void test(int a) {}\n}\n\nA a = new A();\na.test(true); // Error: Expected int but got boolean\n</code></pre> - Analysis: The analyzer flags an error because the method <code>test</code> expects an <code>int</code> argument, but a <code>boolean</code> is provided.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-5-incorrect-number-of-arguments-in-method-call","title":"Example 5: Incorrect Number of Arguments in Method Call","text":"<p><pre><code>a.test(100, 200); // Error: Expected 1 argument but got 2\n</code></pre> - Analysis: The semantic analyzer detects that the number of arguments provided to <code>test</code> does not match its signature, resulting in an error.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#resolving-instance-variables-with-this","title":"Resolving Instance Variables with <code>this</code>","text":"<p>The <code>this</code> keyword in Java serves to reference the current instance of a class and is crucial for resolving conflicts between class fields and local variables with overlapping names. During semantic analysis, ensuring the correct usage of <code>this</code> is essential for maintaining the integrity of the object-oriented structure.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-6-using-this-for-accurate-scope-resolution","title":"Example 6: Using <code>this</code> for Accurate Scope Resolution","text":"<pre><code>public class A {\n    int i;\n\n    public void test() {\n        i = 100; // Sets the instance variable i declared in the class\n        int i = 200;\n        System.out.println(i); // Prints 200\n        System.out.println(this.i); // Prints 100\n    }\n}\n</code></pre> <ul> <li>Analysis: In this example, the semantic analyzer must differentiate between the local variable <code>i</code> and the instance variable <code>i</code>. The use of <code>this.i</code> explicitly accesses the class field, while <code>i</code> refers to the local variable within the test method. The analyzer ensures accurate scope resolution, maintaining the proper distinctions and accesses.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#correct-usage-of-break-and-continue","title":"Correct Usage of <code>break</code> and <code>continue</code>","text":"<p>The <code>break</code> and <code>continue</code> statements are used exclusively within loop constructs to either exit a loop prematurely or skip to the next iteration of the loop, respectively. Using them outside of such contexts is semantically incorrect and should be flagged during the analysis phase.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-7-ensuring-break-and-continue-are-used-within-loop-scopes","title":"Example 7: Ensuring break and continue are Used Within Loop Scopes","text":"<pre><code>class A {\n    void test() {\n      continue; // Error: Expected a loop scope\n\n      while(true) break; // Ok: loop scope\n    }\n}\n</code></pre> <ul> <li>Analysis: The semantic analyzer must ensure that both break and continue appear within loop constructs such as <code>for</code>, <code>while</code>, or <code>do-while</code> loops. An error should be flagged if they are used outside of these contexts.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#ensuring-effective-semantic-analysis","title":"Ensuring Effective Semantic Analysis","text":"<p>To implement a robust semantic analyzer, your compiler should: - Maintain comprehensive symbol tables to track variable declarations and manage scope. - Consistently enforce type constraints across expressions, assignments, and method calls. - Provide clear and detailed error messages that facilitate debugging and code correction.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#generating-tac","title":"Generating TAC","text":"<p>In this section, we'll explore how to generate Three Address Code (TAC) for various control structures commonly found in programming. TAC serves as an intermediate representation, bridging the gap between the high-level program code and the final target code, typically facilitating optimization and easier translation.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#if-statement","title":"<code>if</code> Statement","text":"<p>For <code>if</code> statements, TAC generation involves creating conditional and unconditional jumps to handle logical branches. As outlined previously, here's a quick recap:</p> <p>Java Example:</p> <pre><code>if (condition) {\n    // Then block\n    ...\n} else {\n    // Else block\n    ...\n}\n</code></pre> <p>TAC Output:</p> <pre><code>if (!condition) goto if_else;\nif_then:\n    // Then block\n    ...\n    goto if_end;\nif_else:\n    // Else block\n    ...\nif_end:\n</code></pre>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#while-loop","title":"<code>while</code> Loop","text":"<p>While loops require a conditional check at the beginning to decide whether the loop body should execute.</p> <p>Java Example:</p> <pre><code>while (condition) {\n    // Loop body\n    ...\n}\n</code></pre> <p>TAC Output:</p> <pre><code>loop_start:\n    if (!condition) goto loop_end;\n    // Loop body\n    ...\n    goto loop_start;\nloop_end:\n</code></pre>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#do-while-loop","title":"<code>do-while</code> Loop","text":"<p>The <code>do-while</code> loop differs from the <code>while</code> loop in that it guarantees the loop body executes at least once before the condition is checked at the end. This feature necessitates a specific structure in the generated TAC to accommodate this initial execution prior to the condition evaluation.</p> <p>Java Example:</p> <pre><code>do {\n    // Loop body\n    ...\n} while (condition);\n</code></pre> <p>TAC Output:</p> <pre><code>do_start:\n    // Loop body\n    ...\n    if (condition) goto do_start;\n</code></pre>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#for-loop","title":"<code>for</code> Loop","text":"<p>The <code>for</code> loop in Java is structured to contain initialization, condition evaluation, and increment/decrement expressions, allowing it to handle loop control succinctly. Translating this into TAC involves breaking it down into distinct steps to replicate the logical flow of a <code>for</code> loop.</p> <p>Java Example:</p> <pre><code>for (init; condition; incr) {\n    // Loop body\n    ...\n}\n</code></pre> <p>TAC Output:</p> <pre><code>init;        // Initialize loop control variables\nfor_start:\n    if (!condition) goto for_end;  // Evaluate loop condition\n    // Loop body\n    ...\n    incr;     // Increment/decrement loop control variables\n    goto for_start;  // Repeat the loop\nfor_end:\n</code></pre>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#implementing-inheritance-in-c","title":"Implementing Inheritance in C","text":"<p>Although C is not naturally object-oriented and does not support inheritance in the same way that Java does, we can simulate it using structures (<code>struct</code>). This section explains how you can mimic inheritance in C, which will be useful for the code generation part of your Mini-Java to C compiler project.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#understanding-inheritance-in-c","title":"Understanding Inheritance in C","text":"<p>Inheritance is a key feature of object-oriented programming (OOP) that allows a class to inherit properties and behaviors (fields and methods) from another class. While C lacks native OOP support, we can use structures to achieve a form of inheritance-like behavior by embedding one structure within another.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-java-code-with-inheritance","title":"Example: Java Code with Inheritance","text":"<p>Consider the following Java classes demonstrating simple inheritance:</p> <pre><code>class A {\n    int a;\n}\n\nclass B extends A {\n    int b;\n}\n</code></pre>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#equivalent-c-code-using-structures","title":"Equivalent C Code Using Structures","text":"<p>The same inheritance relationship can be represented in C through structures as follows:</p> <pre><code>struct A {\n    int a;\n};\n\nstruct B {\n    struct A super;  // Embedding structure A within B to simulate inheritance\n    int b;\n};\n</code></pre> <p>In this example: - <code>struct A</code> defines a single integer field <code>a</code>. - <code>struct B</code> includes <code>struct A</code> as a field named <code>super</code>, which acts like the superclass part of <code>B</code>.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#accessing-inherited-fields","title":"Accessing Inherited Fields","text":"<p>To access fields from the \"superclass\" in a C structure, you use the <code>super</code> field, which allows you to seamlessly interact with inherited attributes. Here's how you can access <code>a</code> from an instance of <code>struct B</code>:</p> <p><pre><code>struct B *instance = malloc(sizeof(struct B));\ninstance-&gt;super.a = 20;  // Accessing field 'a' from the \"inherited\" part of B\n</code></pre> By embedding a parent structure within a child structure, you replicate the concept of inheritance in C. This method enables the management of hierarchical data relationships similar to classes in Java and is crucial when generating C code from Mini-Java.</p> <p>By embedding a parent structure within a child structure, you replicate the concept of inheritance in C. This method enables the management of hierarchical data relationships similar to classes in Java and is crucial when generating C code from Mini-Java.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#casting-between-structures","title":"Casting Between Structures","text":"<p>One of the advantages of embedding a parent structure as the first field of a child structure in C is that it allows you to cast between the child and parent types effortlessly. This capability is integral to simulating polymorphism and inheritance found in object-oriented languages like Java.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#why-the-order-matters","title":"Why the Order Matters","text":"<p>By placing the <code>super</code> field (which represents the parent structure) as the first field in <code>struct B</code>, you ensure that the memory layout of <code>struct A</code> and the beginning of <code>struct B</code> are identical. This alignment allows you to treat an instance of <code>struct B</code> as if it were an instance of <code>struct A</code>, enabling casting in C.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-casting-in-c","title":"Example: Casting in C","text":"<p>Consider the following example where casting is applied:</p> <pre><code>// Example of casting\nstruct B *instanceB = malloc(sizeof(struct B));\nstruct A *instanceA = (struct A *)instanceB; // Cast B to A\n\n// Accessing field 'a' directly through the casted instance\ninstanceA-&gt;a = 10;\n</code></pre>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#method-overriding-example","title":"Method Overriding Example","text":"<p>In object-oriented languages like Java, method overriding allows a subclass to provide a specific implementation of a method that is already defined in its superclass. This is an important feature for polymorphism and dynamic method dispatch. Here\u2019s how it can be represented in C using function pointers.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#java-example-with-method-overriding","title":"Java Example with Method Overriding","text":"<p>Consider the following Java classes:</p> <pre><code>class A {\n    int a;\n    int test(int d) {\n         return a * d;\n    }\n}\n\nclass B extends A {\n    @Override\n    int test(int d) { \n        return a + d; \n    }\n}\n</code></pre>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#translating-to-c-with-function-pointers","title":"Translating to C with Function Pointers","text":"<p>To simulate method overriding in C, you can use function pointers within structures. This approach allows each class (struct) to define its own implementation of a method, mimicking the polymorphic behavior seen in Java.</p> <p>Here's how you can translate the Java classes into C:</p> <pre><code>struct A {\n    int a;\n\n    // Function pointer for the 'test' method\n    int (*function_test)(void *, int);\n};\n\nstruct B {\n    struct A super;  // 'super' structure to simulate inheritance\n\n    // Function pointer for the 'test' method\n    int (*function_test)(void *, int);\n};\n</code></pre>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#implementing-functions","title":"Implementing Functions","text":"<p>You need to generate functions corresponding to each method implementation in your Java classes. These functions are critical for simulating dynamic method dispatch in C. Here are the example implementations:</p> <pre><code>int A_function_test(void *caller, int d) {\n    struct A* caller_A = (struct A *) caller;\n    return caller_A-&gt;a * d;\n}\n\nint B_function_test(void *caller, int d) {\n    struct B* caller_B = (struct B *) caller;\n    return caller_B-&gt;super.a + d;\n}\n</code></pre> <p>In these functions, casting is used to convert the generic <code>void*</code> caller back to its appropriate type (<code>struct A</code> or <code>struct B</code>) before accessing the fields. This step is crucial for ensuring that the correct data members are accessed, thus maintaining the integrity of the object's structure and behavior.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#initializing-structs-and-setting-up-function-pointers","title":"Initializing Structs and Setting Up Function Pointers","text":"<p>Once you've implemented the necessary functions, you'll need to initialize instances of your structs and set up their function pointers. This is an essential process to ensure that each object has access to the correct method implementations, enabling polymorphic behavior.</p> <pre><code>struct A *instanceA = malloc(sizeof(struct A));\ninstanceA-&gt;function_test = A_function_test;\n\nstruct B *instanceB = malloc(sizeof(struct B));\ninstanceB-&gt;function_test = B_function_test;\ninstanceB-&gt;super.function_test = B_function_test; // Overriding the superclass method\n</code></pre>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#invoking-methods","title":"Invoking Methods","text":"<p>Function pointers provide a mechanism to invoke the correct method implementations dynamically, which is crucial for simulating the polymorphic behavior typical in object-oriented programming. Here's how you can make use of function pointers in practice:</p> <p><pre><code>int res = instanceB-&gt;function_test(instanceB, 100);\n</code></pre> In this example, <code>instanceB-&gt;function_test(instanceB, 100)</code> dynamically calls <code>B_function_test</code>, as expected in an overridden context. This call demonstrates method overriding, ensuring that specific behavior defined in <code>struct B</code> is executed, even when accessed via a reference with a broader class perspective.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#handling-int-array","title":"Handling int[] Array","text":"<p>In translating Mini-Java to C, handling arrays requires careful attention since Java provides built-in functionality such as accessing the length of an array with ease. Meanwhile, C uses basic static arrays without intrinsic length properties. To address this, we outline a method to simulate Java's array length feature using a predefined structure in C.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#suggestion-predefined-int_array-class","title":"Suggestion: Predefined int_array Class","text":"<p>To effectively handle <code>int[]</code> arrays from Mini-Java, you can define a custom structure in C that includes an explicit length field. This structure is crucial for being able to handle array length operations similarly to Java. Here's a simple example:</p> <pre><code>typedef struct {\n    int length;\n    int *data;\n} int_array;\n\nint_array *new_int_array(int size) {\n    int_array *arr = (int_array *) malloc(sizeof(int_array));\n    arr-&gt;length = size;\n    arr-&gt;data = (int *) calloc(size, sizeof(int));\n    return arr;\n}\n</code></pre>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#translating-mini-java-array-declarations","title":"Translating Mini-Java Array Declarations","text":"<p>When translating array declarations from Mini-Java to C, it's crucial to accurately represent the Java features. Here's how you can handle this translation using a custom <code>int_array</code> structure:</p> <ul> <li>Declaration Example:</li> </ul> <p>In Mini-Java, declaring an integer array looks like this:</p> <pre><code>int[] a;\n</code></pre> <p>When translated to C, it becomes:</p> <pre><code>int_array *a;\n</code></pre> <p>Here, <code>a</code> is declared as a pointer to an <code>int_array</code> structure, which will handle the array data and its metadata, such as length.</p> <ul> <li>Array Initialization Example:</li> </ul> <p>An array initialization in Mini-Java, like:</p> <pre><code>int[] a = new int[100];\n</code></pre> <p>Translates to:</p> <pre><code>int_array *a = new_int_array(100);\n</code></pre> <p>This C code creates an <code>int_array</code> with a specified length of 100, allocating memory for storing 100 integers.</p> <ul> <li>Accessing Length Example:</li> </ul> <p>If you need to access the length of the array, Mini-Java syntax:</p> <pre><code>a.length\n</code></pre> <p>Translates to C as:</p> <pre><code>a-&gt;length\n</code></pre> <ul> <li>Accessing Element at Index Example:</li> </ul> <p>Accessing an array element at a specific index in Mini-Java, such as:</p> <pre><code>a[20]\n</code></pre> <p>Translates to C as:</p> <pre><code>a-&gt;data[20]\n</code></pre> <p>This accesses the 21st element in the array stored within the <code>data</code> pointer of the <code>int_array</code> structure.</p> <p>This accesses the <code>length</code> field directly within the <code>int_array</code> structure, allowing you to manage and use the size of arrays as you would within Java.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#tac-with-functions-and-stack","title":"TAC with Functions and Stack","text":"<p>This part of the project is optional and delves into a more advanced aspect of compiler design. </p> <p>As we know, Three Address Code (TAC) doesn't inherently support direct method calls typical of higher-level languages. In striving for a more detailed compilation process, one can bypass the automatic use of C's internal stack and instead implement a custom stack for handling function calls.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#concept","title":"Concept","text":"<p>In traditional TAC, there is no direct representation for function calls and stack management. For those interested in expanding their compiler's capabilities, implementing a custom stack helps simulate the process of managing function calls, arguments, and return values at a lower level.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#example-translation","title":"Example Translation","text":"<p>Consider the following C code snippet that defines and calls a simple function:</p> <pre><code>int test(int arg) {\n    printf(\"Hello World: %d\\n\", arg);\n    return arg * 2;\n}\n\nint main() {\n    int result = test(24);\n    printf(\"Result: %d\\n\", result);\n    return 0;\n}\n</code></pre> <p>In converting this to a system that uses a custom stack for function calls, you might create a translation that looks something like this:</p> <ul> <li>Declare a Custom Stack:</li> <li> <p>Initialize a stack to handle the function frames manually.</p> </li> <li> <p>Push Arguments to Stack:</p> </li> <li> <p>Push the argument <code>24</code> and LR (the label you want to backto after function call) onto the custom stack before invoking the function.</p> </li> <li> <p>Use Goto and Label for Function Invocation:</p> </li> <li> <p>Label sections of the code dedicated to functions for direct jumping using <code>goto</code>.</p> </li> <li> <p>Use Goto to Return from Function: </p> </li> <li>Manipulate the stack to simulate returning from a function, managing the function flow manually.</li> </ul> <p>Here's a conceptual view of the translated code:</p> <pre><code>#define STACK_SIZE 1000\nvoid *stack[STACK_SIZE];\nint stack_top = -1;\n\n#define push_to_stack(value) stack[++stack_top] = (void *) (value)\n#define pop_from_stack() stack[stack_top--]\n\nint main() {\n    goto main;\n\n    main: {\n        push_to_stack(24);\n        void *lr = &amp;&amp;main_after_test; // Simulate Link Register\n        push_to_stack(lr);\n        goto test;\n\n        main_after_test:\n        int result = (int) (intptr_t) pop_from_stack();\n        printf(\"Result: %d\\n\", result);\n        return 0;\n    }\n\n    test: {\n        void *lr = pop_from_stack();\n        int arg = (int) (intptr_t) pop_from_stack();\n        printf(\"Hello World: %d\\n\", arg);\n        push_to_stack(arg * 2);\n        goto *lr;\n    }\n}\n</code></pre> <ul> <li>Analysis: In this setup, you manually manage the stack to handle function arguments and positions via explicit <code>push</code> and <code>pop</code> operations. Function entry and exit points are controlled using labeled sections and <code>goto</code> statements, providing a clear, albeit more complex, method of simulating function calls.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#obvious-constraints-and-examples","title":"Obvious Constraints and Examples","text":"<p>When developing your Mini-Java compiler, it\u2019s crucial to enforce certain constraints to ensure the program's correctness and reliability. Below are some key constraints that should be enforced during the compilation process, along with examples to illustrate each:</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#constraint-1-single-entry-point","title":"Constraint 1: Single Entry Point","text":"<p>A valid Mini-Java program should have only one entry point, defined with the signature <code>public static void main(String[] args)</code>.</p> <p>Example of Correct Usage: Ensure there is exactly one <code>main</code> method with the specified signature: <pre><code>public class Main {\n    public static void main(String[] args) { // Entry point\n        System.out.println(24);\n    }\n}\n</code></pre></p> <ul> <li>Analysis: Ensure that your compiler checks for exactly one <code>main</code> method with the specified signature. Programs with zero or multiple <code>main</code> methods should be flagged with an error.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#constraint-2-unique-class-names","title":"Constraint 2: Unique Class Names","text":"<p>No two classes within the same program should have the same name to prevent ambiguity during type resolution and compilation.</p> <p>Example of Incorrect Usage: Attempting to declare multiple classes with the same name should lead to an error: <pre><code>public class A {}\npublic class A {} // Error: Class A is already defined\n</code></pre></p> <ul> <li>Analysis: The semantic analyzer should flag an error whenever two classes with the same name are declared, enforcing unique identifiers for every class. (Also applies for methods and variables within a class)</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#constraint-3-method-and-class-definition-order","title":"Constraint 3: Method and Class Definition Order","text":"<p>The order of method and class definitions should not affect the program\u2019s semantics. Thus, you should allow references to classes and methods defined later in the program.</p> <p>Example of Non-Sequential Definitions: It is valid to reference a class before its definition: <pre><code>public class B extends A {}\npublic class A {}\n</code></pre></p> <ul> <li>Analysis: The compiler should correctly handle references to <code>A</code> in class <code>B</code>, even though <code>A</code> is defined later in the file. Your compiler needs to manage these dependencies correctly through scope management and forward declarations.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#constraint-4-translating-systemoutprintln","title":"Constraint 4: Translating System.out.println()","text":"<p>In Mini-Java, any call to <code>System.out.println(arg)</code> should properly translate to C\u2019s <code>printf</code> function for output.</p> <p>Example Translation: <pre><code>System.out.println(arg);\n</code></pre> Should be translated to  <pre><code>printf(\"%d\\n\", arg);\n</code></pre></p> <ul> <li>Analysis: During translation to C, ensure that <code>System.out.println()</code> calls are converted to <code>printf</code> with appropriate format specifiers to match the argument type, allowing correct console output in the translated code.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#constraint-5-ignoring-comments","title":"Constraint 5: Ignoring Comments","text":"<p>Your code should correctly ignore comments, both single-line comments (<code>//</code>) and multi-line comments (<code>/* ... */</code>), to focus only on the executable code.</p> <p>Examples of Comments to Ignore:</p> <pre><code>// This is a single-line comment\n/* This is a \n   multi-line comment */\n</code></pre> <ul> <li>Analysis: During lexical analysis, the lexer should remove or skip over comments, ensuring that they do not interfere with the parsing and compilation process. This step helps maintain focus on the actual code logic without the distraction of non-executable comments.</li> </ul>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#constraint-6-no-cyclic-dependencies","title":"Constraint 6: No Cyclic Dependencies","text":"<p>Class hierarchies should not contain cycles, which can lead to infinite loops and logical errors during compilation.</p> <p>Example of Cyclic Dependency (Incorrect): <pre><code>public class A extends B {}\npublic class B extends A {} // Error: Cyclic inheritance detected\n</code></pre></p> <ul> <li>Analysis: The semantic analyzer should detect cyclic inheritance and flag an error. This constraint ensures that the class inheritance hierarchy forms a directed acyclic graph (DAG), maintaining logical soundness and preventing infinite loops in inheritance.</li> </ul> <p>By enforcing these constraints, you can ensure the correctness and robustness of Mini-Java programs compiled by your tool while also maintaining consistency with standard programming practices.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#deliverables","title":"Deliverables","text":"<ol> <li>ANTLR Grammar File: The <code>.g4</code> file defining the MiniJava syntax (If using ANTLR).</li> <li>Parser Implementation: Code for the parser and lexer (generated by ANTLR or your own code).</li> <li>Semantic Analyzer Code: Implementation of type checking and scope resolution.</li> <li>TAC Generation Module: Code that translates the parse tree into Three-Address Code in C.</li> <li>Documentation: A comprehensive report detailing the design decisions, implementation steps, and testing results.</li> </ol>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#timeline","title":"Timeline","text":"Week Task 1 Define MiniJava language syntax and semantics 2 Set up ANTLR and create grammar file 3 Generate parser and lexer; construct parse tree 4 Implement semantic analysis 5 Develop TAC generation module 6 Testing and validation of generated code 7 Finalize documentation and prepare presentation"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#teamwork","title":"Teamwork","text":"<p>Read more at Teamwork Policy page.</p>"},{"location":"projects/spring2025/minijava-to-c-compiler-development-spring2025/#conclusion","title":"Conclusion","text":"<p>Embarking on this compiler course project provides a compelling opportunity to delve into the intricacies of language processing and compiler design. By translating Mini-Java code into C, you will gain valuable insights into how high-level programming constructs are implemented at a lower level, offering both theoretical and practical understanding.</p> <p>Throughout this project, you are encouraged to explore and implement the core components of a compiler\u2014lexical analysis, parsing, semantic analysis, and code generation. Additionally, leveraging tools such as ANTLR can simplify parts of the development process, allowing you to focus on other challenging aspects, like semantic analysis and code generation.</p>"},{"location":"projects/spring2025/team_work_policy/","title":"Teamwork Policy for Compiler Design Course Project","text":""},{"location":"projects/spring2025/team_work_policy/#objective","title":"Objective","text":"<p>To ensure effective collaboration, communication, and accountability among team members working on the Compiler Design Course Project focused on the generation of Three-Address Code for Mini-Java Language.</p>"},{"location":"projects/spring2025/team_work_policy/#team-composition","title":"Team Composition","text":"<ul> <li>Each team must consist of at most three members.</li> <li>Teams should be formed based on mutual agreement among members/students.</li> <li>Each member is expected to contribute equally to the project.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#example-of-roles-and-responsibilities-assignments","title":"Example of Roles and Responsibilities Assignments","text":""},{"location":"projects/spring2025/team_work_policy/#team-leader","title":"Team Leader","text":"<ul> <li>Coordinates meetings and manages project timelines.</li> <li>Serves as the primary point of contact with the instructor.</li> <li>Ensures that tasks are assigned and deadlines are met.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#developer-1","title":"Developer 1","text":"<ul> <li>Focuses on building the parser and lexer using ANTLR.</li> <li>Responsible for writing the ANTLR grammar file and generating the parser.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#developer-2","title":"Developer 2","text":"<ul> <li>Works on semantic analysis and symbol table management.</li> <li>Implements type checking and scope resolution functionalities.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#developer-3-if-applicable","title":"Developer 3 (if applicable)","text":"<ul> <li>Focuses on Three-Address Code generation and output formatting.</li> <li>Assists in testing and validation of the generated code.</li> </ul> <p>Note: Roles can be adjusted based on team preferences and strengths but must be discussed in the project final report.</p>"},{"location":"projects/spring2025/team_work_policy/#communication","title":"Communication","text":""},{"location":"projects/spring2025/team_work_policy/#regular-meetings","title":"Regular Meetings","text":"<ul> <li>Teams are required to meet at least once a week to discuss progress, challenges, and next steps.</li> <li>Meetings can be held in-person or virtually, depending on team preferences.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#communication-channels","title":"Communication Channels","text":"<ul> <li>Establish a preferred communication platform (e.g., Slack, Discord, email) for ongoing discussions and updates.</li> <li>Document important decisions and discussions in a shared document accessible to all members.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#task-management","title":"Task Management","text":""},{"location":"projects/spring2025/team_work_policy/#task-breakdown","title":"Task Breakdown","text":"<ul> <li>Divide the project into manageable tasks and assign them to team members based on their roles.</li> <li>Use project management tools (e.g., Trello, Asana, GitHub Projects) to track progress.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#accountability","title":"Accountability","text":"<ul> <li>Each member is responsible for completing their assigned tasks on time.</li> <li>Team members should support each other in overcoming challenges and ensuring project milestones are met.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#conflict-resolution","title":"Conflict Resolution","text":""},{"location":"projects/spring2025/team_work_policy/#open-communication","title":"Open Communication","text":"<ul> <li>Encourage open discussion of any conflicts or disagreements among team members.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#mediation","title":"Mediation","text":"<ul> <li>If conflicts cannot be resolved internally, seek assistance from the instructor or course facilitator for mediation.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#submission-and-documentation","title":"Submission and Documentation","text":""},{"location":"projects/spring2025/team_work_policy/#collaborative-work","title":"Collaborative Work","text":"<ul> <li>All code and documentation should reflect the collaborative effort of the team.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#version-control","title":"Version Control","text":"<ul> <li>Use version control systems (e.g., Git) to manage code changes and maintain a history of contributions by each member.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#evaluation-criteria","title":"Evaluation Criteria","text":"<ul> <li>Team performance will be evaluated based on:</li> <li>Quality of contributions from each member.</li> <li>Collaboration and communication effectiveness.</li> <li>Adherence to deadlines and project milestones.</li> </ul>"},{"location":"projects/spring2025/team_work_policy/#conclusion","title":"Conclusion","text":"<p>This teamwork policy aims to foster a positive and productive working environment for all team members. By following these guidelines, teams can effectively collaborate on the Compiler Design Course Project, ensuring a successful outcome while enhancing individual learning experiences.</p>"},{"location":"projects/student-solutions/minijava-to-c/Aghajari/","title":"CheckOut Main Repository: MiniJava-C-Compiler","text":""},{"location":"projects/student-solutions/minijava-to-c/Aghajari/#minijava-c-compiler","title":"MiniJava-C-Compiler","text":"<p>The MiniJava-C Compiler is a project designed to compile MiniJava code into C. MiniJava is a simplified version of Java that includes essential object-oriented features, making it ideal for educational purposes and lightweight application development. This compiler parses MiniJava code, performs semantic checks, and generates equivalent C code for execution.</p> <p>Note: This project was developed for educational purposes as part of the Compiler Design course at Amirkabir University of Technology (AUT).</p>"},{"location":"projects/student-solutions/minijava-to-c/Aghajari/#compiler-phases","title":"Compiler Phases","text":"<ol> <li>Lexer:</li> <li>Converts the input MiniJava code into a stream of tokens.</li> <li>Identifies keywords, identifiers, literals, and symbols.</li> <li>Reports lexical errors if invalid characters are encountered.</li> <li>Uses SimpleJavaLexer</li> <li>Parser:</li> <li>Analyzes the token stream to construct an Abstract Syntax Tree (AST).</li> <li>Ensures the syntax conforms to the MiniJava grammar.</li> <li>Reports syntax errors with precise locations.</li> <li>Semantic Analyzer:</li> <li>Performs type checking and ensures the semantic correctness of the program.</li> <li>Validates variable declarations, method calls, and expression types.</li> <li>Reports semantic errors, such as type mismatches or undeclared variables.</li> <li>Code Generator:</li> <li>Translates the validated AST into equivalent C code.</li> <li>Handles variable initialization, method calls, control flow, and expressions.</li> <li>Produces optimized and readable C output.</li> </ol>"},{"location":"projects/student-solutions/minijava-to-c/Aghajari/#features","title":"Features","text":"<ul> <li>Object-Oriented Programming Support :</li> <li>Classes and inheritance</li> <li>Method overriding</li> <li>Field access across inheritance chains</li> <li>this reference</li> <li>Control Structures :</li> <li><code>if</code>, <code>else</code> statements</li> <li><code>for</code>, <code>while</code> loops</li> <li>break and continue statements</li> <li>Arrays :</li> <li>Integer array support (int[])</li> <li>Array length property</li> <li>Array indexing</li> <li>Basics :</li> <li>Basic data types: int, boolean</li> <li>Classes and objects</li> <li>Method calls</li> <li>Conditional statements (if-else)</li> <li>Loops (while)</li> <li>Arithmetic operations (+, -, *, /, %)</li> <li>Logical operations (&amp;&amp;, ||, !)</li> <li>Relational operators (&lt;, &lt;=, &gt;, &gt;=, ==, !=)</li> <li>Variable assignments</li> </ul>"},{"location":"projects/student-solutions/minijava-to-c/Aghajari/#example","title":"Example","text":"<pre><code>class Main {\n    public static void main(String[] args) {\n        Calculator calc = new Calculator();\n        int result = 2 + calc.add(4, calc.multiply(2, 4) / 2) * 4;\n        System.out.println(result);\n    }\n}\nclass Base {\n    public int add(int a, int b) {\n        return a + b;\n    }\n}\nclass Calculator extends Base {\n    public int multiply(int a, int b) {\n        return a * b;\n    }\n}\n</code></pre> <p>Compiles to:</p> <pre><code>// Main.c\nint main() {\n    Calculator *calc = $_new_Calculator();\n    int result;\n    int $_t_1 = calc-&gt;$_function_multiply(calc, 2, 4);\n    int $_t_2 = $_t_1 / 2;\n    int $_t_3 = calc-&gt;super.$_function_add(calc, 4, $_t_2);\n    int $_t_4 = $_t_3 * 4;\n    int $_t_5 = 2 + $_t_4;\n    result = $_t_5;\n    printf(\"%d\\n\", result);\n}\n\n// Base.h\nstruct Base {\n    int (*$_function_add)(void *, int, int);\n};\ntypedef struct Base Base;\nint Base_add(void *$this, int a, int b);\nBase *$_new_Base();\n\n// Base.c\nBase *$_new_Base() {\n    Base *self = (Base *) malloc(sizeof(Base));\n    self-&gt;$_function_add = Base_add;\n    return self;\n}\nint Base_add(void *$this, int a, int b) {\n    Base *super = (Base *) $this;\n    int $_t_0 = a + b;\n    return $_t_0;\n}\n\n// Calculator.h\nstruct Calculator {\n    Base super;\n    int (*$_function_multiply)(void *, int , int );\n};\ntypedef struct Calculator Calculator;\nint Calculator_multiply(void *$this, int a, int b);\nCalculator *$_new_Calculator();\n\n// Calculator.c\nCalculator *$_new_Calculator() {\n    Calculator *self = (Calculator *) malloc(sizeof(Calculator));\n    self-&gt;$_function_multiply = Calculator_multiply;\n    self-&gt;super.$_function_add = Base_add;\n    return self;\n}\nint Calculator_multiply(void *$this, int a, int b) {\n    Calculator *super = (Calculator *) $this;\n    int $_t_0 = a * b;\n    return $_t_0;\n}\n</code></pre> <p>Parse Tree: <pre><code>...\nClass{\n  Name: Calculator\n  Extends: Base\n  Fields: (0)\n  Methods: (1)\n    Method{Name: multiply, Type: int, Params: (Field{Name: a, Type: int}, Field{Name: b, Type: int})} {\n      CodeBlock\n        Return: \n          BinaryExpression (*) (Type:int)\n            Reference (Type:int): a\n            Reference (Type:int): b\n    }\n}\n</code></pre></p>"},{"location":"projects/student-solutions/minijava-to-c/Aghajari/#implementation-details","title":"Implementation Details","text":"<ul> <li>Class Translation</li> <li>Classes become C structs</li> <li>Methods become function pointers</li> <li>Inheritance uses nested structs</li> <li>Inheritance validation</li> <li>Static type checking</li> <li>Method Dispatch</li> <li>Virtual method tables via function pointers</li> <li>$this pointer passed as first argument</li> <li>Method override verification</li> </ul>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/capstone-projects/","title":"Capstone Projects","text":""},{"location":"blog/category/solutions/","title":"Solutions","text":""}]}